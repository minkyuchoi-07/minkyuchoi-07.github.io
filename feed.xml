<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Minkyu Choi&#39;s Personal Webpage</title>
    <link>https://minkyuchoi-07.github.io/</link>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml"/>
    
    <description>This is a personal Webpage</description>
    <pubDate>Sun, 01 Dec 2024 02:29:22 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>PEERNet: Benchmarking Networked Robotics on Wifi, 5G, and Beyond</title>
      <link>https://minkyuchoi-07.github.io/2024/12/01/peernet/</link>
      <guid>https://minkyuchoi-07.github.io/2024/12/01/peernet/</guid>
      <pubDate>Sun, 01 Dec 2024 00:05:21 GMT</pubDate>
      <description>
      
        &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2409.06078&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Paper-PDF-green.svg&quot; alt=&quot;Paper&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/pdf/2409.06078&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2403.11021-b31b1b.svg&quot; alt=&quot;arXiv&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/UTAustin-SwarmLab/PEERNet&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Code-PEERNet-blue.svg&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;Aditya Narayanan, Pranav Kasibhatla, &lt;strong&gt;Minkyu Choi&lt;/strong&gt;, Po-han Li, Ruihan Zhao, Sandeep Chinchali&lt;br&gt;&lt;em&gt;International Conference on Intelligent Robots and Systems (IROS), 2024&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;This post introduces &lt;em&gt;PEERNet&lt;/em&gt;, a Python package for real-time benchmarking of networked robotic systems. It provides concise and modular methods for performance analysis of the entire system stack.&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p><a href="https://arxiv.org/pdf/2409.06078"><img src="https://img.shields.io/badge/Paper-PDF-green.svg" alt="Paper"></a> <a href="https://arxiv.org/pdf/2409.06078"><img src="https://img.shields.io/badge/arXiv-2403.11021-b31b1b.svg" alt="arXiv"></a> <a href="https://github.com/UTAustin-SwarmLab/PEERNet"><img src="https://img.shields.io/badge/Code-PEERNet-blue.svg" alt="GitHub"></a> </p><p>Aditya Narayanan, Pranav Kasibhatla, <strong>Minkyu Choi</strong>, Po-han Li, Ruihan Zhao, Sandeep Chinchali<br><em>International Conference on Intelligent Robots and Systems (IROS), 2024</em>.</p><p>This post introduces <em>PEERNet</em>, a Python package for real-time benchmarking of networked robotic systems. It provides concise and modular methods for performance analysis of the entire system stack.</p><span id="more"></span><div style="text-align: center; display: flex; justify-content: center; align-items: center; min-height: 20vh; flex-direction: column;">  <span style="max-width: 70%; max-height: 70%; height: auto;">    <img src="https://hackmd.io/_uploads/ryxuxVStA.png" " alt title="Figure 1" style="width: 100%; height: auto;">  </span>  <span style="max-width: 70%; text-align: left; display: block; margin-top: 10px;">    <div>      <strong>Fig. 1 PEERNet is an end-to-end profiling tool for networked robotics. It excels in ease of deployment across various platforms and offers extensive benchmarking capabilities.</strong>    </div>  </span></div><br><h2 id="Why-benchmarking-networked-robotics"><a href="#Why-benchmarking-networked-robotics" class="headerlink" title="Why benchmarking networked robotics?"></a>Why benchmarking networked robotics?</h2><p>Consider a mobile robot running a Machine Learning (ML) pipeline for perception, localization, and path planning. Here and in many other practical scenarios, offloading computationally expensive ML tasks to a cloud server over public networks is a viable optimization.</p><p>Introducing offloaded computation brings up several system-design questions which require hard statistics and real data to answer:</p><ul><li><em>Which ML model maximizes decision accuracy while minimizing end-to-end latency?</em></li><li><em>How much better is the end-to-end latency when using a 5G network as opposed to a 4G LTE network?</em></li><li><em>How does data compression affect end-to-end latency in offloaded inference? Is the overhead of compression worth the faster networking latency? Does that answer change when using a 5G network as opposed to LTE or WiFi?</em></li></ul><p>More generally, real-time benchmarking is required for:</p><ol><li><p><strong>Deployment in the real world:</strong> Before a robotic system can be deployed, decisions need to be made on hardware, ML models, networking protocols, etc.‚Äìthese decisions must be informed by real-world statistics.</p></li><li><p><strong>Debugging exising systems:</strong> Finding and removing bottlenecks in existing robotic systems requires hard numbers. ML model latency, network delay, or even hardware limitations could all be a system‚Äôs bottleneck.</p></li><li><p><strong>Optimizing networked robotics:</strong> Many novel techniques for optimizing data sharing, offloading policies, and distributed ML are in active development. Assessing the performance of new algorithms on real hardware is essential before optimizations can be applied.</p></li></ol><h2 id="What-makes-benchmarking-non-trivial"><a href="#What-makes-benchmarking-non-trivial" class="headerlink" title="What makes benchmarking non-trivial?"></a>What makes benchmarking non-trivial?</h2><p>Benchmarking networked robotics is fundamentally more challenging than simply timing ML models and testing sensors. The incorporation of networks into robotic systems significantly complicates the benchmarking problem by introducing:</p><ol><li><p><strong>Stochastic delay</strong>: Network delays are highly stochastic and system dependent. Delays are hard to predict and often change significantly from one setup to another.</p></li><li><p><strong>Network Timing and Asymmetric Delay</strong>: Profiling networking operations by itself is difficult due to imperfect clock synchronization. Round-trip times, while commonly used, are of little use in networked robotic systems, where devices rarely upload and download similar amounts of data. For example, our mobile robot from earlier may upload gigabytes of images and lidar scans while downloading only bytes of labels. Hence, round-trip delay fails to provide any meaningful insight into this system.</p></li></ol><h2 id="Introducing-PEERNet"><a href="#Introducing-PEERNet" class="headerlink" title="Introducing PEERNet"></a>Introducing PEERNet</h2><p>We present PEERNet, the <strong>P</strong>rofiler for <strong>E</strong>nd-to-<strong>E</strong>nd <strong>R</strong>eal-time <strong>Net</strong>worked Robotics. PEERNet is a highly modular and extensible package for profiling networked robotic systems, complete with one-way network delay estimation. PEERNet interfaces with industry standard hardware and software such as Nvidia embedded GPUs, Robot Operating System (ROS), and Zero MQ (ZMQ). PEERNet exposes a CLI for rapid benchmarking of offloaded inference systems, and we demonstrate PEERNet‚Äôs ease of use and versatility.</p><p>At a high level, PEERNet focuses on profiling networked robotics by looking at the life-cycle of data. When sensors sample data, serializable logging metadata is attached, and propagated through the system normally. This metadata includes all necessary information for tracking and profiling sampling time, ML inference latency, and one-way network delay.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>We illustrate and validate PEERNet by implementing and profiling three networked robotic systems. All of our experiments are conducted on physical hardware and live wireless networks, demonstrating that PEERNet is modular, robust, and capable of precisely profiling various systems.</p><h3 id="Benchmarking-offloaded-inference"><a href="#Benchmarking-offloaded-inference" class="headerlink" title="Benchmarking offloaded inference"></a>Benchmarking offloaded inference</h3><p>Our first experiment demonstrates how PEERNet can be used to profile and understand offloaded image classification, such as in our mobile robot example. By precisely quantifying latency costs across local and cloud hardware and for various model sizes, PEERNet allows for intelligent device/model selection.</p><div style="text-align: center; display: flex; justify-content: center; align-items: center; min-height: 20vh; flex-direction: column;">  <span style="max-width: 70%; max-height: 70%; height: auto;">    <img src="https://hackmd.io/_uploads/S1pz-4HK0.png" " alt title="Figure 2" style="width: 100%; height: auto;">  </span>  <span style="max-width: 70%; text-align: left; display: block; margin-top: 10px;">    <div>      <strong>Fig. 2 PEERNet precisely quantifies inference costs at the edge and in the cloud.</strong> For the EfficientNetV2 family of models, local computation on an edge device is roughly 2.5 times as slow as offloaded computation to a cloud server, but cloud servers display a high variance in inference latency.    </div>  </span></div><br><h3 id="Identifying-non-intuitive-behavior-in-Vison-Language-Models"><a href="#Identifying-non-intuitive-behavior-in-Vison-Language-Models" class="headerlink" title="Identifying non-intuitive behavior in Vison Language Models"></a>Identifying non-intuitive behavior in Vison Language Models</h3><p>In our second experiment, we use PEERNet to explore inference with Vision Language Models (VLMs) at the edge, an emerging task in robotic pipelines. We show that PEERNet is capable of identifying non-intuitive behaviors in VLM inference, such as a bimodal output token distribution.</p><div style="text-align: center; display: flex; justify-content: center; align-items: center; min-height: 20vh; flex-direction: column;">  <span style="max-width: 70%; max-height: 70%; height: auto;">    <img src="https://hackmd.io/_uploads/BJoQ-4BFC.png" " alt title="Figure 3" style="width: 100%; height: auto;">  </span>  <span style="max-width: 70%; text-align: left; display: block; margin-top: 10px;">    <div>      <strong>Fig. 3 Profiling with PEERNet reveals non-intuitive behavior of a vision language model.</strong> Responses to a single prompt vary in length, with two centers in the distribution of output length. Consequently, the inference latency is bimodal.    </div>  </span></div><br><h3 id="Optimizing-teleoperation-of-a-robot-arm"><a href="#Optimizing-teleoperation-of-a-robot-arm" class="headerlink" title="Optimizing teleoperation of a robot arm"></a>Optimizing teleoperation of a robot arm</h3><p>Our final experiment applies PEERNet to benchmark a full teleoperation pipeline. We explore all combinations of edge device, cloud compute, compression, and network on a teleoperated robot arm performing a pick-and-place task. We demonstrate that profiling with PEERNet solves the hardware selection problem, and is easily applicable to existing robotic pipelines.</p><div style="text-align: center; display: flex; justify-content: center; align-items: center; min-height: 20vh; flex-direction: column;">  <span style="max-width: 70%; max-height: 70%; height: auto;">    <img src="https://hackmd.io/_uploads/Hy9EbNBFC.png" " alt title="Figure 4" style="width: 100%; height: auto;">  </span>  <span style="max-width: 70%; text-align: left; display: block; margin-top: 10px;">    <div>      <strong>Fig. 4 Profiling with PEERNet reveals latency tradeoffs and costs in end-to-end teleoperation pipelines.</strong> PEERNet identifies the combination of an RTX3090 GPU, local image resizing, and a LAN to be the most performative teleoperation configuration, with lower end-to-end latency than local inference.    </div>  </span></div><br><h2 id="Looking-to-try-PEERNet-out"><a href="#Looking-to-try-PEERNet-out" class="headerlink" title="Looking to try PEERNet out?"></a>Looking to try PEERNet out?</h2><p>Our code is open-source! You can find it at github.com/UTAustin-SwarmLab/PEERNet</p>]]></content:encoded>
      
      <comments>https://minkyuchoi-07.github.io/2024/12/01/peernet/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Diffusion Based Video Compression</title>
      <link>https://minkyuchoi-07.github.io/2024/07/15/diffusion-based-video-compression/</link>
      <guid>https://minkyuchoi-07.github.io/2024/07/15/diffusion-based-video-compression/</guid>
      <pubDate>Mon, 15 Jul 2024 01:34:25 GMT</pubDate>
      <description>
      
        &lt;p&gt;&lt;a href=&quot;https://drive.google.com/file/d/1zAPmyW9Mge0uZxeOFtyQ9lr1Dg4l4R43/view?usp=sharing&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Paper-pdf-green.svg&quot; alt=&quot;Paper&quot;&gt;&lt;/a&gt;&lt;br&gt;Recent advances have enabled diffusion models to efficiently compress videos while maintaining high visual quality. By storing only keyframes and using these models to interpolate frames during playback, this method ensures high fidelity with minimal data. The process is adaptive, balancing detail retention and compression ratio, and can be conditioned on lightweight information like text descriptions or edge maps for improved results.&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p><a href="https://drive.google.com/file/d/1zAPmyW9Mge0uZxeOFtyQ9lr1Dg4l4R43/view?usp=sharing"><img src="https://img.shields.io/badge/Paper-pdf-green.svg" alt="Paper"></a><br>Recent advances have enabled diffusion models to efficiently compress videos while maintaining high visual quality. By storing only keyframes and using these models to interpolate frames during playback, this method ensures high fidelity with minimal data. The process is adaptive, balancing detail retention and compression ratio, and can be conditioned on lightweight information like text descriptions or edge maps for improved results.</p><span id="more"></span><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>The rapid growth of digital video content across various platforms necessitates more efficient video compression technologies. Traditional compression methods struggle to balance compression efficiency with high visual fidelity. This blog post discusses a novel approach using diffusion-based models for adaptive video compression, showcasing substantial improvements in efficiency and quality.</p><p><strong>Traditional Compression Challenges:</strong> Standard compression algorithms, like H.264/AVC, H.265/HEVC, and the newer H.266/VVC, primarily focus on reducing spatial and temporal redundancies. However, they often fall short when dealing with high-quality streaming requirements, particularly for content with complex textures or fast movements.</p><p><strong>Breakthrough with Diffusion Models:</strong> Diffusion models, originally designed for image synthesis, have demonstrated potential in enhancing video compression. These models capture intricate image patterns, offering a paradigm shift from predictive to conditional frame generation.</p><h1 id="Theoretical-Framework"><a href="#Theoretical-Framework" class="headerlink" title="Theoretical Framework"></a>Theoretical Framework</h1><p>This section delves into the mathematical foundations and technical descriptions of how diffusion models are applied to video compression.</p><p><strong>Diffusion for Frame Generation:</strong> Diffusion models perform a forward process where noise is progressively added to the video frames, and a reverse process where this noise is removed to reconstruct the original video content, hence allowing for efficient frame prediction.</p><p><strong>Video Compression via Diffusion:</strong> The core idea is to use the generative capabilities of diffusion models to predict future frames based on past frames, significantly reducing the need to transmit every frame.</p><div style="text-align: center; display: flex; justify-content: center; align-items: center; min-height: 20vh; flex-direction: column;">  <span style="max-width: 70%; max-height: 70%; height: auto;">    <img src="/2024/07/15/diffusion-based-video-compression/approach_diagram.png" alt title="Main Design Architecture" style="width: 100%; height: auto;">  </span>  <span style="max-width: 70%; text-align: left; display: block; margin-top: 10px;">    <em>Figure 1: <strong>Main Design Architecture:</strong> A video streaming pipeline using our method: Identical diffusion models are placed at a host and client; a quality function to govern frames to send downstream; and compression using Matryoshka Representation Learning (MRL). Combining all these design features significantly increase reconstruction quality and reduce compression size</em>  </span></div><br><h1 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h1><p>We introduce a unique approach that combines traditional video compression techniques with the advanced capabilities of diffusion models.</p><p><strong>Adaptive Video Compression Using MCVD:</strong> Our method employs the Masked Conditional Video Diffusion (MCVD) model, which facilitates the generation of high-quality frames from masked past and future frames. This allows for dynamic adjustment of compression based on frame quality.</p><p><strong>Integration of Matryoshka Representation Learning:</strong> To enhance compression further, we incorporate Matryoshka Representation Learning (MRL), which optimizes the encoding space and significantly reduces the size of data packets transmitted across networks.</p><div style="text-align: center; display: flex; justify-content: center; align-items: center; min-height: 20vh; flex-direction: column;">  <span style="max-width: 50%; max-height: 50%; height: auto;">    <img src="/2024/07/15/diffusion-based-video-compression/mrl_concept.png" alt title="Matryoshka Representation Learning" style="width: 100%; height: auto;">  </span>  <span style="max-width: 50%; text-align: left; display: block; margin-top: 10px;">    <em>Figure 2: <strong>Matryoshka Representation Learning:</strong> Identical losses are calculated for each chunk of the embedding and then summed up (as shown on the left). This leads to better size-to-performance ratio on classification tasks (as shown on the right).</em>  </span></div><br><h1 id="Experimental-Validation"><a href="#Experimental-Validation" class="headerlink" title="Experimental Validation"></a>Experimental Validation</h1><p>We conducted several experiments to validate the effectiveness of our proposed method against traditional compression standards.</p><p><strong>Experimental Setup:</strong> The experiments were carried out using a dataset from the Kodak Image Suite, with metrics such as Bits Per Pixel (BPP) and Peak Signal to Noise Ratio (PSNR) used to evaluate performance.</p><div style="text-align: center; display: flex; justify-content: center; align-items: center; min-height: 20vh; flex-direction: column;">  <span style="max-width: 40%; max-height: 40%; height: auto;">    <img src="/2024/07/15/diffusion-based-video-compression/table1.png" alt title="Extreme image compression on Kodak images" style="width: 100%; height: auto;">  </span>  <span style="max-width: 40%; text-align: left; display: block; margin-top: 10px;">    <em>Figure 3: <strong>Extreme image compression on Kodak images:</strong> Our method can reconstruct the image with much smaller bpps on a universal test image, Kodak.</em>  </span></div><br><p><strong>Results:</strong> Our results, seen in <code>Figure 3</code>, exemplify MRL improves the image compression without loss of reconstructive quality. We use the same NIC model in our video streaming application, where due to limited infrastructure, we use a pre-trained MCVD diffusion model trained on a Cityscape dataset. Our method shows comparable video reconstruction performance, measured in PSNR, with <em>lower</em> bits-per-pixel (BPP)s, signifying greater compression without performance loss. Our main results are shown in <code>Figure 4</code> and <code>Figure 6</code>.</p><div style="text-align: center; display: flex; justify-content: center; align-items: center; min-height: 20vh; flex-direction: column;">  <span style="max-width: 50%; max-height: 50%; height: auto;">    <img src="/2024/07/15/diffusion-based-video-compression/avg_bpp_vs_avg_psnr.png" alt title="Extreme video compression with higher performance" style="width: 100%; height: auto;">  </span>  <span style="max-width: 50%; text-align: left; display: block; margin-top: 10px;">    <em>Figure 4: <strong>Extreme video compression with higher performance:</strong> Our method has a higher compression rate with better performance than the benchmark. Colored circles represent variance around the mean values for 40 sample videos, with blue for the benchmark and orange for our method.</em>  </span></div><br><div style="text-align: center; display: flex; justify-content: center; align-items: center; min-height: 20vh; flex-direction: column;">  <span style="max-width: 60%; max-height: 60%; height: auto;">    <img src="/2024/07/15/diffusion-based-video-compression/wrt_dim.png" alt title="Better video reconstruction with a higher compression rate" style="width: 100%; height: auto;">  </span>  <span style="max-width: 60%; text-align: left; display: block; margin-top: 10px;">    <em>Figure 5: <strong>Better video reconstruction with a higher compression rate:</strong> Given small embedding dimensions, 50 and 100, our method can reconstruct better video with smaller bpps.</em>  </span></div><br><p>We first compare our method to DBVC with respect to the size of the dimension.  Our experiment shows that given a small dimension, our method can have much higher PSNRs and lower BPPs, as shown in <code>Figure 5</code>. Our results also illustrate that our method has a higher compression rate due to MRL reducing the encoded vector size, which induces a lower BPP.</p><p>Next, we compare the overall performance of our method against the benchmark, DBVC, through the BPP and PSNR metrics.<br>Our method has an ~8% higher average PSNR than the benchmark for the same compression rate, shown in <code>Figure 4</code>. Upon careful examination, we can see improvements of nearly 30% for smaller dimensions and larger BPPs.</p><div style="text-align: center; display: flex; justify-content: center; align-items: center; min-height: 20vh; flex-direction: column;">  <span style="max-width: 90%; max-height: 90%; height: auto;">    <img src="/2024/07/15/diffusion-based-video-compression/bpp_psnr.png" alt title="Ablation across encoding dimension size" style="width: 100%; height: auto;">  </span>  <span style="max-width: 90%; text-align: left; display: block; margin-top: 10px;">    <em>Figure 6: <strong>Ablation across encoding dimension size:</strong> Our method outperforms the baseline for lower encoded dimension size at the cost of a slightly reduced PSNR.</em>  </span></div><br><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>The integration of diffusion models into video compression marks a significant advancement in multimedia technologies. By leveraging these models, we can significantly reduce the bandwidth required for high-quality video streaming. The next steps include optimizing these models for real-time processing and exploring their applications in live video feeds.</p>]]></content:encoded>
      
      <comments>https://minkyuchoi-07.github.io/2024/07/15/diffusion-based-video-compression/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Distill Diffusion</title>
      <link>https://minkyuchoi-07.github.io/2024/03/16/distill-diffusion/</link>
      <guid>https://minkyuchoi-07.github.io/2024/03/16/distill-diffusion/</guid>
      <pubDate>Sat, 16 Mar 2024 23:15:50 GMT</pubDate>
      <description>
      
        &lt;p&gt;Distill series ‚Äì diffusion model.&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>Distill series ‚Äì diffusion model.</p><span id="more"></span><h2 id="Conditional-Diffusion-for-Video"><a href="#Conditional-Diffusion-for-Video" class="headerlink" title="Conditional Diffusion for Video"></a>Conditional Diffusion for Video</h2><p>Let‚Äôs test first learn about a diffusion model. A diffusion model generates data by reversing a diffusion process that gradually adds noise to the data unitl it becomes pure noise. </p><p>The process is modeled in two phases: the foreard process and the reverse (or backward) process. </p><p>Let $x_0 \in \mathbb{R}^d$ be a simple from the data distribution $p_{\text{data}}$. </p><ol><li><strong>Forward Process (Diffusion):</strong> This is where we start with data $x_0$‚Äã from our desired distribution and add noise over several steps until we get to a point where our data is indistinguishable from noise. </li><li><strong>Reverse Process (Denoising):</strong> <span style="color: red;">We learn to reverse the noise addition process.</span> If we do this correctly we can start with noise and apply the reverse process to generate new data sample that appear as if they were drawn from the target data distribution.</li></ol><h3 id="Algorithm-with-code"><a href="#Algorithm-with-code" class="headerlink" title="Algorithm with code"></a>Algorithm with code</h3><p>The process is mathematically decribed using Markov chains with Gaussian transitions. </p><h4 id="Forward-Transition-Kernel"><a href="#Forward-Transition-Kernel" class="headerlink" title="Forward Transition Kernel:"></a><strong>Forward Transition Kernel:</strong></h4><ol><li><p>$q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1 - \beta_t}x_{t-1}, \beta_t \mathbf{I})$</p></li><li><p>At each time step $t$, a Gaussian noise is added to the data. $\beta_t$‚Äã‚Äã is a variance schedule that is chosen beforehand and determines  <span style="color: red;">how much noise to add at each step.</span></p></li><li><p>Note that we take a square of $\beta$ to scale down $x_{t-1}$ so that after adding noise, the total variance remains 1. It scales down the previous difussed image to make room for the noise and then adding Gaussian noise with just the right variance to maintain the overall variance of the process.</p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">def</span> <span class="title function_">forward_diffusion</span>(<span class="hljs-params">x_0, T</span>):</span><br><span class="line">    x_t = x_0</span><br><span class="line">    <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, T+<span class="hljs-number">1</span>):</span><br><span class="line">        beta_t = beta[t-<span class="hljs-number">1</span>]</span><br><span class="line">        x_t = np.sqrt(<span class="hljs-number">1.</span> - beta_t) * x_t + np.sqrt(beta_t) * np.random.normal(size=x_0.shape)</span><br><span class="line">    <span class="hljs-keyword">return</span> x_t</span><br></pre></td></tr></table></figure></li></ol><h4 id="Accumulated-Kernel"><a href="#Accumulated-Kernel" class="headerlink" title="Accumulated Kernel:"></a><strong>Accumulated Kernel:</strong></h4><div style="text-align: center; display: flex; justify-content: center; align-items: center; min-height: 20vh;">  <img src="/2024/03/16/distill-diffusion/accumulated_kernel_draw.png" alt title style="max-width: 50%; max-height: 50%; height: auto;"></div><ol><li><p>This describes how to sample $x_t$ directly from $x_0$ without going through all intermidate steps.</p></li><li><p>$q(x_t | x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t}x_0, (1 - \bar{\alpha}_t)$‚Äã</p></li><li><img src="/2024/03/16/distill-diffusion/eq_3_1.png" width="250" height="100" alt title> </li><li><p>$\bar{\alpha}_t$ is the accumulated product of (1-$\beta_s$) from time 1 to $t$ and represents <strong>the overall scale of the original data</strong> present in $x_t$. </p></li></ol><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">def</span> <span class="title function_">sample_from_x0</span>(<span class="hljs-params">x_0, t, alpha_bar</span>):</span><br><span class="line">    alpha_t = alpha_bar[t-<span class="hljs-number">1</span>]</span><br><span class="line">    <span class="hljs-keyword">return</span> np.sqrt(alpha_t) * x_0 + np.sqrt(<span class="hljs-number">1.</span> - alpha_t) * np.random.normal(size=x_0.shape)</span><br></pre></td></tr></table></figure><h4 id="Reverse-Transition-Kernel-Reverse-Diffusion-Process"><a href="#Reverse-Transition-Kernel-Reverse-Diffusion-Process" class="headerlink" title="Reverse Transition Kernel (Reverse Diffusion Process)"></a>Reverse Transition Kernel (Reverse Diffusion Process)</h4><div style="text-align: center; display: flex; justify-content: center; align-items: center; min-height: 20vh;">  <img src="/2024/03/16/distill-diffusion/reverse_kernel_draw.png" alt title style="max-width: 50%; max-height: 50%; height: auto;"></div><p>The Reverse Diffusion Process (RDP), used to generate new samples from a distribution by reversing the Forward Diffusion Process (FDP). The FDP gradually adds noise to the data, and the RDP aims to <strong>learn</strong> to reverse this noise addition to recreate the data from the noisy distribution.</p><img src="/2024/03/16/distill-diffusion/rdp_eq.png" width="600" height="100" alt title> <p>$\tilde{\mu}_t$ is the mean of the reverse transition kernel at time $t$, which is a linear combination of the original data point $x_0$ and the current noisy data point $x_t$. </p><p>The $\tilde \beta_t$ term is the variance of the Gaussian distribution for the reverse step, which changes at each step, reflecting how the uncertainty decreases as we approach the original data distribution.</p><p>The coefficient for $x_0$ determines how much of the original data point $x_0$ is retained in the reverse transition. As $t$ decreases, $\bar \alpha_{t-1}$ increases, and $1-\bar \alpha_{t-1}$ decreases, making the coefficient larger. This means that as we get closer to $t=0$, we are realying more on the original data point $x_0$ to reconstruct $x_{x-1}$.</p><p>The coefficient for $x_t$ determine the contribution of the current noisy data point $x_t$ to the reverse transition. As $t$ decreases, $\alpha_t$ and $\bar \alpha_{t-1}$ is decreases, making the coefficient smaller.</p><p>We want to remove just the right amount of noise (represented by $x_t$) while restoring the right amount of signal (represented by $x_0$). Initially, $x_t$ is mostly noisy, so its coefficient is relatively small, but as we reverse the diffusion process, $x_t$ gradually becomes less noisy, and its influence increases. </p><p>In the reverse diffusion process, we are effectively using these coefficient to ‚Äúinterploate‚Äù between the noise and the data at each step. Initially, the process is noise-dominated, and we have only small ‚Äúhint‚Äù of the data. But as we step back through time, the data‚Äôs contribution becomes larger and the noise‚Äôs contribution becomes smaller, ultimately allowing us to reconstruct a sample that resembles the original data distribution.</p><p>It‚Äôs important to note that in practice, $\tilde \mu$‚Äã would be predicted by a neural network trained to estimate the denoising step.</p><h4 id="With-a-Neural-Network"><a href="#With-a-Neural-Network" class="headerlink" title="With a Neural Network"></a>With a Neural Network</h4><p>In a neural network-based approach to reversing the diffusion process to generate data, we focus on estimating the original data $x_0$ from its noisy version $x_t$ without having direct knowledge of $x_0$. This is a critical aspect of the reverse process in diffusion-based generative models.</p><h5 id="Estimation-of-x-0"><a href="#Estimation-of-x-0" class="headerlink" title="Estimation of  $x_0$:"></a>Estimation of  $x_0$:</h5><ul><li>Since $x_0$ is unknown during generation, we estimate it from $x_t$, the noisy data at a current timestep.</li><li>The estimation equation $ \hat{x}_0 = \frac{(x_t - \sqrt{1 - \bar{\alpha}_t} \epsilon)}{\sqrt{\bar{\alpha}_t}} $ rearranges the forward process (Equation 2).</li><li>The noise $ \epsilon $ added at each diffusion step is unknown; thus, we utilize a neural network parameterized by $ \theta(x_t | t) $ to estimate this noise.</li></ul><h5 id="Neural-Network-and-Loss-Function"><a href="#Neural-Network-and-Loss-Function" class="headerlink" title="Neural Network and Loss Function:"></a>Neural Network and Loss Function:</h5><ul><li>The neural network is defined by its parameters $ \theta $, which are refined during training.</li><li>The loss function $ L(\theta) $ aims to minimize the discrepancy between the estimated noise $ \epsilon $ and the noise predicted by the network $ \theta(x_t | t) $.</li><li>By minimizing this loss, the network is trained to predict the noise added to $ x_0 $ to produce $ x_t $.</li></ul><h4 id="Score-Function"><a href="#Score-Function" class="headerlink" title="Score Function:"></a>Score Function:</h4><ul><li>Estimating $ \epsilon $ is tantamount to calculating the scaled gradient of the log density of $ x_t $ with respect to itself, scaled by $ \frac{1}{1 - \bar{\alpha}_t} $.</li><li>This score function denotes the most significant increase direction in the log probability density of the noisy data.</li><li>Simply put, it indicates the direction from the noisy data $ x_t $ towards the original data $ x_0 $.</li></ul><p>To delve deeper into the neural network training and the role of the loss function:</p><ul><li>The neural network $ \theta(x_t | t) $ is time-conditional, altering its behavior based on the timestep $ t $, which allows for the accommodation of the varying levels of noise in $ x_t $.</li><li>The loss function $ L(\theta) $ is explicitly designed to make $ \theta(x_t | t) $ an effective estimator of the noise $ \epsilon $ that was incorporated into $ x_0 $ at the time $ t $ during the forward process. Training the network to minimize this loss effectively instructs it on reversing the diffusion process.</li><li>The score function $ \nabla_{x_t} \log q_t(x_t | x_0) $ allows the network‚Äôs output to be directly associated with the data‚Äôs probability density gradients. This gradient, also known as the score, informs us on adjusting $ x_t $ to enhance its likelihood under the data distribution, which is fundamental to generating representative samples of the original data distribution.</li></ul><p>In practice, this methodology enables generative models to create samples from intricate distributions by methodically refining noise into structured data. The neural network is trained to counteract the noise introduced in the diffusion process, thus efficiently generating new data samples from the target distribution.</p><p>The entire training process includes the following iterative steps:</p><ul><li>Begin with a sample $ x_0 $ from the true data distribution.</li><li>Introduce noise to generate $ x_t $ following the forward diffusion process.</li><li>Employ the neural network to estimate the noise $ \epsilon $.</li><li>Adjust the neural network parameters $ \theta $ to minimize the disparity between the estimated and the actual noise, in adherence to the loss function.</li></ul><p>Upon successful training, the neural network is capable of generating new data samples by initiating from noise and applying the learned reverse process.</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] <a href="https://arxiv.org/abs/2205.09853">MCVD: Masked Conditional Video Diffusion for Prediction, Generation, and Interpolation</a></p>]]></content:encoded>
      
      <comments>https://minkyuchoi-07.github.io/2024/03/16/distill-diffusion/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Neuro Symbolic Video Search with Temporal Logic</title>
      <link>https://minkyuchoi-07.github.io/2024/03/16/neuro-symbolic-video-search/</link>
      <guid>https://minkyuchoi-07.github.io/2024/03/16/neuro-symbolic-video-search/</guid>
      <pubDate>Sat, 16 Mar 2024 23:15:50 GMT</pubDate>
      <description>
      
        &lt;p&gt;&lt;a href=&quot;https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/10038.pdf&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Paper-PDF-green.svg&quot; alt=&quot;Paper&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2403.11021&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2403.11021-b31b1b.svg&quot; alt=&quot;arXiv&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://utaustin-swarmlab.github.io/nsvs-project-page.github.io/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/ProjectWebpage-NSVS--TL-orange.svg&quot; alt=&quot;Website&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/UTAustin-SwarmLab/Neuro-Symbolic-Video-Search-Temporal-Logic&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Code-NSVS--TL-blue.svg&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/UTAustin-SwarmLab/Temporal-Logic-Video-Dataset&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Code-TLV--Dataset-blue.svg&quot; alt=&quot;GitHub&quot;&gt;&lt;/a&gt;&lt;br&gt;&lt;strong&gt;Minkyu Choi&lt;/strong&gt;, Harsh Goel, Mohammad Omama, Yunhao, Yang, Sahil Shah, and Sandeep Chinchali&lt;br&gt;&lt;em&gt;European Conference on Computer Vision (ECCV), 2024&lt;/em&gt; &lt;span style=&quot;color: red;&quot;&gt;&lt;strong&gt;‚Äì Accepted for oral presentation!&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p><a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/10038.pdf"><img src="https://img.shields.io/badge/Paper-PDF-green.svg" alt="Paper"></a> <a href="https://arxiv.org/abs/2403.11021"><img src="https://img.shields.io/badge/arXiv-2403.11021-b31b1b.svg" alt="arXiv"></a> <a href="https://utaustin-swarmlab.github.io/nsvs-project-page.github.io/"><img src="https://img.shields.io/badge/ProjectWebpage-NSVS--TL-orange.svg" alt="Website"></a> <a href="https://github.com/UTAustin-SwarmLab/Neuro-Symbolic-Video-Search-Temporal-Logic"><img src="https://img.shields.io/badge/Code-NSVS--TL-blue.svg" alt="GitHub"></a> <a href="https://github.com/UTAustin-SwarmLab/Temporal-Logic-Video-Dataset"><img src="https://img.shields.io/badge/Code-TLV--Dataset-blue.svg" alt="GitHub"></a><br><strong>Minkyu Choi</strong>, Harsh Goel, Mohammad Omama, Yunhao, Yang, Sahil Shah, and Sandeep Chinchali<br><em>European Conference on Computer Vision (ECCV), 2024</em> <span style="color: red;"><strong>‚Äì Accepted for oral presentation!</strong></span></p><span id="more"></span><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Imagine if I asked you to locate the iconic ‚ÄúI am flying‚Äù scene from the 3-hour-long Titanic movie. This scene is a complex symphony of multiple semantic events and their long-term temporal relations. Modern state-of-the-art (SOTA) activity recognition networks, which couple semantic reasoning and temporal logic, surprisingly fail at long-term reasoning across frames. Is there a way to decouple the two for effective long-term video understanding? </p><div style="text-align: center; display: flex; justify-content: center; align-items: center; min-height: 20vh; flex-direction: column;">  <span style="max-width: 90%; max-height: 70%; height: auto;">    <img src="teaser_2.gif" alt title="Figure 5" style="width: 100%; height: auto;">  </span>  <span style="max-width: 70%; text-align: left; display: block; margin-top: 10px;">  </span></div><br><p><strong>Introducing NSVS: Neuro-Symbolic Video Search</strong>. Our recent paper, accepted at ECCV 2024, tackles this problem and outperforms competing baselines by 9-15% on state-of-the-art datasets such as Waymo and NuScenes.</p><h2 id="Why-do-we-need-long-term-reasoning-in-videos"><a href="#Why-do-we-need-long-term-reasoning-in-videos" class="headerlink" title="Why do we need long-term reasoning in videos?"></a>Why do we need long-term reasoning in videos?</h2><p>There has been a significant increase in video data production, with platforms such as YouTube receiving 500 hours of uploads every minute. Additionally, autonomous vehicle companies like Waymo generate 10-100 TB of data daily, and worldwide security cameras record around 500 PB daily. Consequently, we require tools with sophisticated query capabilities to navigate this immense volume of video content.</p><p>For instance, a query such as ‚ÄúFind me all scenes where event A happened, event B did not occur, and event C occurs hours later‚Äù requires advanced methods capable of long-term temporal reasoning. Such long-term reasoning is a common use case in surveillance, video analysis, and similar fields that existing video foundation models fail to address.</p><h2 id="Why-do-existing-methods-fail-at-long-term-reasoning-in-videos"><a href="#Why-do-existing-methods-fail-at-long-term-reasoning-in-videos" class="headerlink" title="Why do existing methods fail at long-term reasoning in videos?"></a>Why do existing methods fail at long-term reasoning in videos?</h2><p>Our key insight is that video foundation models intertwine per-frame perception and temporal reasoning into a single deep network. This makes it difficult for them to understand temporal nuances over the long term. Hence, decoupling but co-designing semantic understanding and temporal reasoning is essential for efficient scene identification. We propose a system that<br>leverages vision-language models for semantic understanding of individual frames but effectively reasons about the long-term evolution of events using state machines and temporal logic (TL) formulae that inherently capture memory. </p><p>The figure below shows comparative performance on the event identification tasks. The accuracy of event identification with Video Language Models (Blue/Green) drops as video length or query complexity increases. On the other hand, NSVS (Orange) shows consistent performance irrespective of video length or query complexity.</p><div style="text-align: center; display: flex; justify-content: center; align-items: center; min-height: 20vh; flex-direction: column;">  <span style="max-width: 70%; max-height: 70%; height: auto;">    <img src="figure1.png" alt title="Figure 1" style="width: 100%; height: auto;">  </span>  <span style="max-width: 70%; text-align: left; display: block; margin-top: 10px;">    <em><strong>Fig. 1 Comparative Performance on the Event Identification Task: Video Language Models versus <span style="font-style: normal;">NSVS-TL</span>.</strong> The accuracy of event identification with Video Language Models (Blue/Green) drops as video length or query complexity increases. In contrast, <span style="font-style: normal;">NSVS-TL</span> (Orange) shows consistent performance irrespective of video length or query complexity.</em>  </span></div><br><h2 id="NSVS-Demystified"><a href="#NSVS-Demystified" class="headerlink" title="NSVS - Demystified"></a>NSVS - Demystified</h2><p>We attribute the consistent performance of NSVS observed in the above figure to the decoupling of per-frame semantic understanding and temporal reasoning.  While we plug and play off-the-shelf foundation models like YOLO, CLIP, or LLAVA for semantic understanding, we build upon the massive literature on Formal Methods using state machines and temporal logic (TL) formulae for temporal reasoning.  </p><p>Formal Methods are mathematical techniques used to specify, verify, and prove the correctness of systems. Temporal logic (TL) is a subset of formal methods that describe sequences of events or states over time. TL provides a structured framework for describing and reasoning about the temporal properties of sequences or processes. It extends classical logic with temporal operators to express propositions about the flow of time. To the best of our knowledge, this is the first work to adapt TL for long-term activity recognition. Although it is not necessary to understand this blog, we recommend that readers refer to <a href="https://www.youtube.com/playlist?list=PLMBx8HjvK7672qEl6bdnXdzYEbLP_lWPw">this crash course</a> for an in-depth understanding of TL. </p><h2 id="The-NSVS-Pipeline"><a href="#The-NSVS-Pipeline" class="headerlink" title="The NSVS Pipeline"></a>The NSVS Pipeline</h2><p>Coming back to our example of locating the ‚ÄúI am flying‚Äù scene from the 3-hour-long Titanic movie, how does NSVS solve it? The query ‚ÄúI‚Äôm flying‚Äù is first decomposed into semantically meaningful atomic propositions such as ‚Äúman hugging woman‚Äù, ‚Äúship on the sea‚Äù, and ‚Äúkiss‚Äù from a high-level user query. SOTA vision and vision-language models are then employed to annotate the existence of these atomic propositions in each video frame. Subsequently, we construct an automaton or state machine that models the video‚Äôs temporal evolution based on the list of per-frame atomic propositions detected in the video. Finally, we evaluate when and where this automaton satisfies the user‚Äôs query. This also provides confidence measures through formal verification which enables the user to further assess the specific scenes pertaining to a complex query in a long video. We further assess this pipeline for long-term reasoning in videos for queries with varying complexity on a suite of experiments.</p><div style="text-align: center; display: flex; justify-content: center; align-items: center; min-height: 20vh; flex-direction: column;">  <span style="max-width: 70%; max-height: 70%; height: auto;">    <img src="figure2.png" alt title="Figure 2" style="width: 100%; height: auto;">  </span>  <span style="max-width: 70%; text-align: left; display: block; margin-top: 10px;">    <div>      <strong>Fig. 2 NSVS-TL Pipeline.</strong> The input query ‚Äî ‚ÄúFind the ‚ÄòI‚Äôm Flying‚Äô scene from Titanic‚Äù ‚Äî is first decomposed into semantically meaningful atomic propositions such as ‚Äúman hugging woman‚Äù, ‚Äúship on the sea‚Äù, and ‚Äúkiss‚Äù from a high-level user query. SOTA vision and vision-language models are then employed to annotate the existence of these atomic propositions in each video frame. Subsequently, we construct a probabilistic automaton that models the video‚Äôs temporal evolution based on the list of per-frame atomic propositions detected in the video. Finally, we evaluate when and where this automaton satisfies the user‚Äôs query. We do this by expressing it in a formal specification language that incorporates temporal logic. The TL equivalent of the above query is ALWAYS (‚òê) ‚Äúman hugging woman‚Äù UNTIL (ùïå) ‚Äúship on the sea‚Äù UNTIL (ùïå) ‚Äúkiss‚Äù. Formal verification techniques are utilized on the automaton to retrieve scenes that satisfy the TL specification.    </div>  </span></div><br><h2 id="Long-term-Video-Understanding-Results"><a href="#Long-term-Video-Understanding-Results" class="headerlink" title="Long-term Video Understanding Results"></a>Long-term Video Understanding Results</h2><p>As shown previously, current video-language foundation models such as Video-Llama and ViCLIP excel at scene identification and description in short videos, however, they struggle with long-term and complex temporal queries. Hence, we‚Äôve crafted stronger benchmarks that couple Large Language Models (LLMs) like GPT for reasoning with per-frame annotations from a CV model. Essentially, we replace the sophisticated state machines in NSVS that reason about temporal logic queries with an LLM. This allows us to see how video length impacts scene identification performance when utilizing LLMs.</p><div style="text-align: center; display: flex; justify-content: center; align-items: center; min-height: 20vh; flex-direction: column;">  <span style="max-width: 70%; max-height: 70%; height: auto;">    <img src="figure3.png" alt title="Figure 3" style="width: 100%; height: auto;">  </span>  <span style="max-width: 70%; text-align: left; display: block; margin-top: 10px;">    <em><strong>Fig. 3  Performance of NSVS-TL Across Different Video Lengths.</strong> Illustrates the F1 scores for scene retrieval against the video length, fulfilling the ``A until B'' temporal specification.</em>  </span></div><br><p>Our comprehensive evaluations include scene identification tasks in multi-event sequences with extended temporal events. Specifically, these tasks focus on scenarios where event A persists from the beginning until event B occurs at the end. Therefore, these tasks provide crucial insights into the long-term reasoning capabilities of Large Language Models (LLMs), especially as the temporal distances between events increase. We found that while GPT-3.5 and GPT-3.5 Turbo Instruct struggle with videos longer than 500 seconds, and GPT-4‚Äôs performance declines sharply beyond 1000 seconds, our NSVS method maintains consistent accuracy even for videos up to 40 minutes long. This demonstrates NSVS‚Äôs robust capability in handling complex, temporally extended video content, potentially opening new avenues for video analysis and understanding.</p><div style="text-align: center; display: flex; justify-content: center; align-items: center; min-height: 20vh; flex-direction: column;">  <span style="max-width: 90%; max-height: 70%; height: auto;">    <img src="figure4.png" alt title="Figure 4" style="width: 100%; height: auto;">  </span>  <span style="max-width: 70%; text-align: left; display: block; margin-top: 10px;">    <em><strong>Fig. 4 Comparative Performance of NSVS-TL Across Complex Temporal Logic Specifications.</strong> Demonstrate NSVS-TL‚Äôs performance to benchmarks across different TL specifications.</em>  </span></div><br><h2 id="The-TLV-Datasets"><a href="#The-TLV-Datasets" class="headerlink" title="The TLV Datasets"></a>The TLV Datasets</h2><p>Existing datasets comprise video annotations for events across short durations. To address this gap in state-of-the-art video datasets for temporally extended activity, we introduce the Temporal Logic Video (TLV) datasets. These datasets come in two flavors: synthetic and real-world. Our synthetic TLV datasets are crafted by cleverly stitching together static images from popular collections like COCO and ImageNet, allowing us to inject a wide array of temporal logic specifications. We‚Äôve also created two video datasets with TL specifications based on real-world autonomous vehicle driving footage from NuScenes and Waymo open-source datasets. We believe that the proposed datasets would enable researchers to benchmark their methods for long-term video understanding and temporal reasoning tasks.</p><div style="text-align: center; display: flex; justify-content: center; align-items: center; min-height: 20vh; flex-direction: column;">  <span style="max-width: 90%; max-height: 70%; height: auto;">    <img src="figure5.png" alt title="Figure 5" style="width: 100%; height: auto;">  </span>  <span style="max-width: 70%; text-align: left; display: block; margin-top: 10px;">  </span></div><br><h2 id="More-on-NSVS-TL"><a href="#More-on-NSVS-TL" class="headerlink" title="More on NSVS-TL"></a>More on NSVS-TL</h2><p>For more information, come see us at the upcoming ECCV 2024 conference. You can find the paper <a href="https://arxiv.org/pdf/2403.11021">here</a>, the <a href="https://utaustin-swarmlab.github.io/nsvs-project-page.github.io/">project webpage</a>, and play with our open-sourced <a href="https://github.com/UTAustin-SwarmLab/Temporal-Logic-Video-Dataset">datasets</a> and <a href="https://github.com/UTAustin-SwarmLab/Neuro-Symbolic-Video-Search-Temporal-Logic">code</a>.</p><h2 id="Citation"><a href="#Citation" class="headerlink" title="Citation"></a>Citation</h2><figure class="highlight plaintext hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">@inproceedings&#123;</span><br><span class="line">@inproceedings&#123;Choi_2024_ECCV,</span><br><span class="line">  author=&#123;Choi, Minkyu and Goel, Harsh and Omama, Mohammad and Yang, Yunhao and Shah, Sahil and Chinchali, Sandeep&#125;,</span><br><span class="line">  title=&#123;Towards Neuro-Symbolic Video Understanding&#125;,</span><br><span class="line">  booktitle=&#123;Proceedings of the European Conference on Computer Vision (ECCV)&#125;,</span><br><span class="line">  month=&#123;September&#125;,</span><br><span class="line">  year=&#123;2024&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      <comments>https://minkyuchoi-07.github.io/2024/03/16/neuro-symbolic-video-search/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Distill Neuro-Symbolic AI</title>
      <link>https://minkyuchoi-07.github.io/2023/08/18/distill-neuro-symbolic-ai/</link>
      <guid>https://minkyuchoi-07.github.io/2023/08/18/distill-neuro-symbolic-ai/</guid>
      <pubDate>Fri, 18 Aug 2023 23:08:50 GMT</pubDate>
      <description>
      
        &lt;p&gt;Distilling Neuro-Symbolic AI&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>Distilling Neuro-Symbolic AI</p><span id="more"></span><h1 id="What-is-Neuro-Symbolic-AI"><a href="#What-is-Neuro-Symbolic-AI" class="headerlink" title="What is Neuro-Symbolic AI"></a>What is Neuro-Symbolic AI</h1><p>Neuro-Symbolic AI Problem Statement: In the evolving landscape of Artificial Intelligence, there exists a pressing need for system capable of learning from experience and subsequently reasoning about the acquired knowledge from uncertain environments. These systems, inspired by temporal logic, should be adept at handling changes in their environment, emphasizing the computational efficiency of their learning processes. Central to their reasoning capabilities are the foundational principles of abduction, deduction, and induction, which collectively form the cornerstone of a neuro-symbolic AI approach.</p>]]></content:encoded>
      
      <comments>https://minkyuchoi-07.github.io/2023/08/18/distill-neuro-symbolic-ai/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Multi-Agent Reinforcement Learning with Epistemic Priors</title>
      <link>https://minkyuchoi-07.github.io/2023/07/01/multi-agent-reinforcement-learning-with-epistemic-priors/</link>
      <guid>https://minkyuchoi-07.github.io/2023/07/01/multi-agent-reinforcement-learning-with-epistemic-priors/</guid>
      <pubDate>Sat, 01 Jul 2023 04:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;&lt;a href=&quot;https://openreview.net/pdf?id=5cWF3p2jDi&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Paper-PDF-green.svg&quot; alt=&quot;Paper&quot;&gt;&lt;/a&gt;&lt;br&gt;Thayne T. Walker, Jaime S. Ide, &lt;strong&gt;Minkyu Choi&lt;/strong&gt;, Michael John Guarino, and Kevin Alcedo.&lt;br&gt;&lt;em&gt;International Conference on Control, Decision and Information Technologies (CoDit), 2023&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The coordination of multiple autonomous agents is essential for achieving collaborative goals efficiently, especially in environments with limited communication and sensing capabilities. Our recent study, presented at CoDIT 2023, explores a novel method to tackle this challenge. We introduce &lt;strong&gt;Multi-Agent Reinforcement Learning with Epistemic Priors (MARL-EP)&lt;/strong&gt;, a technique that leverages shared mental models to enable high-level coordination among agents, even with severely impaired sensing and zero communication.&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p><a href="https://openreview.net/pdf?id=5cWF3p2jDi"><img src="https://img.shields.io/badge/Paper-PDF-green.svg" alt="Paper"></a><br>Thayne T. Walker, Jaime S. Ide, <strong>Minkyu Choi</strong>, Michael John Guarino, and Kevin Alcedo.<br><em>International Conference on Control, Decision and Information Technologies (CoDit), 2023</em></p><p>The coordination of multiple autonomous agents is essential for achieving collaborative goals efficiently, especially in environments with limited communication and sensing capabilities. Our recent study, presented at CoDIT 2023, explores a novel method to tackle this challenge. We introduce <strong>Multi-Agent Reinforcement Learning with Epistemic Priors (MARL-EP)</strong>, a technique that leverages shared mental models to enable high-level coordination among agents, even with severely impaired sensing and zero communication.</p><span id="more"></span><h2 id="Problem-Definition"><a href="#Problem-Definition" class="headerlink" title="Problem Definition"></a>Problem Definition</h2><p>Imagine a scenario where multiple autonomous agents need to navigate from their respective starting positions to specific goal locations. The challenge intensifies when these agents cannot communicate with each other and have limited sensing capabilities, leading to potential collisions or inefficient goal achievement. This problem is prevalent in various applications, including warehouse logistics, firefighting, surveillance, and transportation.</p><h3 id="Motivating-Example"><a href="#Motivating-Example" class="headerlink" title="Motivating Example"></a>Motivating Example</h3><p>Consider a cooperative navigation problem where agents must move from their start states to goal states without colliding with each other or obstacles. Traditional methods rely on accurate state information and communication, but what happens when these are unavailable? Figure 1 in our paper illustrates such a scenario where two agents must navigate to their goals without knowing each other‚Äôs real-time positions. If each agent independently chooses the shortest path, they will collide. However, with shared knowledge of each other‚Äôs goals and an understanding of common conventions, they can avoid collisions and navigate efficiently.</p><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><h3 id="Multi-Agent-Reinforcement-Learning"><a href="#Multi-Agent-Reinforcement-Learning" class="headerlink" title="Multi-Agent Reinforcement Learning"></a>Multi-Agent Reinforcement Learning</h3><p>Multi-Agent Reinforcement Learning (MARL) involves training multiple agents to make decisions that maximize a cumulative reward. This is often modeled using Decentralized Partially Observable Markov Decision Processes (DEC-POMDPs). These processes account for the fact that agents have only partial information about the environment and must make decisions based on this limited view. Traditional MARL approaches face significant challenges when communication is restricted or when sensing is limited.</p><div style="text-align: center; display: flex; justify-content: center; align-items: center; min-height: 20vh; flex-direction: column;">  <span style="max-width: 70%; max-height: 70%; height: auto;">    <img src="/2023/07/01/multi-agent-reinforcement-learning-with-epistemic-priors/figure1.png" alt title="Figure 1" style="width: 100%; height: auto;">  </span>  <span style="max-width: 70%; text-align: left; display: block; margin-top: 10px;">    <em>Figure 1: <strong> (a) An example instance of a cooperative navigation problem and (b) a solution for the problem instance.</strong></em>  </span></div><br><h3 id="Epistemic-Logic"><a href="#Epistemic-Logic" class="headerlink" title="Epistemic Logic"></a>Epistemic Logic</h3><p>Epistemic logic deals with reasoning about knowledge and beliefs. It allows agents to estimate the knowledge of other agents and predict their actions based on this estimation. By incorporating epistemic logic into MARL, we enable agents to infer unobservable parts of the environment, thereby making more informed decisions.</p><h2 id="Reinforcement-Learning-with-Epistemic-Priors"><a href="#Reinforcement-Learning-with-Epistemic-Priors" class="headerlink" title="Reinforcement Learning with Epistemic Priors"></a>Reinforcement Learning with Epistemic Priors</h2><p>Our approach, MARL-EP, integrates epistemic priors into the decision-making process of each agent. These priors serve as a shared mental model, helping agents infer the unobservable parts of the environment and coordinate their actions.</p><h3 id="Convention-of-Operation"><a href="#Convention-of-Operation" class="headerlink" title="Convention of Operation"></a>Convention of Operation</h3><p>A convention of operation is a set of predefined rules or protocols that guide agents‚Äô actions to achieve coordination. For example, in traffic systems, the convention might be to drive on the right side of the road. By following such conventions, agents can predict each other‚Äôs actions even without direct communication.</p><h3 id="Epistemic-Blueprints"><a href="#Epistemic-Blueprints" class="headerlink" title="Epistemic Blueprints"></a>Epistemic Blueprints</h3><p>In the MARL-EP architecture, each agent uses a deterministic multi-agent planner to generate an epistemic blueprint, a complete multi-agent plan. This blueprint guides the agent‚Äôs actions, assuming that all agents have identical plans and follow the same conventions. This method allows agents to coordinate implicitly, leveraging shared knowledge and conventions to achieve their goals.</p><h3 id="Multi-Agent-RL-with-Epistemic-Priors"><a href="#Multi-Agent-RL-with-Epistemic-Priors" class="headerlink" title="Multi-Agent RL with Epistemic Priors"></a>Multi-Agent RL with Epistemic Priors</h3><p>We employ the QMIX algorithm, which decomposes the global value function into local value functions for each agent, making the learning process more efficient. In our modified approach, we incorporate epistemic priors into the training process. These priors enhance the agents‚Äô understanding of the global state, improving coordination and overall performance.</p><div style="text-align: center; display: flex; justify-content: center; align-items: center; min-height: 20vh; flex-direction: column;">  <span style="max-width: 70%; max-height: 70%; height: auto;">    <img src="/2023/07/01/multi-agent-reinforcement-learning-with-epistemic-priors/figure2.png" alt title="Figure 2" style="width: 100%; height: auto;">  </span>  <span style="max-width: 70%; text-align: left; display: block; margin-top: 10px;">    <em>Figure 2: <strong> MARL-EP System Architecture.</strong></em>  </span></div><br><h4 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h4><p>Here‚Äôs a simplified version of our algorithm:</p><ol><li><strong>Initialize</strong> the parameters for the mixing network, agent networks, and hypernetwork.</li><li><strong>Estimate epistemic priors</strong> for each agent at the beginning of each episode.</li><li><strong>Train agents</strong> using local observations augmented with these epistemic priors.</li><li><strong>Update model parameters</strong> iteratively based on the observed rewards and transitions.</li></ol><h2 id="Experimental-Results"><a href="#Experimental-Results" class="headerlink" title="Experimental Results"></a>Experimental Results</h2><p>We validated our approach using the Simple-Spread task in the Multi-agent Particle Environment (MPE). This task involves multiple agents cooperating to reach specific landmarks while avoiding collisions. We tested five different scenarios to evaluate the performance of our method:</p><ol><li><strong>No sensing (baseline)</strong>: Agents have no access to other agents‚Äô locations.</li><li><strong>Limited sensing</strong>: Agents can sense nearby agents.</li><li><strong>Perfect sensing</strong>: Agents know the locations of all other agents.</li><li><strong>No sensing, with priors (QMIX-EP)</strong>: Similar to baseline but with estimated locations of other agents.</li><li><strong>Limited sensing, with priors (QMIX-EP)</strong>: Limited sensing augmented with estimated locations.</li></ol><div style="text-align: center; display: flex; justify-content: center; align-items: center; min-height: 20vh; flex-direction: column;">  <span style="max-width: 70%; max-height: 70%; height: auto;">    <img src="/2023/07/01/multi-agent-reinforcement-learning-with-epistemic-priors/figure3.png" alt title="Figure 3" style="width: 100%; height: auto;">  </span>  <span style="max-width: 70%; text-align: left; display: block; margin-top: 10px;">    <em>Figure 3: <strong>MPE: Simple-spread task. Three agents cooperate to reach the three landmarks as quick as possible, while avoiding collisions. </strong></em>  </span></div><br><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p>The results, depicted in Figure 4 of our paper, show significant improvements in performance with the use of epistemic priors. In scenarios with no or limited sensing, MARL-EP achieved performance levels close to those with perfect sensing. This demonstrates the effectiveness of using epistemic priors for enhancing coordination among agents.</p><div style="text-align: center; display: flex; justify-content: center; align-items: center; min-height: 20vh; flex-direction: column;">  <span style="max-width: 70%; max-height: 70%; height: auto;">    <img src="/2023/07/01/multi-agent-reinforcement-learning-with-epistemic-priors/figure4.png" alt title="Figure 4" style="width: 100%; height: auto;">  </span>  <span style="max-width: 70%; text-align: left; display: block; margin-top: 10px;">    <em>Figure 4: <strong>Evaluation of trained QMIX agents for different cases. </strong></em>  </span></div><br><h2 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h2><p>Our study demonstrates that integrating epistemic priors into multi-agent reinforcement learning can significantly enhance coordination in environments with limited sensing and communication. By leveraging shared mental models and conventions of operation, agents can infer the actions of others and achieve high levels of coordination without direct communication.</p><p>Future work will focus on applying this approach in real-world scenarios and exploring real-time updates to the epistemic blueprints. We believe that MARL-EP holds great potential for advancing the capabilities of autonomous multi-agent systems in various applications.</p><p>For those interested in the technical details and further results, we encourage you to read our full paper presented at CoDIT 2023.</p><h2 id="Citation"><a href="#Citation" class="headerlink" title="Citation"></a>Citation</h2><figure class="highlight plaintext hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">@inproceedings&#123;</span><br><span class="line">walker2023multiagent,</span><br><span class="line">title=&#123;Multi-Agent Reinforcement Learning with Epistemic Priors&#125;,</span><br><span class="line">author=&#123;Thayne T. Walker and Jaime S. Ide and Minkyu Choi and Michael John Guarino and Kevin Alcedo&#125;,</span><br><span class="line">booktitle=&#123;PRL Workshop Series &#123;\textendash&#125; Bridging the Gap Between AI Planning and Reinforcement Learning&#125;,</span><br><span class="line">year=&#123;2023&#125;,</span><br><span class="line">url=&#123;https://openreview.net/forum?id=5cWF3p2jDi&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      <comments>https://minkyuchoi-07.github.io/2023/07/01/multi-agent-reinforcement-learning-with-epistemic-priors/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Distill Reinforcement Learning</title>
      <link>https://minkyuchoi-07.github.io/2023/04/02/distill-rl/</link>
      <guid>https://minkyuchoi-07.github.io/2023/04/02/distill-rl/</guid>
      <pubDate>Sun, 02 Apr 2023 21:07:40 GMT</pubDate>
      <description>
      
        &lt;p&gt;Distilling Reinforcement Learning&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>Distilling Reinforcement Learning</p><span id="more"></span><h1 id="What-is-Reinforcement-Learning"><a href="#What-is-Reinforcement-Learning" class="headerlink" title="What is Reinforcement Learning"></a>What is Reinforcement Learning</h1><p>Consider an agent situated in an unfamiliar setting, capable of garnering rewards through engagment with its surroundings. The agent‚Äôs objective is to perform actions that optimize the accumulation of rewards. Practical examples of this concept incldue a gaming bot striving for top scors or a robot executing physical tasks with tanglible objects, although the possibilities extend beyond these instances. </p><p>The primary objective of Reinforcement Learning (RL) is to develop an effective strategy for an agent through iterative trials and straightforward feedback. By emplying the optimial strategy, the agent can dynamically adpat to the environment, thereby maximizing future rewards. </p><p>Let‚Äôs delve into some fundamental concepts in RL. </p><p>An agent operates within an environment, and the way the environment, and the way the environment reacts to specific actions is dictacted by a model, which may be known or unknown to us. The agent can occupy on of many state ($s \in S$) within the environment and choose to take one of several actions ($a \in A$) to transition between states. The transition probabilities between state (P) determine the agents‚Äô destination state. Upon taking an action, the environment offers a reward ($r \in R$) as feedback. </p><p>The model outlines the reward function and transition probabilities. Depending on whether we know the model‚Äôs working or not, there are two distnct situations. </p><p><strong>Known Model:</strong> Execute model-based RL with perfect inffomration for planning. When the environment is fully understood, the optimal solution can be found using Dynamic Programming (DP). This is reminiscent of solving problems such as the ‚Äúlongest increasing subsequence‚Äù or ‚Äútraveling salesman problem.‚Äù</p><p><strong>Unknown Model:</strong> Perform model-free RL or attempt to explicitly learn the model as part of the algorithm, given that information is incomplete. The majority of the subsequent content addresses scenarios where the model is unknown. </p><p>The agent‚Äôs policy, $\pi(s)$, offeres guidance on the optimal action to take in a specific state to maximize total rewards. Each state is linked to a value function, $V(s)$, which forecasts the anticipated amount of future rewards obtainable in that state by following the corresponding policy. In essence, the value funcution measures a state‚Äôs quality. Both policy and value functions are the targets of reinforcement learning. </p><p>The interaction between the agent and the environment involves a series of actions and observed reward over time, $t = 1,2, ‚Ä¶, T$. Throughout this process, the agent accumulates knowledge about the environment, learns the optimial policy, and decides on the next action to take to efficiently learn the best policy. Let‚Äôs denote the state, action, and reward at time step $t$ as $S_t, A_t$ and $R_t$ respectively. Therefore, the interaction sequence is fully represented by a single <strong>episode</strong> (also referred to as ‚Äútrial‚Äù or ‚Äútrajectory‚Äù) that concludes at the terminal state $S_T$:<br>$$<br>S_1, A_1,R_1,S_2,A_2,R_2,‚Ä¶,S_T<br>$$</p><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>The model serves as a representation of the environment, enabling us to understand or deduce how the environment interacts with and offers feedback to the agent. The model consists of two primary components: the transition probability function $P$ and the reward function $R$.</p><p>Consider a situation where we are in state $s$ and decide to take action $a$, leading to the next state <code>s&#39;</code> and receiving reward <code>r</code>. This single transtion step is denoted by the tuple $(s, a, s‚Äô, r)$.</p><p>The transition function $P$ documents the likelihood of transitioning from state <code>s</code> to <code>s&#39;</code> upon taking action <code>a</code> and acquiring reward $r$. $\mathbb{P}$ symbolizes ‚Äúprobability‚Äù in this context.<br>$$<br>P(s‚Äô,r|s,a) = \mathbb{P}[S_{t+1}=s‚Äô, R_{t+1}=r|S_t=s, A_t=a]<br>$$<br>Thus, the state=transition function can be defined as a function of $P(s‚Äô, r|s,a):$<br>$$<br>P_{ss‚Äô}^a =P(s‚Äô|s,a)=\mathbb{P}[S_{t+1}=s‚Äô,|S_t=s, A_t=a]=\sum_{r \in R}P(s‚Äô,r|s,a)<br>$$<br>The reward function R is obtained by the next reward triggered by an action:<br>$$<br>R(s,a) = \mathbb{E}[R_{t+1}|S_t=s, A_t=a]=\sum_{r \in R}r\sum_{s‚Äô\in S}P(s‚Äô,r|s,a)<br>$$<br><strong>Model-Based vs Model-free:</strong></p><p>Model-based methods rely on constructing and utilizing a model of the environment to plan and make decions, while model-free methods learn the optimal policy directly from the agent‚Äôs experiences without building an explicit model. Each approach has its own advantages and disadvantages, depending on the problem and environment at hand</p><ul><li><strong>Model-based:</strong> It relies on building an explicit model of the environment, which includes transition probabilities (how the environment changes with actions) and reward functions. It uses the model to plan and make decisions, siulating potential future scenarios to find the optimal policy. It can be more sample-efficient, as it leverages the environment‚Äôs model to learn faster and require fewer interactions with the environment. However, building an accurate model can be challenging, particularly for complex environments with large state and action spaces. Example: Dynamic Programming techniques like Value Iteration and Policy Iteration</li><li><strong>Model-free:</strong> It does not rely on explicit model of the environment; instead, it learns the optimal policy directly from the agent‚Äôs experiences (state transitions and rewards). It learns through trial-and-error, using techniques like Temporal Difference learning or Monte Carlo methods. This method can be more straightforward to implement, as they do not require an explicit model of the environment, making them suitable for complex and high-dimensional environments. However, they might require more interations with the environment to learn the optimal policy, making them less sample-efficient. Example: Q-Learning, SARSA, and Actor-Critic algorithms. </li></ul><h2 id="Policy"><a href="#Policy" class="headerlink" title="Policy"></a>Policy</h2><p>A policy, denoted as the agent‚Äôs behavior function $\pi$, provides guidance on which action to take in a given state $s$. It represents a mapping from state $s$ to action $a$ and can be either deterministic or stochastic:</p><ul><li>Deterministic: $\pi(s)=a$</li><li>Stochastic: $\pi(a|s)= \mathbb{P}_{\pi}[A=a|S=s]$ </li></ul><p><strong>On-policy vs Off-policy:</strong></p><p>On-policy methods learn directly from the agent‚Äôs interactions with the environment while adhearing to the current policy. In contrast, off-policy methods learn from experiences generated by a different policy, allowing for greater flexibility and more efficient learning from a diverse set of experiences.</p><ul><li><strong>On-policy:</strong> On-policy methods learn the value function and policy by using the same policy for both exploration and exploitation. Thy follow the current policy while making decisions and simultaneously update the policy based on the experiences gathered. These methods usually strike a balance between exploration and exploitation during learning.</li><li><strong>Off-policy:</strong> It separate the policy used for learning (target policy) from the policy used for exploration (behavior policy). The behavior policy is responsible for generating experiences, while the target policy is updated based on the experiences collected by the behavior policy. This separation allows off-policy methods to learn from a wider range of experiences, including historical data or experiences from other agents. </li></ul><h2 id="Value-Function"><a href="#Value-Function" class="headerlink" title="Value Function"></a>Value Function</h2><p>The value function represents the expected cumulative reward that an agent can obtain, starting from a specific state and following a given policy. The value function helps the agent to estimate the long-term value of each state, considering future rewards it may receive by taking actions according to the policy. It is a key concept in RL, as it helps guid the agent‚Äôs decision-making process to maximize the total reward over time.  Let‚Äôs compute the return $G_t$ starting from time $t$:<br>$$<br>G_t = R_{t+1} + \gamma R_{t+2} + ‚Ä¶ \sum_{k=0}^{\infty}\gamma R_{t+k+1}<br>$$<br>The discount factor $\gamma$ which ranges from 0 to 1, discounts future rewars for several reasons:</p><ol><li>Future reward may be more uncertain</li><li>Immediate rewards are oftern more appealing than delayed gratification, as humans tend to prefer to enjoying themselves now rather than waiting for years</li><li>Discounting offeres mathematical simplicity, as it eliminates the need to account for infinite future steps when calculating returns </li><li>It helps mitigate concerns about infinite loops in the state transition graph, ensuring the agent converges on an optimal policy</li></ol><p><strong>State-value function ($V(s)$):</strong> This function estimates the expected cumulative reward when starting from state $s$ and <code>following a particular policy</code>. It considers the value of the current state without taking into account the immediate action the agent will take.<br>$$<br>V_{\pi}(s) = \mathbb E_{\pi}[G_t|S_t=s]<br>$$<br><strong>Action-value function($Q(s,a)$):</strong> This function estimates the expectued cumulative reward when starting from state $s$, taking action $a$, and then <code>following a particular policy</code>. It considers the value of the current state-action pair, accouting for both the state and the action the agent will take.<br>$$<br>Q_{\pi}(s,a) = \mathbb E_{\pi}[G_t|S_t=s, A_t=a]<br>$$<br>Furthermore, when following the target policy $\pi$ we can leverage the probability destribution over possible actions and the Q-values to derive the state-value:<br>$$<br>V_{\pi}(s)=\sum_{a \in A}Q_{\pi}(s,a) \pi (a|s)<br>$$<br>The action advantage function, also known as the ‚ÄúA-value,‚Äù represents the difference between the action-value (Q-value) and state-value (V-value). The function quantifies the relative benefit of taking a specific acion in a given state, compared to the average value of that state following the target policy. By calculating the A-value, we can identify which actions are advantageous or disadvantageous in a particular state, guiding the agent‚Äôs decision-making process for improved performance.<br>$$<br>A_{\pi}(s,a) = Q_{\pi}(s,a)- V_{\pi}(s)<br>$$</p><h2 id="Markov-Decision-Processes"><a href="#Markov-Decision-Processes" class="headerlink" title="Markov Decision Processes"></a>Markov Decision Processes</h2><p>In more formal terms, the majority of reinforcement learning problems can be formulated as Markov Decision Processes (MDPs). All states in an MDP exhibit the ‚ÄúMarkov‚Äù property, which means that the future depends solely on the current state, rather than the entire history:<br>$$<br>\mathbb{P}[S_{t+1} | S_t] = \mathbb{P}[S_{t+1} | S_1, ‚Ä¶, S_t]<br>$$<br>In other words, given the present state, the future and the past are conditionally independent. This is because the current state contains all the necessary information to determine future outcomes. A Markove decision process can be defined as $\mathcal{M} =&lt;\mathcal{S},\mathcal{A},\mathcal{P},\mathcal{R}, \gamma&gt;$, where:</p><ul><li>$\mathcal{S}$: a set of states;</li><li>$\mathcal{A}$: a set of actions;</li><li>$\mathcal{P}$: transition probability function;</li><li>$\mathcal{R}$: reward function;</li><li>$\gamma$: the discount factor is used to account for future rewards. In an unfamilar environment, we lack complete information about the transition probabilities ($\mathcal{P}$) and reward function ($\mathcal{R}$). </li></ul><h2 id="Bellman-Equations"><a href="#Bellman-Equations" class="headerlink" title="Bellman Equations"></a>Bellman Equations</h2><p>Bellman equations are essential in RL because they provide a systematic way to decompose and solve complex decision-making problems. They underpin many widely-used RL algorithms and help acheive optimality and convergence to effective policies - more reasons:</p><p><strong>Optimal Substructure:</strong> Bellman equations exploit the fact that optimal solutions can be built from optimal-sub solutions. This property allows RL algorithms to find the best actions to take in any given state by decomposing complex problems into simpler sub-problems. </p><p><strong>Dynamic Programming:</strong> Bellman equations form the foundation of dynamic programming techinques such as Value Iteration and Policy Iteration. These technique are used to compute optimal policies by iteratively updating state values or action values until convergence.</p><p><strong>Temporal Difference Learning:</strong> Bellman equations are a key component of temporal difference learning algorithm like Q-learning and State-Action-Reward-State-Action (SARSA). These algorithms use the difference between successive state values to update the value estimates, which allows them to learn online and incrementally from experience.</p><p><strong>Convergence to Optimal Policies:</strong> The recursive nature of the Bellman equations ensures that algorithms based on them can find optimal policies, provided certain conditions are met (like sufficient exploration). This guarantee of optimality is an attractive feature for solving complex decision-making problems. </p><p><strong>Markov Decision Processes:</strong> Bellman equations enable the modeling of RL problems as Markov Decision Processes (MDPs), where the optimal decision-making process depends only on the current state and not the history. This simplificiation allows for efficient computation and generalization across a wide range or problems. </p><p>The value function can be broken down into the immediate reward combined with the discounted future values.<br>$$<br>\begin{flalign<em>}<br>v(s) &amp; = \mathbb{E}[G_t|S_t=s] \<br>          &amp; = \mathbb{E}[R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+‚Ä¶|S_t=s] \<br>         &amp; =\mathbb{E}[R_{t+1}+\gamma(R_{t+2}+\gamma R_{t+3}+‚Ä¶)|S_t=s] \<br>         &amp; =\mathbb{E}[R_{t+1}+\gamma G_{t+1}|S_t=s] \<br>         &amp; =\mathbb{E}[R_{t+1}+\gamma V(S_{t+1})|S_t=s] \<br>\end{flalign</em>}<br>$$</p><p>$$<br>\begin{flalign<em>}<br>Q(s,a) &amp; =\mathbb{E}[R_{t+1}+\gamma V(S_{t+1})|S_t=s, A_t=a] \<br>&amp; = \mathbb{E}[R_{t+1}+\gamma \mathbb{E}<em>{a \backsim \pi}Q(S</em>{t+1}, a) | S_t=s, A_t = a]<br>\end{flalign</em>}<br>$$<br>The above recursive update process can be further separated into equations based on both state-value and action-value functions. As we progress through future action steps, we alternately extend $V$ and $Q$ by adhearing to the policy  $\pi$</p><h1 id="Meta-Reinforcement-Learning"><a href="#Meta-Reinforcement-Learning" class="headerlink" title="Meta Reinforcement Learning"></a>Meta Reinforcement Learning</h1><p>A robust meta-learning model should effectively generalize to novel tasks or environments, even those it hasn‚Äôt encountered during training. Minimal adaptation should occur during testing with limited exposure. Ideally, the model should self-adjust its internal hidden states to learn without requiring explicit fine-tuning (no gradient backpropagation on trainable variables). </p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p><a href="https://lilianweng.github.io/posts/2018-04-08-policy-gradient/">https://lilianweng.github.io/posts/2018-04-08-policy-gradient/</a></p><p><a href="https://towardsdatascience.com/policy-gradients-in-reinforcement-learning-explained-ecec7df94245">https://towardsdatascience.com/policy-gradients-in-reinforcement-learning-explained-ecec7df94245</a></p><p><a href="https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b">https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b</a></p>]]></content:encoded>
      
      <comments>https://minkyuchoi-07.github.io/2023/04/02/distill-rl/#disqus_thread</comments>
    </item>
    
    <item>
      <title>FeUdal Networks for Hierarchical Reinforcement Learning</title>
      <link>https://minkyuchoi-07.github.io/2023/03/20/feudal/</link>
      <guid>https://minkyuchoi-07.github.io/2023/03/20/feudal/</guid>
      <pubDate>Mon, 20 Mar 2023 02:04:23 GMT</pubDate>
      <description>
      
        &lt;p&gt;The paper introduces a hierarchical reinforcement learning framework called Feudal Networks (FuN) that enables agents to learn a hierarchy of temporal abstractions. The proposed model consists of a Manager and a Worker, with the Manager learning high-level policies and the Worker learning low-level policies. The approach is demonstrated to achieve state-of-the-art performance on a range of challenging tasks in the Atari domain.&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>The paper introduces a hierarchical reinforcement learning framework called Feudal Networks (FuN) that enables agents to learn a hierarchy of temporal abstractions. The proposed model consists of a Manager and a Worker, with the Manager learning high-level policies and the Worker learning low-level policies. The approach is demonstrated to achieve state-of-the-art performance on a range of challenging tasks in the Atari domain.</p><span id="more"></span><h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><p>FeUda Network (FuN) is a hierarchincal reinforcement learning method inspired by the feudal reinforcement learning proposal of <a href="https://dl.acm.org/doi/10.5555/2987061.2987095">Dayan and Hinton</a>. The need for hierarchical reinforcement learning arises because most of the State-Of-The-Art (SOTA) reinforcement learning algorithms are focused on task specifications and are lmited to low-level executions, which receive states and execute actions. They do not perform any high-level planning or goal selection based on state information. Hierarchical reinforcement learning overcomes this limitation by introducing higher-level policies or networks that plan and selects goals while lower-level policies execute actions accordingly.</p><p>FuN employs two modules: the Manager and the Worker. The Manager operates at a lower temporal resolution, receiving states and setting abstract goals that are conveyed to and enacted by the Worker <a href="https://arxiv.org/pdf/1703.01161.pdf">[1]</a>. In contrast, the Worker generates primitive actions by observing the environment at every tick. By decoupling the Manager and the Worker, FuN facilitates long timescale credit assignment and the emergence of sub-policies associated with different goals set by the Manager. This enables FuN to outperform strong baseline agents on tasks that require long-term credit assignment or memorization.</p><h1 id="The-model"><a href="#The-model" class="headerlink" title="The model"></a>The model</h1><p>FuN is a modular neural network consisting of two modules - the Worker and the Manager <a href="https://arxiv.org/pdf/1703.01161.pdf">[1]</a>. Let‚Äôs go little bit in depth on the role of two modules.<br><img src="/2023/03/19/feudal/figure_1_schematic_illustration.png" alt="figure_1: The schematic illustration of FuN"></p><p>Figure 1 illustrates the overal design and the following equation explaines the forward dynmaics of FuN:<br><img src="/2023/03/19/feudal/figure_2_eq1_rmb.png" alt="figure 2: Equation of forward dynamics of FuN"></p><p>The Manager and the Worker modules of FuN share a perceptual module that computes a shared representation $z_t$ based on environment observation $x_t$. The Manager‚Äôs main objective is to generate a goal given states, while the Worker produces primitive actions conditioned on external observation, internal state, and the goal from the Manager. The goals $g_t$ are trained using an approximate transition policy gradient, which exploits the knowledge that the Worker‚Äôs behavior will align with the goal defined by the Manager. The goals are then converted to embedded vectors $w_t$ via a linear transform $œÜ$, which are used to train the Worker and eventually produce the policy ‚Äì a vector of probabilities over primitive actions.</p><p>The Worker is trained through intrinsic reward to generate actions that move towards the goal directions to be achieved. The Worker‚Äôs RNN produces $U_t$ for the policy, which is computed from a product between $U_t$ and $w_t$. This training approach enables FuN to learn a hierarchy of sub-policies associated with different goals, and facilitates long-term credit assignment, allowing the agent to outperform a strong baseline agent on tasks involving long-term credit assignment or memorization.</p><h1 id="Goal-Embedding"><a href="#Goal-Embedding" class="headerlink" title="Goal Embedding"></a>Goal Embedding</h1><p>Computing the goal embedding is one of novelties of this paper. Essentially, it modulates the policy via ‚Äúa multiplicative interaction in a low dimensional goal-embedding space.‚Äù As a reminder, the embedded goal $w_t$ is obtained by a linear transformation from $R^d$ to $R^k$, where the last $c$ goals are first pooled by summation and then embedded into a vector $w_t$ to ensure that the embedded vector can incorporate multiple goals.  By having a multiplicative interaction between $U_t$ and $w_t$ the policy can be modulated to incorporate different goal directions from the Manager. </p><p>Firstly, we have to pool the last $c$ goals and do summation. Due to pooling the goals over several time-steps, the conditioning from the Manager varies smoothly. The reason for pooling the last $c$ goals is to create a summary of the recent sub-goals that the Manager has selected. This summary allows the Worker to have a sense of the overall direction the Manager wants to pursue.</p><p>And then we need to embed the pooled goals into vector $w$ using linear projection $œÜ$. The projection œÜ is linear, with no biases, and is learnt with gradients coming from the Worker‚Äôs actions. ‚ÄúSince $\phi$ has no biases it can never produce a constant non-zero vector ‚Äì which is the only way the setup could ignore the Manager‚Äôs input. It makes sure that the goal output by the Manager always influences the final policy.‚Äù <a href="https://arxiv.org/pdf/1703.01161.pdf">[1]</a>. By not including any bias terms, the linear projection function $\phi$ ensures that the output vector is never shifted away from zero. This is important because if $\phi$ did include bias terms, it could potentially produce a non-zero vector even if the input vector from the Manager was zero. In this case, the Worker‚Äôs policy would not be influenced by the Manager‚Äôs input</p><p>Lastly, the Worker‚Äôs embedding matrix U is then combined with the goal embedding w via a matrix-vector product </p><h1 id="Learning"><a href="#Learning" class="headerlink" title="Learning"></a>Learning</h1><p>The whole learning process is following a standard reinforcement learning set up. The agent selects the action given observations from the environment and receives reward for the selected actions. The goal of the agent is to maximize the discounted return. The agent‚Äôs behaviour is defined by its action-selection policy $\pi$. </p><p>However,  FuN propose ‚Äúinstead to independently train the Manager to predict advantageous directions (transitions) in state space and to intrinsically reward the Worker to follow these directions.‚Äù If the Worker can fulfil the goal of moving  these directions (as it is rewarded for doing), then we should end up taking advantageous trajectories through state space. </p><p>The Worker network receives two inputs - the current stat of the environment and a goal vector selected by the Manager. Based on these inputs, The Worker network provides an output action to be executed by the agent in the environment. The primary objective of the Worker network is to acquire a policy that maximizes the intrinsic reward, which is determined based on the progress made towards the present goal vector.</p><p>The FuN architecture uses policy gradient training to learn the policy, which involves computing gradients of the expected reward with respect to the network parameters of the embedded goal from the Manager and using them to update the network parameters</p><p>The issue with computing these gradients in the FuN architecture is that the state representation used by the Worker network depends on the network parameters as well as world observations from the environment. This presents a challenge, as unlike the conventional reinforcement learning process that trains end-to-end through gradient descent on either the ‚ÄúPolicy‚Äù directly or via TD-learning, FuN takes a different approach. Specifically, the goal from the Manager would lose any semantic meaning if it were incorporated as an input to the Worker network, so FuN treats these goals as internal latent variables of the model. The FeUdal Network architecture overcomes the dependence of the state representation on the network parameters when computing the gradient of the cosine similarity between the state representation and the goal vector by assuming that the state representation is fixed and does not change with the network parameters.</p><p>By doing this, the FeUdal Network architecture is able to utilize the cosine similarity between the state representation and the goal vector to provide the Worker network with a useful learning signal without encountering any issues with the chain rule of calculus.</p><p>The update rule (gradient) of the goal from the Manger is as follow:<br>$$<br>\nabla g_t = A_t^M \nabla_\theta d_{cos} (s_{t+c}-s_t,g_t(\theta)),<br>$$<br>where $A_t^M = R_t - V_t^M(x_t,\theta)$ is the Manager‚Äôs advantage function, coputed using a value function estimate $V_t^M(x_t,\theta)$ from the internal critic.</p><p>The definition of intrinsic reward that incentivizes the Wroker to pursue the specified goals is as follows:<br>$$<br>r_t^I = 1/c \sum_{i=1}^cd_{cos}(s_t-s_{t-i}, g_{t-i})<br>$$<br>The paper suggests that incorporating directions (latent goals) enables the Worker network to make directional shifts in the latent space without assuming arbitrary absolute locations, making it a more practical approach for high-dimensional and continuous latent spaces where exploring all possible locations would be difficult.</p><p>In fact, the feudal reinforcement learning formulation by (<a href="https://dl.acm.org/doi/10.5555/2987061.2987095">Dayan and Hinton in 1993</a>) proposed completely hiding the reward signal from lower levels of hierarchy in the learning process. However, this paper, a softer approach of adding an instrinsic reward for following the goals to the environment reward was taken. The Worker, therfore, is trained to maximise a weighted sum $R_t+\alpha R_t^I$ , where $\alpha$ is a hyperparameter that regulates the influence of the intrinsic reward. A large $\alpha$ value would give more influence to the Worker than the task-specific policies.</p><p>At the end, the task specific policy $\pi$ can be trained to maximize intrinsic reward from a conventional deep reinforcement learning such as actor critic (<a href="https://arxiv.org/abs/1611.05397">Mnih et al., 2016</a>):<br>$$<br>\nabla\pi_t=A_t^D \nabla_\theta \log \pi(a_t|x_t;\theta),<br>$$<br>where $A_t^D = (R_t + \alpha R_t^I - V_t^D(x_t;\theta))$ a.k.a. the Advantage function is calculated using an internal critic, which estimates the value functions for both rewards. Importantly, the Worker and Manager have different discount factors $\lambda$ for comuting the return since the Worker can be more greedy and focus on immediate rewards while the Manager can put efforts on a long-term planning. </p><h1 id="Transition-Policy-Gradients"><a href="#Transition-Policy-Gradients" class="headerlink" title="Transition Policy Gradients"></a>Transition Policy Gradients</h1><p>The transition policy refer to to the meathod of transitioning from one policy to another policy. A hight-level policy $o_t = \mu(s_t,\theta)$ selects amoung sub-policies (possibly from a continuous set) where it‚Äôs only used for fixed behaviours - remember goal is lasting for c steps. Then a follow-up question is how sub-policies are transitioned over to others. The paper said a transition distribution, $p(s_{t_c}|s_t,o_t)$ , the distribution of states that end up at the end of the sub policy, given the start state is used to enact next sub-policy. The transition policy, $\pi^{TP}(s_{t+c}|s_t)=p(s_{t_c}|s_t,\mu(s_t,\theta))$, is composed with the high-level policy with the transition distribution. </p><p>The paper says it‚Äôs valid to refer to this as a policy instead of a general transition distribution is becase the original MDP is isomorphic to a new MDP with policy $\pi^{TP}$ and transition function $s_{t+c} = \pi^{TP}(s_t)$ Therefore, the policy gradient theorem can be applied to the transition policy $\pi^{TP}$<br>$$<br>\nabla_{\theta \pi_t^{TP}} = \mathop{\mathbb{E}}[(R_t-V(s_t))]\nabla_{\theta} \log p(s_{t_c}|s_t,\mu(s_t,\theta)))<br>$$<br>The FuN framework offers a departure from the conventional approach of directly optimizing the policy to maximize the cumulative reward. Instead, it models and optimizes the transition policy, which is a distribution over future states given the current state and action. By modeling the transitions, the agent can bypass the complex trajectory of the Worker and directly optimize the predicted transition distribution. This is possible because, with knowledge of where the trajectories are likely to end up, we can follow the policy gradient of the predicted transition, rather than the Worker‚Äôs behavior. FuN assumes that the direction in state-space, $s_{t+c}‚àís_t$, follows a von Mises-Fisher distribution.</p><p>The von Mises-Fisher distribution is used in FuN to model the direction in state-space that the agent may transition to when executing a given sub-policy. The Manager outputs the goal vector that encodes the desired direction of transition and sets the mean direction of the distribution. Meanwhile, the concentration parameter determines how likely the agent is to transition in the direction of the goal vector versus in a random direction.</p><h1 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h1><p>The perceptual module $f^{percept}$ is a convolutional network (CNN) followed by a fully connected layer. I think the perceptual module can be modified to differenty type of networks depedning on use cases. Each convolutional and fully-connected layer is followed by a rectifier non-linerarity - this is substantially the same CNN as in, the only difference is that in the pre-processing state all colour channels are retained.</p><p><strong>Dilated LSTM</strong></p><p>The paper proposed a novel RNN architecture for the Manager, which runs at lower temporal resolution than the world observation from the environment. A dilate LSTM is analogously defined to dilated convolutional networks (Yu &amp; Koltun, 2016).</p><p>It basically separte $r$  groups of sub-states or ‚Äòcores‚Äô instead of full state $h_t$. So the hidden state of Dilated LSTM is  $h= (\hat{h^i})_{i=1}^r,$ meaning that it‚Äôs composed of r separate groups. Threfore, in each timestep $t$, the network equestion is as follow:<br>$$<br>\hat{h}_t^{t\zeta r},g_t=LSTM(s_t,\hat{h} _{t-1}^{t \zeta r}; \theta^{LSTM}),<br>$$<br>where $\zeta$ denotes the modulo operation and allows us to indicate which group of cores is currently being updated. $\theta^{LSMT}$ explicitly stree that the same set of parameters governs the update for each or the $r$ groups within the dLSTM. </p><p>The advantage of this Dilated LSTM is that the $r$ groups of cores in the dLSTM can preserve the memories for long periods while the dLSTM as a whole is still processing and learning from every experience, and it can update the output at every step, which is similar to clockwork RNNs (<a href="https://arxiv.org/abs/1604.06057">Kulkarni et al., 2016</a>), however there the top level ‚Äúticks‚Äù at a fixed, slow pace, whereas the dLSTM observes all the available training data instead. In the experiments we set $r = 10$, and this was also used as the predictions horizon, $c$ <a href="https://arxiv.org/pdf/1703.01161.pdf">[1]</a></p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>In conclusion, this paper presents an interesting approach to the challenge of task selection for reinforcement learning agents, which is often a difficult and non-deterministic problem. The FeUdal Network architecture allows for end-to-end training of both the Manager and Worker modules, which is a remarkable achievement. While training the Manager module on its own is not a difficult task, the paper‚Äôs approach of training the Manager and Worker together in a holistic process is novel and promising. However, the scalability of the Manager‚Äôs role remains a potential issue, as an increasing number of goals and state dimensions may not always result in appropriate goals for the Worker. Further research is necessary to determine the effectiveness of this approach in more complex and diverse environments, as well as to investigate potential solutions for the scalability issue. Overall, the FeUdal Network architecture presents an innovative approach to reinforcement learning with potential for further exploration and refinement.</p><p>If you want to explore experiment result, please find details from the original paper. </p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><p>[1] Vezhnevets, Alexander Sasha, et al. ‚ÄúFeudal networks for hierarchical reinforcement learning.‚Äù <em>International Conference on Machine Learning</em>. PMLR, 2017.</p><p>[2] Dayan, Peter and Hinton, Geoffrey E. Feudal reinforcement learning. In NIPS. Morgan Kaufmann Publishers, 1993.</p><p>[3] Jaderberg, Max, Mnih, Volodymyr, Czarnecki, Wojciech Marian, Schaul, Tom, Leibo, Joel Z, Silver, David, and Kavukcuoglu, Koray. Reinforcement learning with unsupervised auxiliary tasks. arXiv preprint arXiv:1611.05397, 2016.</p><p>[4] Kulkarni, Tejas D., Narasimhan, Karthik R., Saeedi, Ardavan, and Tenenbaum, Joshua B. Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. arXiv preprint arXiv:1604.06057, 2016.</p>]]></content:encoded>
      
      <comments>https://minkyuchoi-07.github.io/2023/03/20/feudal/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Soft Actor Critic with Inhibitory Networks for Retraining UAV Controllers Faster</title>
      <link>https://minkyuchoi-07.github.io/2022/06/01/soft-actor-critic-with-inhibitory-networks-for-retraining-uav-controllers-faster/</link>
      <guid>https://minkyuchoi-07.github.io/2022/06/01/soft-actor-critic-with-inhibitory-networks-for-retraining-uav-controllers-faster/</guid>
      <pubDate>Wed, 01 Jun 2022 04:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;&lt;a href=&quot;https://ieeexplore-ieee-org.ezproxy.lib.utexas.edu/document/9836052&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Paper-IEEE-green.svg&quot; alt=&quot;Paper&quot;&gt;&lt;/a&gt;&lt;br&gt;&lt;strong&gt;Minkyu Choi&lt;/strong&gt;, Max Filter, Kevin, Alcedo, Thayne T. Walker, David Rosenbluth, and Jaime S. Ide.&lt;br&gt;&lt;em&gt;International Conference on Unmanned Aircraft Systems (ICUAS), 2022&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The rapid evolution in autonomous unmanned aerial vehicles (UAVs) technology has spurred significant advancements in their control systems. A prominent challenge in this domain is balancing the agility of Proportional-Integral-Derivative (PID) systems for low-level control with the adaptability of Deep Reinforcement Learning (DRL) for navigation through complex environments. This post delves into a novel approach that combines these technologies to improve the retraining efficiency of UAV controllers using Soft Actor-Critic (SAC) with inhibitory networks.&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p><a href="https://ieeexplore-ieee-org.ezproxy.lib.utexas.edu/document/9836052"><img src="https://img.shields.io/badge/Paper-IEEE-green.svg" alt="Paper"></a><br><strong>Minkyu Choi</strong>, Max Filter, Kevin, Alcedo, Thayne T. Walker, David Rosenbluth, and Jaime S. Ide.<br><em>International Conference on Unmanned Aircraft Systems (ICUAS), 2022</em></p><p>The rapid evolution in autonomous unmanned aerial vehicles (UAVs) technology has spurred significant advancements in their control systems. A prominent challenge in this domain is balancing the agility of Proportional-Integral-Derivative (PID) systems for low-level control with the adaptability of Deep Reinforcement Learning (DRL) for navigation through complex environments. This post delves into a novel approach that combines these technologies to improve the retraining efficiency of UAV controllers using Soft Actor-Critic (SAC) with inhibitory networks.</p><span id="more"></span><h2 id="Introduction-to-UAV-Control-Systems"><a href="#Introduction-to-UAV-Control-Systems" class="headerlink" title="Introduction to UAV Control Systems"></a>Introduction to UAV Control Systems</h2><p>Autonomous UAVs benefit immensely from DRL due to its ability to handle nonlinear airflow effects caused by multiple rotors and to operate in uncertain environments such as those with wind and obstacles. Traditional DRL methods have shown success in training efficient UAV navigation systems. However, real-world applications often necessitate retraining these systems to adapt to new, more challenging tasks.</p><h2 id="The-Challenge-of-Retraining-in-DRL"><a href="#The-Challenge-of-Retraining-in-DRL" class="headerlink" title="The Challenge of Retraining in DRL"></a>The Challenge of Retraining in DRL</h2><p>One significant issue with traditional DRL algorithms like SAC is catastrophic forgetting, where previously learned skills are lost when the system is retrained for new tasks. This problem is especially pronounced in dynamic environments where UAVs must quickly adapt to new challenges without losing their established navigation capabilities.</p><h2 id="Introducing-Soft-Actor-Critic-with-Inhibitory-Networks-SAC-I"><a href="#Introducing-Soft-Actor-Critic-with-Inhibitory-Networks-SAC-I" class="headerlink" title="Introducing Soft Actor-Critic with Inhibitory Networks (SAC-I)"></a>Introducing Soft Actor-Critic with Inhibitory Networks (SAC-I)</h2><p>Inspired by mechanisms in cognitive neuroscience, the proposed SAC-I approach addresses this retraining challenge. Inhibitory control in neuroscience refers to the brain‚Äôs ability to modify ongoing actions in response to changing task demands. Similarly, SAC-I utilizes separate and adaptive state value evaluations along with distinct automatic entropy tuning.</p><h3 id="Key-Components-of-SAC-I"><a href="#Key-Components-of-SAC-I" class="headerlink" title="Key Components of SAC-I"></a>Key Components of SAC-I</h3><ul><li><strong>Multiple Value Functions:</strong> SAC-I employs multiple value functions that operate in a state-dependent manner. This allows the system to retain knowledge of familiar situations while adapting to new ones without forgetting previously learned skills.</li><li><strong>Inhibitory Networks:</strong> An additional value network, termed the inhibitory network, is introduced. This network specifically learns new evaluations required for novel tasks, thus preventing interference with the previously learned value network.</li><li><strong>Dual Entropy Estimation:</strong> The approach also includes estimating two distinct entropy parameters to manage the exploration-exploitation trade-off more effectively during retraining.</li></ul><h2 id="Experimental-Validation"><a href="#Experimental-Validation" class="headerlink" title="Experimental Validation"></a>Experimental Validation</h2><p>The efficacy of SAC-I was validated through experiments using a simulated quadcopter in a high-fidelity environment. The results demonstrated that SAC-I significantly accelerates the retraining process compared to standard SAC methods. Key metrics such as sample efficiency and cumulative success rates were used to benchmark the performance.</p><div style="text-align: center; display: flex; justify-content: center; align-items: center; min-height: 20vh; flex-direction: column;">  <span style="max-width: 70%; max-height: 70%; height: auto;">    <img src="/2022/06/01/soft-actor-critic-with-inhibitory-networks-for-retraining-uav-controllers-faster/figure1.png" alt title="Figure 1" style="width: 100%; height: auto;">  </span>  <span style="max-width: 70%; text-align: left; display: block; margin-top: 10px;">    <em>Figure 1: Takeoff-Target-Aviary-v0 task with obstacle. Go episode: agent starts at (0,0,0.11) and it has to reach and hover around target point at (0, 0, 0.7). Stop episode: obstacle appears randomly after the episode starts, and agent has to avoid it and reach the target.</em>  </span></div><br><h3 id="Results-Summary"><a href="#Results-Summary" class="headerlink" title="Results Summary"></a>Results Summary</h3><ul><li><strong>Faster Retraining:</strong> SAC-I agents showed a remarkable reduction in the time required for retraining, achieving new task proficiency up to five times faster than standard SAC agents.</li><li><strong>Improved Sample Efficiency:</strong> The novel approach maintained high levels of sample efficiency, crucial for practical applications where real-world data collection is expensive and time-consuming.</li></ul><div style="text-align: center; display: flex; justify-content: center; align-items: center; min-height: 20vh; flex-direction: column;">  <span style="max-width: 70%; max-height: 70%; height: auto;">    <img src="/2022/06/01/soft-actor-critic-with-inhibitory-networks-for-retraining-uav-controllers-faster/figure2.png" alt title="Figure 2" style="width: 100%; height: auto;">  </span>  <span style="max-width: 70%; text-align: left; display: block; margin-top: 10px;">    <em>Figure 2: <strong>Average reward during agents training.</strong></em>  </span></div><br><div style="text-align: center; display: flex; justify-content: center; align-items: center; min-height: 20vh; flex-direction: column;">  <span style="max-width: 70%; max-height: 70%; height: auto;">    <img src="/2022/06/01/soft-actor-critic-with-inhibitory-networks-for-retraining-uav-controllers-faster/figure3.png" alt title="Figure 3" style="width: 100%; height: auto;">  </span>  <span style="max-width: 70%; text-align: left; display: block; margin-top: 10px;">    <em>Figure 3: <strong>Cumulative success during agents training.</strong></em>  </span></div><br><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>SAC-I presents a groundbreaking advancement in the retraining of UAV controllers. By leveraging inhibitory networks inspired by cognitive control mechanisms, this approach ensures rapid adaptation to new tasks while preserving established skills. This innovation holds significant promise for real-world UAV applications, where quick adaptation to changing environments is critical.</p><h2 id="Future-Directions"><a href="#Future-Directions" class="headerlink" title="Future Directions"></a>Future Directions</h2><p>Further research will focus on extending this approach to other DRL algorithms and applying it to more complex and dynamic UAV tasks. Additionally, improving the auto-tuning methods for entropy parameters will enhance the stability and performance of SAC-I in various applications.</p><h2 id="Citation"><a href="#Citation" class="headerlink" title="Citation"></a>Citation</h2><figure class="highlight plaintext hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">@INPROCEEDINGS&#123;9836052,</span><br><span class="line">  author=&#123;Choi, Minkyu and Filter, Max and Alcedo, Kevin and Walker, Thayne T. and Rosenbluth, David and Ide, Jaime S.&#125;,</span><br><span class="line">  booktitle=&#123;2022 International Conference on Unmanned Aircraft Systems (ICUAS)&#125;, </span><br><span class="line">  title=&#123;Soft Actor-Critic with Inhibitory Networks for Retraining UAV Controllers Faster&#125;, </span><br><span class="line">  year=&#123;2022&#125;,</span><br><span class="line">  volume=&#123;&#125;,</span><br><span class="line">  number=&#123;&#125;,</span><br><span class="line">  pages=&#123;1561-1570&#125;,</span><br><span class="line">  keywords=&#123;Training;Neuroscience;Parameter estimation;Transfer learning;Reinforcement learning;Autonomous aerial vehicles;Entropy;deep reinforcement learning;soft actor-critic;transfer learning;UAV navigation;quadcopter simulation&#125;,</span><br><span class="line">  doi=&#123;10.1109/ICUAS54217.2022.9836052&#125;&#125;</span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      <comments>https://minkyuchoi-07.github.io/2022/06/01/soft-actor-critic-with-inhibitory-networks-for-retraining-uav-controllers-faster/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Boosting (AdbBoost)</title>
      <link>https://minkyuchoi-07.github.io/2020/04/02/boosting/</link>
      <guid>https://minkyuchoi-07.github.io/2020/04/02/boosting/</guid>
      <pubDate>Thu, 02 Apr 2020 02:47:52 GMT</pubDate>
      <description>
      
        &lt;p&gt;In &lt;a href=&quot;https://en.wikipedia.org/wiki/Machine_learning&quot;&gt;machine learning&lt;/a&gt;, &lt;strong&gt;boosting&lt;/strong&gt; is an &lt;a href=&quot;https://en.wikipedia.org/wiki/Ensemble_learning&quot;&gt;ensemble&lt;/a&gt; &lt;a href=&quot;https://en.wikipedia.org/wiki/Meta-algorithm&quot;&gt;meta-algorithm&lt;/a&gt; for primarily reducing &lt;a href=&quot;https://en.wikipedia.org/wiki/Supervised_learning#Bias-variance_tradeoff&quot;&gt;bias&lt;/a&gt;, and also variance in &lt;a href=&quot;https://en.wikipedia.org/wiki/Supervised_learning&quot;&gt;supervised learning&lt;/a&gt;, and a family of machine learning algorithms that convert weak learners to strong ones. &lt;a href=&quot;https://en.wikipedia.org/wiki/Boosting_(machine_learning)&quot;&gt;Wikipedia&lt;/a&gt;&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>In <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a>, <strong>boosting</strong> is an <a href="https://en.wikipedia.org/wiki/Ensemble_learning">ensemble</a> <a href="https://en.wikipedia.org/wiki/Meta-algorithm">meta-algorithm</a> for primarily reducing <a href="https://en.wikipedia.org/wiki/Supervised_learning#Bias-variance_tradeoff">bias</a>, and also variance in <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised learning</a>, and a family of machine learning algorithms that convert weak learners to strong ones. <a href="https://en.wikipedia.org/wiki/Boosting_(machine_learning)">Wikipedia</a></p><span id="more"></span><h1 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h1><p>Today, I would like to introduce <code>Boosting</code> method in Machine Learning. Basically, <code>Boosting</code> is a set of algorithms (classifiers) which changes weak learner to strong learners. It is one of the ensemble methods for improving the model predictions of any given learning algorithm. However, unlike the regular ensemble method which we group several model and use them in parallel, <strong>Boosting</strong> is using a single model in sequential order with differnt weights. </p><p><strong>Boosting</strong> is also using ramdom sampling with a replacement. It will start training model from Sample 1 to Sample N. In each training, there would be well-classified data and wrong-classified data. Since it allows a replacement, some wrong classified data might be included in different samples or not. </p><p><img src="/2020/04/01/boosting/Boosting-Algorithm.png" alt="Boosting Algorithm"></p><p>Since each previous model affect a current model by assigning weights to data which it coudln‚Äôt classify correctly, this is not parallel - it‚Äôs sequential. At the end, it will use all trained models with different weights and generate the output. One of the disadvantages of Boosting is that it easily get corrupted by outliers because it will assign heavy weights to those outliers and it will mislead the models.</p><h3 id="AdaBoost-Adaptive-Boosting"><a href="#AdaBoost-Adaptive-Boosting" class="headerlink" title="AdaBoost (Adaptive Boosting)"></a>AdaBoost (Adaptive Boosting)</h3><p><img src="/2020/04/01/boosting/Adaboost-flow-chart.png" alt="Adaboost flow chart"></p><p><strong>AdaBoost</strong> works in a way putting more weights on difficult to classify data and less on those already handled well. </p><p>In AdaBoost, we use something called <em>Decision Stumps</em>, the simplest model we could construct on data. It split the data into two subsets based on the feature. To find the best decision stump, we should lay out all features of data along with every possible threshold and look for one gives us best accuracy. </p><p>In this example, I will consturct AdaBoost from the scratch with some mathematical explanation. </p><h4 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h4><p>X1 = (‚àí1,0,+), X2 = (‚àí0.5,0.5,+)<br>X3 = (0,1,‚àí), X4 = (0.5,1,‚àí)<br>X5 = (1,0,+), X6 = (1,‚àí1,+)<br>X7 = (0,‚àí1,‚àí),X8 = (0,0,‚àí)</p><p><img src="/2020/04/01/boosting/figure1.png" alt="figure1"></p><p><strong>How to?</strong></p><p>Constructing $D_t$. It‚Äôs basically weight of each $i^{th}$ data.</p><p>$D_{t+1}(i)$ = $\frac{D_t(i)}{Z_t} * e^{-\alpha_t}$ if $y_i = h_t(x_i)$<br>$D_{t+1}(i)$ = $\frac{D_t(i)}{Z_t} * e^{\alpha_t}$ if $y_i \ne h_t(x_i)$,</p><p>where $Z_t$ = Normalization Constant = Sum of $D_t$</p><p>and $\alpha_t$ = $\frac{1}{2}ln(\frac{1-\epsilon_t}{\epsilon_t})$.</p><p>$h_t: \epsilon_t = \sum\limits_{i=1}^{m}D_t\vert(y_i\ne h_t(x_i))$ if $(y_i\ne h_t(x_i)$ then 1 otherwise 0‚Äã</p><p><strong>Step 1</strong> </p><p>Put an initial random decision stump aka classifier $h_1$</p><p><img src="/2020/04/01/boosting/figure2.png" alt="figure2"></p><p>h1 is a randomly assinged decision stump.<br>Let‚Äôs get $D_1$ = $\frac{1}{m}$, where $m = 8$</p><table><thead><tr><th>t</th><th>err</th><th>alpha</th><th>Z</th><th>d1</th><th>d2</th><th>d3</th><th>d4</th><th>d5</th><th>d6</th><th>d7</th><th>d8</th></tr></thead><tbody><tr><td>1</td><td></td><td></td><td></td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td></tr></tbody></table><p>Now, in a second row, I will put 1 if $h_1$ classify $y_i$ incorrectly, else 0. </p><table><thead><tr><th>t</th><th>err</th><th>alpha</th><th>Z</th><th>d1</th><th>d2</th><th>d3</th><th>d4</th><th>d5</th><th>d6</th><th>d7</th><th>d8</th></tr></thead><tbody><tr><td>1</td><td></td><td></td><td></td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td></tr><tr><td></td><td></td><td></td><td></td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td></tr></tbody></table><p>$h_1$ has classified correctly $d_t(1),d_t(3),d_t(4),d_t(5),d_t(6)$</p><p>Once we have $D_1(i)$ and a result of classification by $h_1$ we could calculate $\epsilon_1$, where $h_t: \epsilon_t = \sum\limits_{i=1}^{m}D_t\vert(y_i\ne h_t(x_i))$ if $(y_i\ne h_t(x_i)$ then 1 otherwise 0</p><p>In a thrid row, I will get all $D_1\vert(y_i\ne h_1(x_i)$, and if take sumation of all values, it‚Äôs our $\epsilon_1$.</p><table><thead><tr><th>t</th><th>err</th><th>alpha</th><th>Z</th><th>d1</th><th>d2</th><th>d3</th><th>d4</th><th>d5</th><th>d6</th><th>d7</th><th>d8</th></tr></thead><tbody><tr><td>1</td><td>0.375</td><td></td><td></td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td></tr><tr><td></td><td></td><td></td><td></td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td></tr><tr><td></td><td></td><td></td><td></td><td>0</td><td>0.13</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.13</td><td>0.13</td></tr></tbody></table><p>Let‚Äôs get $\alpha_1$ and $Z_1$. Remember $Z_t$ = Normalization Constant = Sum of $D_t$, and $\alpha_t$ = $\frac{1}{2}ln(\frac{1-\epsilon_t}{\epsilon_t})$.</p><table><thead><tr><th>t</th><th>err</th><th>alpha</th><th>Z</th><th>d1</th><th>d2</th><th>d3</th><th>d4</th><th>d5</th><th>d6</th><th>d7</th><th>d8</th></tr></thead><tbody><tr><td>1</td><td>0.375</td><td>0.255</td><td>1.000</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td></tr><tr><td></td><td></td><td></td><td></td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td></tr><tr><td></td><td></td><td></td><td></td><td>0</td><td>0.13</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.13</td><td>0.13</td></tr></tbody></table><p>If you see the table above, you could notice that d2, d7, d8 has more weight, 0.13 becase $h_1$ failed to classify them correctly. Based on this information, we could move onto second iteration. In order to calculate $D_2(i)$ we need additional information other than table above.</p><p>$D_{t+1}(i)$ = $\frac{D_t(i)}{Z_t} * e^{-\alpha_t}$ if $y_i = h_t(x_i)$<br>$D_{t+1}(i)$ = $\frac{D_t(i)}{Z_t} * e^{\alpha_t}$ if $y_i \ne h_t(x_i)$</p><p>We need to what what‚Äôs $e^{-\alpha_1}$ and $e^{\alpha_1}$</p><p>$e^{-\alpha_1}=0.775$</p><p>$e^{\alpha_1}=1.291$</p><table><thead><tr><th>t</th><th>err</th><th>alpha</th><th>Z</th><th>d1</th><th>d2</th><th>d3</th><th>d4</th><th>d5</th><th>d6</th><th>d7</th><th>d8</th></tr></thead><tbody><tr><td>1</td><td>0.375</td><td>0.255</td><td>1.000</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td></tr><tr><td></td><td></td><td></td><td></td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td></tr><tr><td></td><td></td><td></td><td></td><td>0</td><td>0.13</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.13</td><td>0.13</td></tr><tr><td>2</td><td>0.323</td><td>0.371</td><td>0.968</td><td>0.097</td><td>0.161</td><td>0.097</td><td>0.097</td><td>0.097</td><td>0.097</td><td>0.161</td><td>0.161</td></tr></tbody></table><p>We‚Äôve got new weights for data. d1 was 0.125; it has become 0.097. d2 has become 0.161 from 0.125 because $h_1$ failed to classfy this data. Let‚Äôs get the $h_2$ - I was focusing on having all <code>+</code>in the same group. </p><p><img src="/2020/04/01/boosting/figure3.png" alt="figure3"></p><p>If you do same calculation we‚Äôve done above, You will get this table:</p><table><thead><tr><th>t</th><th>err</th><th>alpha</th><th>Z</th><th>d1</th><th>d2</th><th>d3</th><th>d4</th><th>d5</th><th>d6</th><th>d7</th><th>d8</th></tr></thead><tbody><tr><td>1</td><td>0.375</td><td>0.255</td><td>1.000</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td></tr><tr><td></td><td></td><td></td><td></td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td></tr><tr><td></td><td></td><td></td><td></td><td>0</td><td>0.13</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.13</td><td>0.13</td></tr><tr><td>2</td><td>0.323</td><td>0.371</td><td>0.968</td><td>0.097</td><td>0.161</td><td>0.097</td><td>0.097</td><td>0.097</td><td>0.097</td><td>0.161</td><td>0.161</td></tr><tr><td></td><td></td><td></td><td></td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td></tr><tr><td></td><td></td><td></td><td></td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.16</td><td>0.16</td></tr></tbody></table><p>$e^{-\alpha_2}=0.490$</p><p>$e^{\alpha_2}=2.041$</p><p>Lastly, let‚Äôs do the final iteration.</p><table><thead><tr><th>t</th><th>err</th><th>alpha</th><th>Z</th><th>d1</th><th>d2</th><th>d3</th><th>d4</th><th>d5</th><th>d6</th><th>d7</th><th>d8</th></tr></thead><tbody><tr><td>1</td><td>0.375</td><td>0.255</td><td>1.000</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td></tr><tr><td></td><td></td><td></td><td></td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td></tr><tr><td></td><td></td><td></td><td></td><td>0</td><td>0.13</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.13</td><td>0.13</td></tr><tr><td>2</td><td>0.194</td><td>0.713</td><td>0.968</td><td>0.097</td><td>0.161</td><td>0.097</td><td>0.097</td><td>0.097</td><td>0.097</td><td>0.161</td><td>0.161</td></tr><tr><td></td><td></td><td></td><td></td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td></td><td></td><td></td><td></td><td>0</td><td>0</td><td>0.1</td><td>0.1</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>3</td><td></td><td></td><td></td><td>0.049</td><td>0.082</td><td>0.204</td><td>0.204</td><td>0.049</td><td>0.049</td><td>0.082</td><td>0.082</td></tr></tbody></table><p><img src="/2020/04/01/boosting/figure4.png" alt="figure4"></p><p>A reasoning of $h_3$ is to have all <code>o</code> in a same group. Also, d5 and d6 have never been misclassified, so I want to have a information of d5 and d6 in my model. And the result will be:</p><table><thead><tr><th>t</th><th>err</th><th>alpha</th><th>Z</th><th>d1</th><th>d2</th><th>d3</th><th>d4</th><th>d5</th><th>d6</th><th>d7</th><th>d8</th></tr></thead><tbody><tr><td>1</td><td>0.375</td><td>0.255</td><td>1.000</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td><td>0.125</td></tr><tr><td></td><td></td><td></td><td></td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td></tr><tr><td></td><td></td><td></td><td></td><td>0</td><td>0.13</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.13</td><td>0.13</td></tr><tr><td>2</td><td>0.323</td><td>0.371</td><td>0.968</td><td>0.097</td><td>0.161</td><td>0.097</td><td>0.097</td><td>0.097</td><td>0.097</td><td>0.161</td><td>0.161</td></tr><tr><td></td><td></td><td></td><td></td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td></tr><tr><td></td><td></td><td></td><td></td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.16</td><td>0.16</td></tr><tr><td>3</td><td>0.138</td><td>0.916</td><td>0.943</td><td>0.069</td><td>0.115</td><td>0.069</td><td>0.069</td><td>0.069</td><td>0.069</td><td>0.241</td><td>0.241</td></tr><tr><td></td><td></td><td></td><td></td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td></tr><tr><td></td><td></td><td></td><td></td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.07</td><td>0.07</td><td>0</td><td>0</td></tr></tbody></table><p>Graphically, it will look like:</p><p><img src="/2020/04/01/boosting/figure5.png" alt="figure5"></p><p>Lastly, the final classifier is:</p><p>$H_{final}(x)=sign(0.255<em>h_1+0.371</em>h_2+0.943*h_3)$</p>]]></content:encoded>
      
      <comments>https://minkyuchoi-07.github.io/2020/04/02/boosting/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Naive Bayes from scratch</title>
      <link>https://minkyuchoi-07.github.io/2020/03/23/Naive%20Bayes%20from%20scratch/</link>
      <guid>https://minkyuchoi-07.github.io/2020/03/23/Naive%20Bayes%20from%20scratch/</guid>
      <pubDate>Mon, 23 Mar 2020 21:54:04 GMT</pubDate>
      <description>
      
        &lt;p&gt;Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features/predcitors) in a learning problem. &lt;code&gt;Maxumum-likelihood&lt;/code&gt; training can be done by evaluting a closed-form exporession, which takes linear time, rather tahn by expensive iterative approximation as used for many other typs of classifier.  &lt;a href=&quot;https://en.wikipedia.org/wiki/Naive_Bayes_classifier&quot;&gt;Wikipedia&lt;/a&gt;&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features/predcitors) in a learning problem. <code>Maxumum-likelihood</code> training can be done by evaluting a closed-form exporession, which takes linear time, rather tahn by expensive iterative approximation as used for many other typs of classifier.  <a href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier">Wikipedia</a></p><span id="more"></span><h2 id="Usecase-Spam-filter"><a href="#Usecase-Spam-filter" class="headerlink" title="Usecase - Spam filter"></a>Usecase - Spam filter</h2><p>We will use the <code>Naive Bayes</code> algorithm to fit a spam filter. </p><p>Spam filters are used in alal email services to classify received emails as ‚ÄúSpam‚Äù or ‚ÄúNot Spam‚Äù. A simple apporach involves maintaining a vocabulary of words that commonly occur in ‚ÄúSpam‚Äù emails and classifying an email as ‚ÄúSpam‚Äù if the number of words from the dictionary that are present in the email is over a certain threshold. </p><p>Assume we are given the vocabulary consists of 15 words</p><p>$V$ = {secret, offer, low, price, valued, customer, today, dollar, million, sports, is, for, play, healthy, pizza} </p><p>We will use $V_i$ to represent the ith word in $V$. As our training dataset, we are also given 3 example spam messages: <br><br>‚Ä¢ million dollar offer <br><br>‚Ä¢ secret offer today <br><br>‚Ä¢ secret is secret <br></p><p>and 4 example non-spam messages<br><br>‚Ä¢ low price for valued customer <br><br>‚Ä¢ play secret sports today<br><br>‚Ä¢ sports is healthy<br><br>‚Ä¢ low price pizza<br></p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br><span class="line"></span><br><span class="line">V = [<span class="hljs-string">&#x27;secret&#x27;</span>, <span class="hljs-string">&#x27;offer&#x27;</span>, <span class="hljs-string">&#x27;low&#x27;</span>, <span class="hljs-string">&#x27;price&#x27;</span>, <span class="hljs-string">&#x27;valued&#x27;</span>, <span class="hljs-string">&#x27;customer&#x27;</span>, <span class="hljs-string">&#x27;today&#x27;</span>, <span class="hljs-string">&#x27;dollar&#x27;</span>, <span class="hljs-string">&#x27;million&#x27;</span>, <span class="hljs-string">&#x27;sports&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;for&#x27;</span>, <span class="hljs-string">&#x27;play&#x27;</span>, <span class="hljs-string">&#x27;healthy&#x27;</span>, <span class="hljs-string">&#x27;pizza&#x27;</span>]</span><br><span class="line"></span><br><span class="line">message = [<span class="hljs-string">&#x27;million dollar offer&#x27;</span>,<span class="hljs-string">&#x27;secret offer today&#x27;</span>,<span class="hljs-string">&#x27;secret is secret&#x27;</span>,</span><br><span class="line">          <span class="hljs-string">&#x27;low price for valued customer&#x27;</span>, <span class="hljs-string">&#x27;play secret sports today&#x27;</span>, </span><br><span class="line">          <span class="hljs-string">&#x27;sports is healthy&#x27;</span>, <span class="hljs-string">&#x27;low price pizza&#x27;</span>]</span><br></pre></td></tr></table></figure><p><code>train</code> is our input vector x corresponding to each training message, and it has length n = 15 (length of V). Since we have 7 training example of message, We will have 7 by 15 training data - 7 data 15 features. </p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train = np.zeros((<span class="hljs-number">7</span>, <span class="hljs-number">15</span>))</span><br><span class="line">label = [<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="hljs-string">Converting 7 training message data set to x vector which has length n = 15</span></span><br><span class="line"><span class="hljs-string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(train)):</span><br><span class="line">    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(V)):</span><br><span class="line">        <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(message[i].split(<span class="hljs-string">&quot; &quot;</span>))):</span><br><span class="line">            <span class="hljs-keyword">if</span> V[j] == message[i].split(<span class="hljs-string">&quot; &quot;</span>)[k]:</span><br><span class="line">                train[i,j] += <span class="hljs-number">1</span></span><br></pre></td></tr></table></figure><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="hljs-string">This is the feture x matrix.</span></span><br><span class="line"><span class="hljs-string">row = n message</span></span><br><span class="line"><span class="hljs-string">col = i_th feature vector </span></span><br><span class="line"><span class="hljs-string">&#x27;&#x27;&#x27;</span></span><br><span class="line">train</span><br></pre></td></tr></table></figure><pre><code>array([[0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.],       [1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],       [2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],       [0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.],       [1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0.],       [0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])</code></pre><p>Let‚Äôs list them separately</p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(label)):</span><br><span class="line">    <span class="hljs-keyword">if</span> label[i] == <span class="hljs-number">0</span>:</span><br><span class="line">        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;this is &#123;&#125;th message - It&#x27;s SCAM&quot;</span>.<span class="hljs-built_in">format</span>(i))</span><br><span class="line">        <span class="hljs-built_in">print</span>(train[i])</span><br><span class="line">    <span class="hljs-keyword">else</span>:</span><br><span class="line">        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;this is &#123;&#125;th message - It&#x27;s NOT SCAM&quot;</span>.<span class="hljs-built_in">format</span>(i))</span><br><span class="line">        <span class="hljs-built_in">print</span>(train[i])</span><br></pre></td></tr></table></figure><pre><code>this is 0th message - It&apos;s SCAM[0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.]this is 1th message - It&apos;s SCAM[1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]this is 2th message - It&apos;s SCAM[2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]this is 3th message - It&apos;s NOT SCAM[0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.]this is 4th message - It&apos;s NOT SCAM[1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0.]this is 5th message - It&apos;s NOT SCAM[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0.]this is 6th message - It&apos;s NOT SCAM[0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]</code></pre><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="hljs-string">spam and ham array will be count of x_i when each message is spam or not spam.</span></span><br><span class="line"><span class="hljs-string">spam = (Y=0|x_i)</span></span><br><span class="line"><span class="hljs-string">ham = (Y=1|x_i)</span></span><br><span class="line"><span class="hljs-string">&#x27;&#x27;&#x27;</span></span><br><span class="line">spam = np.zeros((<span class="hljs-number">1</span>, <span class="hljs-number">15</span>))</span><br><span class="line">ham = np.zeros((<span class="hljs-number">1</span>, <span class="hljs-number">15</span>))</span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(label)):</span><br><span class="line">    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(train.shape[<span class="hljs-number">1</span>]):</span><br><span class="line">        <span class="hljs-keyword">if</span> label[c] == <span class="hljs-number">0</span>: <span class="hljs-comment"># Spam</span></span><br><span class="line">            spam[<span class="hljs-number">0</span>,i] += train[c,i]</span><br><span class="line">            </span><br><span class="line">        <span class="hljs-keyword">else</span>: <span class="hljs-comment"># Not Spam</span></span><br><span class="line">            ham[<span class="hljs-number">0</span>,i] += train[c,i]</span><br><span class="line">            </span><br><span class="line">    </span><br></pre></td></tr></table></figure><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spam</span><br></pre></td></tr></table></figure><pre><code>array([[3., 2., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0.]])</code></pre><p>If you see a result above, <code>spam[0] = 3</code> ,<code>spam[1] = 2</code> , <code>spam [2] = 0</code>, it means total count of <code>x0 = secret</code> is three among training data classified as <em>spam</em>. We have two <code>offer</code> and zeor <code>low</code> in our training data.  </p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="hljs-string">In this section, we will calculate probabiliy of each word xi when a message is spam or not.</span></span><br><span class="line"><span class="hljs-string">prob_spam = P(x_i|y=0)</span></span><br><span class="line"><span class="hljs-string">prob_ham = P(x_i|y=1)</span></span><br><span class="line"><span class="hljs-string">&#x27;&#x27;&#x27;</span></span><br><span class="line">spam_counter = <span class="hljs-number">0</span></span><br><span class="line">ham_counter = <span class="hljs-number">0</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(label)):</span><br><span class="line">    <span class="hljs-keyword">if</span> label[c] == <span class="hljs-number">0</span>:</span><br><span class="line">        spam_counter += <span class="hljs-number">1</span></span><br><span class="line">        </span><br><span class="line">    <span class="hljs-keyword">else</span>:</span><br><span class="line">        ham_counter += <span class="hljs-number">1</span></span><br><span class="line">        </span><br><span class="line">prob_spam_x_i = spam/spam_counter</span><br><span class="line">prob_ham_x_i = ham/ham_counter</span><br></pre></td></tr></table></figure><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">prob_spam_x_i </span><br></pre></td></tr></table></figure><pre><code>array([[1.        , 0.66666667, 0.        , 0.        , 0.        ,        0.        , 0.33333333, 0.33333333, 0.33333333, 0.        ,        0.33333333, 0.        , 0.        , 0.        , 0.        ]])</code></pre><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">prob_ham_x_i </span><br></pre></td></tr></table></figure><pre><code>array([[0.25, 0.  , 0.5 , 0.5 , 0.25, 0.25, 0.25, 0.  , 0.  , 0.5 , 0.25,        0.25, 0.25, 0.25, 0.25]])</code></pre><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="hljs-string">Bayes Therorm</span></span><br><span class="line"><span class="hljs-string">Calculating P(y=0|x_i)</span></span><br><span class="line"><span class="hljs-string">prob_spam_per_word = P(y=0|x_i)</span></span><br><span class="line"><span class="hljs-string">&#x27;&#x27;&#x27;</span></span><br><span class="line">prob_spam_per_word = np.zeros((<span class="hljs-number">1</span>, <span class="hljs-number">15</span>))</span><br><span class="line">prob_ham_per_word = np.zeros((<span class="hljs-number">1</span>, <span class="hljs-number">15</span>))</span><br><span class="line">prob_spam = <span class="hljs-number">3</span>/<span class="hljs-number">7</span></span><br><span class="line">prob_ham = <span class="hljs-number">4</span>/<span class="hljs-number">7</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(prob_spam_per_word.shape[<span class="hljs-number">1</span>]):</span><br><span class="line">    spam_a = (prob_spam_x_i[<span class="hljs-number">0</span>,i]*prob_spam) </span><br><span class="line">    spam_b = spam_a + (prob_ham_x_i[<span class="hljs-number">0</span>,i]*prob_ham)</span><br><span class="line">    spam_c = spam_a/spam_b</span><br><span class="line">    </span><br><span class="line">    ham_a = (prob_ham_x_i[<span class="hljs-number">0</span>,i]*prob_ham) </span><br><span class="line">    ham_b = ham_a + (prob_spam_x_i[<span class="hljs-number">0</span>,i]*prob_spam)</span><br><span class="line">    ham_c = ham_a/ham_b</span><br><span class="line">    </span><br><span class="line">    prob_spam_per_word[<span class="hljs-number">0</span>,i] = spam_c</span><br><span class="line">    prob_ham_per_word[<span class="hljs-number">0</span>,i] = ham_c</span><br></pre></td></tr></table></figure><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">prob_spam_per_word</span><br></pre></td></tr></table></figure><pre><code>array([[0.75, 1.  , 0.  , 0.  , 0.  , 0.  , 0.5 , 1.  , 1.  , 0.  , 0.5 ,        0.  , 0.  , 0.  , 0.  ]])</code></pre><p>The result above means that the probability of spam per i_th words. For example, the probability of spam when we have <code>secret</code> is 75% and a probability of spam of <code>offer</code> is 100%.</p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">prob_ham_per_word <span class="hljs-comment"># This is the probability of ham per words.</span></span><br></pre></td></tr></table></figure><pre><code>array([[0.25, 0.  , 1.  , 1.  , 1.  , 1.  , 0.5 , 0.  , 0.  , 1.  , 0.5 ,        1.  , 1.  , 1.  , 1.  ]])</code></pre><p><strong>Since we don‚Äôt have enough data, I will just use training dataset to evaluate the bayes classifier</strong></p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">threshold = <span class="hljs-number">0.50</span> <span class="hljs-comment"># In what percentage do you want to classfy an eamil as spam.</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(train.shape[<span class="hljs-number">0</span>]):</span><br><span class="line">    conditional_prob_spam = prob_spam</span><br><span class="line">    conditional_prob_ham = prob_ham</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(prob_spam_per_word.shape[<span class="hljs-number">1</span>]):</span><br><span class="line">        <span class="hljs-keyword">if</span> train[t][i] == <span class="hljs-number">1</span>:</span><br><span class="line">            conditional_prob_spam = conditional_prob_spam * prob_spam_per_word[<span class="hljs-number">0</span>,i]</span><br><span class="line">            conditional_prob_ham = conditional_prob_ham * prob_ham_per_word[<span class="hljs-number">0</span>,i]</span><br><span class="line">            </span><br><span class="line">    <span class="hljs-keyword">if</span> conditional_prob_spam != <span class="hljs-number">0</span>:</span><br><span class="line">        prob = conditional_prob_spam / (conditional_prob_spam + conditional_prob_ham) * <span class="hljs-number">100</span></span><br><span class="line">    <span class="hljs-keyword">else</span>:</span><br><span class="line">        prob = <span class="hljs-number">0.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">if</span> prob &gt; threshold*<span class="hljs-number">100</span>:</span><br><span class="line">        label = <span class="hljs-string">&quot;SPAM&quot;</span>  </span><br><span class="line">    <span class="hljs-keyword">else</span>:</span><br><span class="line">        label = <span class="hljs-string">&quot;NOT SPAM&quot;</span></span><br><span class="line">        </span><br><span class="line">    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;&#123;&#125; th email is &#123;&#125; with a probability of being spam &#123;&#125;%&quot;</span>.<span class="hljs-built_in">format</span>(t,label,prob))</span><br><span class="line">    <span class="hljs-comment">#print(prob)</span></span><br></pre></td></tr></table></figure><pre><code>0 th email is SPAM with a probability of being spam 100.0%1 th email is SPAM with a probability of being spam 100.0%2 th email is NOT SPAM with a probability of being spam 42.857142857142854%3 th email is NOT SPAM with a probability of being spam 0.0%4 th email is NOT SPAM with a probability of being spam 0.0%5 th email is NOT SPAM with a probability of being spam 0.0%6 th email is NOT SPAM with a probability of being spam 0.0%</code></pre><h4 id><a href="#" class="headerlink" title></a></h4><p>Given a new message <code>‚Äútoday is secret‚Äù</code>, decide whether it is spam or not spam, based on the Naive Bayes classifier, learned from the above data.</p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="hljs-string">&quot;today is secret&quot;</span></span><br><span class="line"><span class="hljs-string">Vectorizing the email.. </span></span><br><span class="line"><span class="hljs-string">&#x27;&#x27;&#x27;</span></span><br><span class="line">target = np.array([<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>])</span><br><span class="line">target[<span class="hljs-number">0</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="hljs-string">Please play around with different threshold. </span></span><br><span class="line"><span class="hljs-string">&#x27;&#x27;&#x27;</span></span><br><span class="line">threshold = <span class="hljs-number">0.50</span> <span class="hljs-comment"># In what percentage do you want to classfy an eamil as spam.</span></span><br><span class="line"></span><br><span class="line">conditional_prob_spam = prob_spam</span><br><span class="line">conditional_prob_ham = prob_ham</span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(prob_spam_per_word.shape[<span class="hljs-number">1</span>]):</span><br><span class="line">    <span class="hljs-keyword">if</span> target[i] == <span class="hljs-number">1</span>:</span><br><span class="line">        conditional_prob_spam = conditional_prob_spam * prob_spam_per_word[<span class="hljs-number">0</span>,i]</span><br><span class="line">        conditional_prob_ham = conditional_prob_ham * prob_ham_per_word[<span class="hljs-number">0</span>,i]</span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">if</span> conditional_prob_spam != <span class="hljs-number">0</span>:</span><br><span class="line">    prob = conditional_prob_spam / (conditional_prob_spam + conditional_prob_ham) * <span class="hljs-number">100</span></span><br><span class="line"><span class="hljs-keyword">else</span>:</span><br><span class="line">    prob = <span class="hljs-number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">if</span> prob &gt; threshold*<span class="hljs-number">100</span>:</span><br><span class="line">    label = <span class="hljs-string">&quot;SPAM&quot;</span>  </span><br><span class="line"><span class="hljs-keyword">else</span>:</span><br><span class="line">    label = <span class="hljs-string">&quot;NOT SPAM&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;&#123;&#125; th email is &#123;&#125; with a probability of being spam &#123;&#125;%&quot;</span>.<span class="hljs-built_in">format</span>(t,label,prob))</span><br><span class="line"><span class="hljs-comment">#print(prob)</span></span><br></pre></td></tr></table></figure><pre><code>6 th email is SPAM with a probability of being spam 69.23076923076923%</code></pre>]]></content:encoded>
      
      <comments>https://minkyuchoi-07.github.io/2020/03/23/Naive%20Bayes%20from%20scratch/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Feature Engineering</title>
      <link>https://minkyuchoi-07.github.io/2020/01/24/feature-engineering-101/</link>
      <guid>https://minkyuchoi-07.github.io/2020/01/24/feature-engineering-101/</guid>
      <pubDate>Fri, 24 Jan 2020 03:20:01 GMT</pubDate>
      <description>
      
        &lt;p&gt;If you ask yourself what‚Äôs the most important thing in machine learning, what‚Äôs your answer? All data scientist would have different answers. &lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>If you ask yourself what‚Äôs the most important thing in machine learning, what‚Äôs your answer? All data scientist would have different answers. </p><span id="more"></span><p>Among the other many answers, I believe feature engineering is one of the most important in machine learning. Sometimes, it‚Äôs a more critical step than a model selection and training a model because a model cannot improve a model itself even though we put a lot of effort on a hyper parameter turning. However, well selected/extracted features could be applied to many different models and improve a performance. </p><p>A feature engineering includes feature selection and feature extraction. A feature selection is trial-error process to select relevant features from existing features. Since all features are simply selected from original features it‚Äôs easy to interpret what those features means. However, it is difficult to consider a relationship in selected features. </p><p>On the other hands, a feature extraction is more like functional process to extract relevant features from existing features. It requires a form of function which enable an algorithm to create/extract a new set of features. A relationship between features will be considered and number of features could be significantly reduced. Yet, an interpretation of extracted features is not easy.</p><p>We should use different methods of feature engineering depending on a machine learning algorithm we want to use. </p><p>In supervised learning, we could select features form <code>Information gain</code>, <code>Stepwise regression</code>, <code>LASSO</code>, <code>Genetic algorithms</code>, etc. If we want to extract features, <code>Partial Least Squares (PLS)</code> is an option. </p><p>In unsupervised learning, we could do a feature selection with <code>PCA loading</code>; a feature extraction uses <code>Principal component analysis (PCA)</code>, <code>Wavelets transforms</code>, <code>Autoencoder</code>, etc. </p><h5 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h5><p><a href="https://pngimage.net/features-png-7/">Thumnail</a></p>]]></content:encoded>
      
      <comments>https://minkyuchoi-07.github.io/2020/01/24/feature-engineering-101/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Î®∏Ïã†Îü¨Îãù - Í∏∞Î≥∏Ïö©Ïñ¥</title>
      <link>https://minkyuchoi-07.github.io/2019/12/17/mlcrash02-keyterm/</link>
      <guid>https://minkyuchoi-07.github.io/2019/12/17/mlcrash02-keyterm/</guid>
      <pubDate>Tue, 17 Dec 2019 01:52:35 GMT</pubDate>
      <description>
      
        &lt;p&gt;&lt;code&gt;supervised&lt;/code&gt;  &lt;code&gt;unsupervised&lt;/code&gt; &lt;code&gt;feature&lt;/code&gt; &lt;code&gt;label&lt;/code&gt; ‚Ä¶ Î®∏Ïã†Îü¨ÎãùÏùÑ ÏãúÏûëÌïòÍ≤å ÎêòÎ©¥ ÏÉàÎ°ú Î∞∞ÏõåÏïº ÌïòÎäî Ïö©Ïñ¥Îì§Ïù¥ ÎßéÏ£†? ÌïòÏßÄÎßå Ïù¥Îü¨Ìïú Ïö©Ïñ¥Îì§ÏùÑ ÏûêÏã†Ïùò Í∞úÎÖêÏúºÎ°ú Ïûò Ï†ïÎ¶¨ÌïòÎäî Í≤ÉÏù¥ Ï∞∏ Ï§ëÏöîÌï©ÎãàÎã§. ÏôúÎÉêÌïòÎ©¥, Ïö∞Î¶¨Í∞Ä ÏïûÏúºÎ°ú Î∞∞Ïö∞Í≤å Îê† Î®∏Ïã†Îü¨ÎãùÏùò Í∏∞Ï¥àÍ∞Ä ÎêòÍ∏∞ ÎïåÎ¨∏Ïù¥Ï£†‚Ä¶&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p><code>supervised</code>  <code>unsupervised</code> <code>feature</code> <code>label</code> ‚Ä¶ Î®∏Ïã†Îü¨ÎãùÏùÑ ÏãúÏûëÌïòÍ≤å ÎêòÎ©¥ ÏÉàÎ°ú Î∞∞ÏõåÏïº ÌïòÎäî Ïö©Ïñ¥Îì§Ïù¥ ÎßéÏ£†? ÌïòÏßÄÎßå Ïù¥Îü¨Ìïú Ïö©Ïñ¥Îì§ÏùÑ ÏûêÏã†Ïùò Í∞úÎÖêÏúºÎ°ú Ïûò Ï†ïÎ¶¨ÌïòÎäî Í≤ÉÏù¥ Ï∞∏ Ï§ëÏöîÌï©ÎãàÎã§. ÏôúÎÉêÌïòÎ©¥, Ïö∞Î¶¨Í∞Ä ÏïûÏúºÎ°ú Î∞∞Ïö∞Í≤å Îê† Î®∏Ïã†Îü¨ÎãùÏùò Í∏∞Ï¥àÍ∞Ä ÎêòÍ∏∞ ÎïåÎ¨∏Ïù¥Ï£†‚Ä¶</p><span id="more"></span><hr><p>ÏïàÎÖïÌïòÏÑ∏Ïöî AI Nomad ÏµúÎØºÍ∑úÏûÖÎãàÎã§. Î®∏Ïã†Îü¨Îãù ÏÜçÏÑ±ÏΩîÏä§ Îëê Î≤àÏß∏ ÏÑ∏ÏÖòÏóêÏÑúÎäî Î®∏Ïã†Îü¨ÎãùÏóêÏÑú ÏÇ¨Ïö©ÎêòÎäî Í∏∞Î≥∏Ï†ÅÏù∏ Ïö©Ïñ¥Îì§ÏùÑ Ï†ïÎ¶¨Ìï¥Î≥¥Î†§ Ìï©ÎãàÎã§. </p><h4 id="1-Supervised-and-Unsupervised-Learning"><a href="#1-Supervised-and-Unsupervised-Learning" class="headerlink" title="1. Supervised and Unsupervised Learning"></a>1. Supervised and Unsupervised Learning</h4><p>Î®∏Ïã†Îü¨ÎãùÏùÑ Ï≤òÏùå ÏãúÏûëÌïòÍ≤å ÎêòÎ©¥ Í∞ÄÏû• Î®ºÏ†Ä ÏïåÍ≤å ÎêòÎäî Ïö©Ïñ¥Îäî <code>Supervised Learning</code>  Í≥º <code>Unsupervised Learning</code> ÏûÖÎãàÎã§.  </p><p>ÏòÅÏñ¥Î°ú supervisedÎùºÍ≥† ÌïòÎ©¥ Í∞êÎèÖÏù¥ ÎèºÎã§, Í¥ÄÎ¶¨ÎêòÎã§ Ï†ïÎèÑÎ°ú Ìï¥ÏÑùÎê©ÎãàÎã§. Îî∞ÎùºÏÑú supervised learning ÏùÑ ÏßÅÏó≠ÌïòÎ©¥ Í∞êÎèÖÏùò ÏßÄÏãú ÏïÑÎûò Î∞∞ÏõåÏßÄÎã§ Ï†ïÎèÑÎ°ú Ìï¥ÏÑùÎê©ÎãàÎã§. Ïö∞Î¶¨Í∞Ä Î¨¥Ïñ∏Í∞ÄÎ•º Î∞∞Ïö∏ Îïå Ï¢ãÏùÄ Í∞êÎèÖÏù¥ ÏûàÏúºÎ©¥ Î∞∞ÏõÄÏù¥ Ìé∏Ìï©ÎãàÎã§. ÏôúÎÉêÌïòÎ©¥, Í∑∏Îì§Ïù¥ Ïö∞Î¶¨Í∞Ä Î¨¥ÏóáÏùÑ Î∞∞ÏõåÏïº Ìï†ÏßÄ Í∞êÎèÖÌï¥Ï£ºÍ≥† ÏßÄÏãúÌï¥Ï£ºÍ∏∞ ÎïåÎ¨∏ÏûÖÎãàÎã§. </p><p>Î®∏Ïã†Îü¨ÎãùÏóêÏÑú supervised learning ÎèÑ ÎπÑÏä∑Ìïú ÏùòÎØ∏Í∞Ä ÏûàÎã§Í≥† Ìï† Ïàò ÏûàÏäµÎãàÎã§. Ïª¥Ìì®ÌÑ∞Í∞Ä ÏÇ¨ÎûåÏùò Í∞êÎèÖ ÏïÑÎûò ÏßÄÏãúÎêòÍ≥† ÌïôÏäµÎêòÍ≤å ÎêòÎäî Í±∞Ï£†. Í∞êÎèÖÎì§ÏùÄ ÏÑ†ÏàòÎì§ÏùÑ ÏßÄÎèÑÌïòÍ≥† ÏßÄÏãúÌï©ÎãàÎã§. AÏôÄ BÎ•ºÌïòÎùºÍ≥† ÏôúÎÉêÌïòÎ©¥ Í∞êÎèÖÎì§ÏùÄ AÏôÄ BÎ•º ÌñàÏùÑÎïå CÎùºÎäî Í≤∞Í≥ºÍ∞Ä ÎÇòÏò¨ Í±∞Î•º Í≤ΩÌóòÏùÑ ÌÜµÌï¥ ÏïåÍ∏∞ ÎïåÎ¨∏Ïù¥Ï£†. Î®∏Ïã†Îü¨ÎãùÏóêÏÑú A ÏôÄ B Î•º feature Ïù¥ÎùºÍ≥† Î∂ÄÎ¶ÖÎãàÎã§. Í∑∏Î¶¨Í≥† C Î•º label Ïù¥ÎùºÍ≥† Î∂ÄÎ•¥Ï£†.  supervised learning ÏùÑ ÌïôÏäµÏãúÌÇ§Í∏∞ ÏúÑÌï¥ÏÑúÎäî feature  ÏôÄ label Ïù¥ ÌïÑÏöîÌï©ÎãàÎã§. Í∞êÎèÖÏù¥ ÏûàÍ∏∞ ÎïåÎ¨∏Ïù¥Ï£†.  </p><p>Í∑∏Î†áÎã§Î©¥ Í∞êÎèÖÏù¥ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÎäî unsupervised learning ÏùÄ Ïñ¥Îñ®ÍπåÏöî? Í∞êÎèÖÏùÄ ÏóÜÍ≥† ÏÑ†ÏàòÎì§Îßå ÏûàÎäî ÌåÄÏù¥ ÏûàÎã§Í≥† Í∞ÄÏ†ïÌï©ÏãúÎã§. ÏÑ†ÏàòÎì§ÏùÄ Îß§Ï£º Í≤ΩÍ∏∞Ïóê ÎÇòÍ∞ÄÏïº ÌïòÏ£†. Íµ¨Îã®Ï£ºÎäî ÏÑ†ÏàòÎì§ÏóêÍ≤å Î¶¨Í∑∏ 3ÏúÑ ÏïàÏóê Îì§ÏßÄ ÏïäÏúºÎ©¥ ÌåÄÏùÑ Ìï¥Ï≤¥ÌïòÍ≤†Îã§Í≥† Ìï©ÎãàÎã§. Í∞êÎèÖÏù¥ ÏóÜÎã§Îäî Í≤ÉÏùÄ label Ïù¥ ÏóÜÎã§Îäî Í≤ÉÏù¥Ï£†. ÏÑ†ÏàòÎì§ÏùÄ ÏßÅÍ¥ÄÏ†ÅÏúºÎ°ú Ïñ¥Îñ†Ìïú ÌõàÎ†®ÏùÑ Ìï¥Ïïº Ìï†ÏßÄÎäî Ïïå Ïàò ÏûàÏùÑ Í≤ÉÏûÖÎãàÎã§. ÌïòÏßÄÎßå Í∞êÎèÖÏùò Í≤ΩÌóòÏù¥ ÏóÜÎäî Í≤ÉÏù¥Ï£†. Í∑∏Îü¨ÎØÄÎ°ú unsupervised learning ÏóêÎäî feature ÎßåÏù¥ Ï°¥Ïû¨Ìï©ÎãàÎã§. Ïù¥ ÏÉÅÌô©ÏóêÏÑú ÏÑ†ÏàòÎì§ÏùÄ Ïñ¥ÎñªÍ≤å Í∞êÎèÖ ÏóÜÏù¥ ÌåÄÏùÑ ÎπåÎî© Ìï† Ïàò ÏûàÏùÑÍπåÏöî? Ïä§Ïä§Î°ú ÎãµÏùÑ Ï∞æÏïÑÏïºÏ£†. ÌõàÎ†®Í≥º Í≤ΩÍ∏∞Î•º ÌÜµÌïú <code>trial and error</code> Ï¶â ÏãúÌñâÏ∞©Ïò§Î•º Í±∞Ï≥êÏÑú ÏÑ†ÏàòÎì§ Ïä§Ïä§Î°ú Î∞∞ÏõåÍ∞ÄÎäî ÏàòÎ∞ñÏóê ÏóÜÍ≤†Ï£†. Ïù¥Í≤ÉÏù¥ Î®∏Ïã†Îü¨ÎãùÏóêÏÑú unsupervised learning Ïù¥ÎùºÍ≥† Î∂àÎ¶¨Îäî Í≤ÉÏûÖÎãàÎã§. Îç∞Ïù¥ÌÑ∞ÏóêÎäî feature Îì§Îßå Ï°¥Ïû¨ÌïòÍ≥† label Ïù¥ ÏóÜÏäµÎãàÎã§. Îî∞ÎùºÏÑú Ïó¨Îü¨ ÏïåÍ≥†Î¶¨Ï¶òÎì§Ïù¥ feature ÏùÑ Í∞ÄÏßÄÍ≥† ÏãúÌñâÏ∞©Ïò§Î•º Í±∞Ï≥ê Í∞ÄÏû• Ïù¥ÏÉÅÏ†ÅÏù∏ label ÏùÑ Ï∞æÏïÑÍ∞ÄÎäî Í≤ÉÏùÑ Ïö∞Î¶¨Îäî unsupervised learning Ïù¥ÎùºÍ≥† Ìï©ÎãàÎã§</p><h4 id="2-Labels-Í≤∞Í≥ºÍ∞í"><a href="#2-Labels-Í≤∞Í≥ºÍ∞í" class="headerlink" title="2. Labels (Í≤∞Í≥ºÍ∞í)"></a>2. Labels (Í≤∞Í≥ºÍ∞í)</h4><p>ÏïûÏóêÏÑú ÎÇòÏò® <code>label</code>ÏùÄ Ïñ¥Îñ†Ìïú xÎì§Ïóê ÎåÄÌïú y Ï¶â Í≤∞Í≥ºÍ∞íÏù¥ÎùºÍ≥† Ï†ïÎ¶¨ Ìï† Ïàò ÏûàÍ≤†ÏäµÎãàÎã§. ÏòàÎ•º Îì§Î©¥, ÎØ∏ÎûòÏùò Ï£ºÏãùÍ∞ÄÍ≤©, ÏÇ¨ÏßÑ ÏÜçÏóê Îì±Ïû•Ìïú ÎèôÎ¨ºÏùò Ï¢ÖÎ•ò, Ïñ¥Îñ§ ÏÜåÎ¶¨Ïóê ÏùòÎØ∏ Îì±Ïù¥ ÏûàÍ≤†Íµ∞Ïöî. </p><h4 id="3-Feature-ÏõêÏù∏"><a href="#3-Feature-ÏõêÏù∏" class="headerlink" title="3. Feature (ÏõêÏù∏)"></a>3. Feature (ÏõêÏù∏)</h4><p><code>Feature</code>ÏùÄ Í∞ÑÎã®ÌïòÍ≤å Ïñ¥Îñ§ Í≤∞Í≥ºÏóê ÎåÄÌïú ÏõêÏù∏ÏûÖÎãàÎã§. yÎùºÎäî Í≤∞Í≥ºÏóê ÎåÄÌïú xÎùºÎäî ÏõêÏù∏Ïù¥Ï£†. ÎåÄÌïôÍµê ÏÑ±Ï†ÅÏùÑ Í≤∞Í≥ºÎùºÍ≥† ÌïúÎã§Î©¥ Ïù¥ Í≤∞Í≥ºÏóê ÎåÄÌïú feature Ï¶â ÏõêÏù∏ÏúºÎ°úÎäî ÌïòÎ£® Í≥µÎ∂Ä ÏãúÍ∞Ñ, Ïó∞Ïï† Ïó¨Î∂Ä, ÏïÑÎ•¥Î∞îÏù¥Ìä∏ Ïó¨Î∂Ä, Îì±Îì±Ïù¥ ÏûàÍ≤†ÎÑ§Ïöî. </p><h4 id="4-Model-Î™®Îç∏"><a href="#4-Model-Î™®Îç∏" class="headerlink" title="4. Model (Î™®Îç∏)"></a>4. Model (Î™®Îç∏)</h4><p><code>Î™®Îç∏</code>ÏùÄ featureÏôÄ feature ÏÇ¨Ïù¥ Îòê featureÏôÄ labelÏùò Í¥ÄÍ≥ÑÎùºÍ≥† Î≥¥ÏãúÎ©¥ Îê©ÎãàÎã§. Î™®Îç∏ÏùÄ AÏôÄ BÎùºÎäî featureÎì§Ïùò Í¥ÄÍ≥ÑÎ•º ÏÑ§Î™ÖÌïòÍ≥† A,B ÎùºÎäî featureÎì§Í≥º CÎùºÎäî labelÏùò Í¥ÄÍ≥ÑÎ•º ÏÑ§Î™ÖÌï©ÎãàÎã§. Ïù¥Îì§Ïùò Í¥ÄÍ≥ÑÎ•º Ïûò ÏÑ§Î™ÖÌïòÎäî Î™®Îç∏ÏùÑ Ïö∞Î¶¨Îäî Ï¢ãÏùÄ Î™®Îç∏Ïù¥ÎùºÍ≥† Î∂ÄÎ•¥Î©∞ Ï¢ãÏùÄ Í≤∞Í≥ºÎ•º ÎÇ¥Í≤å Îê©ÎãàÎã§. </p><h4 id="5-Train-ÌïôÏäµ"><a href="#5-Train-ÌïôÏäµ" class="headerlink" title="5. Train (ÌïôÏäµ)"></a>5. Train (ÌïôÏäµ)</h4><p>Ïö∞Î¶¨Í∞Ä Î™®Îç∏ÏùÑ Ïù¥ÏïºÍ∏∞Ìï† Îïå <code>train</code> ÏãúÌÇ®Îã§ <code>ÌïôÏäµ</code> ÏãúÌÇ®Îã§Îäî Ïö©Ïñ¥Î•º ÎßéÏù¥ ÏÇ¨Ïö©Ìï©ÎãàÎã§. Î™®Îç∏ÏùÑ ÌïôÏäµÏãúÌÇ®Îã§ Ï¶â train ÌïúÎã§Îäî ÎßêÏùÄ ÏâΩÍ≤å ÎßêÌïòÎ©¥ Î™®Îç∏ÏùÑ ÎßåÎì†Îã§ Î™®Îç∏ÏùÑ ÌïôÏäµÏãúÌÇ®Îã§Í≥† Ïù¥Ìï¥ÌïòÏãúÎ©¥ ÎêòÍ≤†ÏäµÎãàÎã§. Í∑∏Î†áÎã§Î©¥ Î™®Îç∏ÏùÑ ÎßåÎì§Í≥† ÌïôÏäµÏãúÌÇ®Îã§ÎäîÎç∞ Ïñ¥ÎñªÍ≤å Î¨¥ÏóáÏúºÎ°ú ÌïôÏäµÏãúÌÇ®Îã§Îäî Í±∞Ï£†? Î∞îÎ°ú Îç∞Ïù¥ÌÑ∞ÏûÖÎãàÎã§. Îç∞Ïù¥ÌÑ∞Ïóê Ï°¥Ïû¨ÌïòÎäî ÎßéÏúºÎ©¥ ÎßéÏùÄ Ï†ÅÏúºÎ©¥ Ï†ÅÏùÄ featureÎì§Í≥º lableÏùÑ ÌÜµÌï¥ÏÑú Î™®Îç∏ÏùÑ ÌïôÏäµÏãúÌÇ§Îäî Í≤ÉÏûÖÎãàÎã§. </p><p>Ïö∞Î¶¨Í∞Ä Í∞ïÏïÑÏßÄÏóêÍ≤å ÏÜêÏùÑ Îã¨ÎùºÍ≥† ÌñàÏùÑ Îïå ÏÜêÏùÑ Ï£ºÎäî ÌõàÎ†®ÏùÑ ÌïúÎã§Í≥† Í∞ÄÏ†ïÏùÑ Ìï¥Î¥ÖÏãúÎã§. Í∞ïÏïÑÏßÄÍ∞Ä Ïù¥ ÌõàÎ†®ÏùÑ ÏäµÎìùÌïòÎäî Î∞©Î≤ïÏùÄ Î≥¥ÌÜµ Îã§ÏùåÍ≥º Í∞ôÍ≤†Ï£†</p><p>ÏÜê -&gt; Í∞ÑÏãù (o)<br>Î∞ú -&gt; Í∞ÑÏãù (x)<br>ÎàÑÏõÄ -&gt; Í∞ÑÏãù (x)<br>ÏÜê -&gt; Í∞ÑÏãù (o)<br>ÏÜê -&gt; Í∞ÑÏãù (o)<br>ÏÜê -&gt; Í∞ÑÏãù (o)</p><p>Í∞ïÏïÑÏßÄÎäî ÏÜêÏùÑ Ï§¨ÏùÑ Îïå Í∞ÑÏãùÏùÑ Î®πÏóàÎã§Îäî Îç∞Ïù¥ÌÑ∞Î•º Í∏∞Î∞òÏúºÎ°ú ÌïôÏäµÌñàÍ≥† ÏÇ¨ÎûåÏù¥ ÏÜêÏù¥ÎùºÍ≥† Ïù¥ÏïºÍ∏∞ÌñàÏùÑ Îïå ÏÜêÏùÑ Ï£ºÎäî ÌñâÎèôÏùÑ ÌïòÍ≤å ÎêòÏ£†. Î®∏Ïã†Îü¨ÎãùÎèÑ ÎßàÏ∞ÆÍ∞ÄÏßÄÎ°ú Îç∞Ïù¥ÌÑ∞Î•º Í∏∞Î∞òÏúºÎ°ú Ïñ¥Îñ§ Î™®Îç∏ÏùÑ ÌïôÏäµÌïòÍ≥†, Ïö∞Î¶¨Í∞Ä Î™®Îç∏Ïóê inputÏùÑ Ï£ºÏóàÏùÑ Îïå Î™®Îç∏ÏùÄ ÌïôÏäµÎêú Îç∞Ïù¥ÌÑ∞Î•º Î∞îÌÉïÏúºÎ°ú Ïñ¥Îñ§ output Ï¶â <code>inference</code> Î•º ÎßåÎì§Ïñ¥ ÎÇ¥Í≤å Îê©ÎãàÎã§. </p><h4 id="6-Inference-ÏïîÏãú"><a href="#6-Inference-ÏïîÏãú" class="headerlink" title="6. Inference (ÏïîÏãú)"></a>6. Inference (ÏïîÏãú)</h4><p><code>Inference</code>Îäî unlabeled exampleÏùÑ ÌïôÏäµÎêú Î™®Îç∏Ïóê Ï†ÅÏö©Ìï† Îïå ÏÇ¨Ïö©Ìï©ÎãàÎã§. Ï¶â Î™®Îç∏ÏùÑ ÌïôÏäµÏãúÌÇ§Í≥† ÏÉàÎ°úÏö¥ inputÏùÑ Î™®Îç∏Ïóê ÎÑ£ÏóàÏùÑ Îïå Î™®Îç∏Ïù¥ ÎßåÎìúÎäî outputÏùÑ inferenceÎùºÍ≥† Î∂ÄÎ•¥Ï£†. Í≤∞Íµ≠ Ïù¥ outputÎèÑ ÏòàÏ∏°Í∞íÏù¥Í∏∞ ÎïåÎ¨∏Ïóê Í≤∞Í≥ºÎ•º ÏïîÏãúÌïúÎã§ ÎùºÎäî ÏùòÎØ∏Î°ú inferenceÎùºÍ≥† Î∂ÄÎ¶ÖÎãàÎã§. </p><h4 id="7-ÏöîÏïΩ"><a href="#7-ÏöîÏïΩ" class="headerlink" title="7. ÏöîÏïΩ"></a>7. ÏöîÏïΩ</h4><p>Îç∞Ïù¥ÌÑ∞Î•º ÎπºÎÜìÍ≥†Îäî Î®∏Ïã†Îü¨ÎãùÏùÑ Ïù¥ÏïºÍ∏∞ Ìï† Ïàò ÏóÜÏäµÎãàÎã§. Îç∞Ïù¥ÌÑ∞Í∞Ä quantitative or qualitative ÌïúÏßÄÎ•º Îñ†ÎÇòÏÑú Î®∏Ïã†Îü¨ÎãùÏóêÏÑú Îç∞Ïù¥ÌÑ∞Î•º Î≥¥Îäî Í¥ÄÏ†êÏùÄ Îî± Îëê Í∞ÄÏßÄ ÏûÖÎãàÎã§. <code>feature</code> ÏôÄ <code>label</code>. Ïù¥Îü¨Ìïú featureÏôÄ labelÏùò Í¥ÄÍ≥ÑÎ•º ÏÑ§Î™ÖÌïòÎäî Í≤ÉÏù¥ Î™®Îç∏Ïù¥Í≥†, Î™®Îç∏ÏùÑ ÎßåÎì§Í∏∞ ÏúÑÌï¥ÏÑúÎäî Îç∞Ïù¥ÌÑ∞Î•º Í∞ÄÏßÄÍ≥† Î™®Îç∏ÏùÑ <code>train</code> (ÌïôÏäµ) ÏãúÌÇ§Îäî Í≥ºÏ†ïÏù¥ ÌïÑÏöîÌï©ÎãàÎã§. Î™®Îç∏ÏùÑ ÌÜµÌï¥ ÏÉùÏÑ±Îêú Í≤∞Í≥ºÍ∞í Ï¶â predictionÏù¥ Î∞îÎ°ú <code>inference</code> Í∞Ä ÎêòÍ≤å ÎêòÎäî Í≤ÉÏù¥Ï£†. </p><p>Ïù¥Î†áÍ≤å Ïù¥Î≤à ÏÑ∏ÏÖòÏóêÏÑúÎäî Í∏∞Î≥∏Ï†ÅÏù∏ Î®∏Ïã†Îü¨Îãù Ïö©Ïñ¥Îì§Ïóê ÎåÄÌï¥ Î∞∞ÏõåÎ¥§ÏäµÎãàÎã§. Îã§Ïùå ÏÑ∏ÏÖòÏùÄ Î≥∏Í≤©Ï†ÅÏúºÎ°ú Î™®Îç∏Ïóê ÏÇ¨Ïö©ÎêòÎäî ÏïåÍ≥†Î¶¨Ï¶òÍ≥º ÏïåÍ≥†Î¶¨Ï¶òÏù¥ ÌïôÏäµÎêòÎäî Î°úÏßÅÏóê ÎåÄÌïú ÏÑ∏ÏÖòÏùÑ Ï§ÄÎπÑÌïòÍ≤†ÏäµÎãàÎã§. </p><p><strong><a href="https://minkyuchoi-07.github.io/2019/12/13/mlcrash01-overview/">&gt;&gt; Previous: Overview</a></strong></p><h5 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h5><p><a href="https://www.kdnuggets.com/2016/05/machine-learning-key-terms-explained.html">Thumnail</a><br><a href="https://developers.google.com/machine-learning/crash-course">Google Machine Learning Crash Course</a></p><hr><p><em>Except as otherwise noted, the content of this page is licensed under the <a href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 License</a>, and code samples are licensed under the <a href="https://www.apache.org/licenses/LICENSE-2.0">Apache 2.0 License</a>. For details, see the <a href="https://developers.google.com/site-policies">Google Developers Site Policies</a>. Java is a registered trademark of Oracle and/or its affiliates.</em></p>]]></content:encoded>
      
      <comments>https://minkyuchoi-07.github.io/2019/12/17/mlcrash02-keyterm/#disqus_thread</comments>
    </item>
    
    <item>
      <title>ÏïîÌò∏ÌôîÎ≥µÌò∏Ìôî</title>
      <link>https://minkyuchoi-07.github.io/2019/12/07/encryption-decryption/</link>
      <guid>https://minkyuchoi-07.github.io/2019/12/07/encryption-decryption/</guid>
      <pubDate>Sat, 07 Dec 2019 21:22:36 GMT</pubDate>
      <description>
      
        &lt;p&gt;Ïó¨Îü¨Î∂ÑÎì§Ïùò Îç∞Ïù¥ÌÑ∞Îäî ÏïàÏ†ÑÌï©ÎãàÍπå? &lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>Ïó¨Îü¨Î∂ÑÎì§Ïùò Îç∞Ïù¥ÌÑ∞Îäî ÏïàÏ†ÑÌï©ÎãàÍπå? </p><span id="more"></span><p>Ïò§ÎäòÏùÄ ÏïîÌò∏Ìôî (Encryption) Î≥µÌò∏Ìôî(Decryption)Ïóê ÎåÄÌïú Ïù¥ÏïºÍ∏∞Î•º ÎÇòÎà†Î≥¥Í≥†Ïûê Ìï©ÎãàÎã§. </p><h2 id="ÏïîÌò∏ÏôÄ-Î≥µÌò∏ÏôÄ"><a href="#ÏïîÌò∏ÏôÄ-Î≥µÌò∏ÏôÄ" class="headerlink" title="ÏïîÌò∏ÏôÄ/Î≥µÌò∏ÏôÄ"></a>ÏïîÌò∏ÏôÄ/Î≥µÌò∏ÏôÄ</h2><p>wÏïîÌò∏ÌôîÎäî Îç∞Ïù¥ÌÑ∞Î•º ÏïîÌò∏Ìôî ÌïòÏó¨ÏÑú ÎàÑÍµ∞Í∞ÄÍ∞Ä ÏùΩÏùÑ Ïàò ÏóÜÎèÑÎ°ù Ï†ïÎ≥¥Î•º Ï†ÑÎã¨ÌôîÎäî Í≥ºÏ†ïÏûÖÎãàÎã§. ÏïîÌò∏ÏôÄÏóêÎäî Ïó¨Îü¨Í∞ÄÏßÄ ÏïåÍ≥†Î¶¨Ï¶òÏù¥ Ïì∞ÏûÖÎãàÎã§. </p><p>Î≥µÌò∏ÏôÄÎäî ÏïîÌò∏ÌôîÎêú Ï†ïÎ≥¥Î•º Îã§Ïãú ÏùΩÏùÑ Ïàò ÏûàÍ≤åÌïòÎäî Í≥ºÏ†ïÏúºÎ°úÏç® Îç∞Ïù¥ÌÑ∞Í∞Ä ÎàÑÏ∂úÎêòÎçîÎùºÎèÑ Î≥µÌò∏ÌôîÎ•º ÌïòÏßÄÎ™ªÌïòÎ©¥ ÏïîÌò∏ÌôîÎêú Îç∞Ïù¥ÌÑ∞Î•º ÏùΩÏùÑ Ïàò ÏóÜÏäµÎãàÎã§. </p><h2 id="ÏïîÌò∏ÏôÄ-Ï¢ÖÎ•ò"><a href="#ÏïîÌò∏ÏôÄ-Ï¢ÖÎ•ò" class="headerlink" title="ÏïîÌò∏ÏôÄ Ï¢ÖÎ•ò"></a>ÏïîÌò∏ÏôÄ Ï¢ÖÎ•ò</h2><ol><li><p>Îã®Î∞©Ìñ• ÏïîÌò∏: ÏïîÌò∏Ìôî ÌõÑ Î≥µÌò∏Ìôî Ìï† Ïàò ÏóÜÏäµÎãàÎã§. ÏòàÎ•º Îì§Î©¥ <code>ÏÇ¨Ïö©Ïûê ÎπÑÎ∞ÄÎ≤àÌò∏</code> ÏÇ¨Ïö©ÏûêÍ∞Ä ÏûÖÎ†•Ìïú ÎπÑÎ∞ÄÎ≤àÌò∏Î•º ÏïîÌò∏Ìôî ÌïòÍ≥† Î™®Îì† Ï†ëÍ∑ºÏûêÎäî ÏïîÌò∏Ìôî Îêú ÏΩîÎìúÎ•º Îã§Ïãú ÌèâÎ¨∏ÏúºÎ°ú Î≥º Ïàò ÏóÜÏäµÎãàÎã§. Ìï¥ÌÇπÏù¥ ÎêòÏñ¥ÎèÑ Î≥µÌò∏ÌôîÍ∞Ä ÍµâÏû•Ìûà Ïñ¥Î†µÏäµÎãàÎã§. ÏòàÏô∏Ï†ÅÏù∏ Í≤ΩÏö∞Î°úÎäî <code>RainbowTable</code> Ïù¥ ÏûàÏäµÎãàÎã§. </p><p>Îçî ÏûêÏÑ∏Ìïú Ï†ïÎ≥¥: <a href="https://www.youtube.com/watch?v=TeIVhioUAXs">Ï∞∏Í≥†ÏòÅÏÉÅ</a></p></li><li><p>ÏñëÎ∞©Ìñ• ÏïîÌò∏: ÏïîÌò∏ÏôÄÏôÄ Î≥µÌò∏Ìôî Î™®Îëê Í∞ÄÎä•Ìï©ÎãàÎã§. <code>ÏÇ¨Ïö©Ïûê Ï£ºÏÜå, Ïù¥Î©îÏùº, Ï†ÑÏûêÏÑúÎ™Ö</code> Îì±Í≥º Í∞ôÏù¥ Ï†ïÎ≥¥Î•º Ïû¨ÏÇ¨Ïö©Ìï¥Ïïº ÎêòÎäî Í≤ΩÏö∞Ïóê ÏÇ¨Ïö©Ìï©ÎãàÎã§. </p><p>ÏñëÎ∞©Ìñ• ÏïîÌò∏ÏóêÎäî ÌÅ¨Í≤å Îëê Í∞ÄÏßÄ Ï¢ÖÎ•òÍ∞Ä ÏûàÏäµÎãàÎã§. </p><ul><li><p>ÎåÄÏπ≠Ìòï ÏïîÌò∏ (ÎπÑÎ∞ÄÌÇ§ ÏïîÌò∏)</p><p>ÎåÄÏπ≠Ìòï ÏïîÌò∏Îäî ÏïîÌò∏Ìôî Ìï† Îïå ÏÇ¨Ïö©ÌïòÎäî ÌÇ§ÏôÄ Î≥µÌò∏Ìôî Ìï† Îïå ÏÇ¨Ïö©ÌïòÎäî ÌÇ§Í∞Ä ÎèôÏùºÌïú ÏïîÌò∏Ìôî Í∏∞Î≤ïÏûÖÎãàÎã§. ÏòàÎ•º Îì§Î©¥ ‚ÄúAPPLE‚ÄùÎ•º ‚ÄúABCDE‚ÄùÎ°ú ÏïîÌò∏Ìôî ÌñàÎã§Î©¥ Î≥µÌò∏ÌôîÎèÑ Î∞òÎìúÏãú ‚ÄúABCDE‚ÄùÎ°ú Ìï¥ÏïºÎê©ÎãàÎã§. ÏòàÎ•º Îì§Î©¥ <code>AES Algorithm</code></p><p>ÌïòÏßÄÎßå ÎåÄÏπ≠Ìòï ÏïîÌò∏ÏóêÎäî ÌÇ§ Î∞∞ÏÜ°Ïóê Í¥ÄÌïú Î¨∏Ï†úÍ∞Ä Î∞úÏÉùÎê©ÎãàÎã§. ÏÜ°Ïã† Ï∏°ÏóêÏÑúÎäî Îç∞Ïù¥ÌÑ∞Î•º ÏïîÌò∏ÌôîÌïú ÌõÑÏóê ÏàòÏã† Ï∏°Ïóê ÏïîÌò∏ÌÇ§Î•º Ï†ÑÎã¨Ìï¥ÏïºÎêòÍ≥† Ï†ÑÎã¨ÌïòÎäî Í≥ºÏ†ïÏóêÏÑú Ïù¥ Ìï®Ìò∏ ÌÇ§Í∞Ä ÌÑ∏Î¶¨Î©¥ Îç∞Ïù¥ÌÑ∞Í∞Ä Ïú†Ï∂úÎê©ÎãàÎã§. Í∑∏Î¶¨Í≥† ÌÇ§ Í¥ÄÎ¶¨Í∞Ä Ïñ¥Î†µÏäµÎãàÎã§.</p></li></ul></li></ol><ul><li><p>ÎπÑÎåÄÏπ≠Ìòï ÏïîÌò∏ (Í≥µÍ∞úÌÇ§ ÏïîÌò∏)</p><p>ÎπÑÎåÄÏπ≠ÌòÑ ÏïîÌò∏Îäî ÏïîÌò∏ÏôÄ ÌÇ§ÏôÄ Î≥µÌò∏Ìôî ÌÇ§Í∞Ä Îã§Î¶ÖÎãàÎã§. </p><p>ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ÏôÄ ÏÑúÎ≤ÑÍ∞Ä Í∞ÅÍ∞ÅÏùò Í≥µÍ∞úÌÇ§ÏôÄ ÎπÑÎ∞ÄÌÇ§Î•º Í∞ñÍ≥†, ÏÑúÎ°ú Í≥µÍ∞úÌÇ§Î•º Í≥µÍ∞úÌï©ÎãàÎã§. ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Îäî ÏÑúÎ≤ÑÏùò Í≥µÍ∞úÌÇ§Î°ú Îç∞Ïù¥ÌÑ∞Î•º ÏïîÌò∏ÌôîÌïú ÌõÑÏóê ÏÑúÎ≤ÑÎ°ú Î≥¥ÎÇ¥Î©¥ ÏÑúÎ≤ÑÎäî ÏûêÏã†Ïùò ÎπÑÎ∞ÄÌÇ§Î•º Í∞ÄÏßÄÍ≥† ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Í∞Ä Î≥¥ÎÇ∏ Îç∞Ïù¥ÌÑ∞Î•º Î≥µÌò∏Ìôî Ìï©ÎãàÎã§. ÏòàÎ•º Îì§Î©¥ <code>RSA, Diffe-Hellman, ECC, etc</code></p><p><code>Í≥µÍ∞úÌÇ§</code> Îäî Í≥µÏú†ÎêòÏßÄÎßå <code>ÏïîÌò∏ÌÇ§</code> Îäî Í≥µÍ∞úÎêòÏßÄ ÏïäÍ∏∞Ïóê Í≥µÍ∞úÌÇ§Í∞Ä Ï§ëÍ∞ÑÏóê ÌÉàÏ∑®ÎêòÏñ¥ÎèÑ Îç∞Ïù¥ÌÑ∞Î•º ÏïàÏ†ÑÌïòÍ≤å ÏßÄÌÇ¨ Ïàò ÏûàÏäµÎãàÎã§. </p><p>ÌïòÏßÄÎßå Î¨∏Ï†úÎäî ÎπÑÎåÄÏπ≠Ìòï ÏïîÌò∏Îäî ÎåÄÏπ≠Ìòï ÏïîÌò∏Ïóê ÎπÑÌï¥ ÎäêÎ¶¨Í≥† ÎßéÏùÄ ÏûêÎ£åÎ•º ÏïîÌò∏ÏôÄ Î≥µÌò∏Ìôî ÌïòÎäîÎç∞ Î∂àÌé∏Ìï©ÎãàÎã§ Îã®Ï†êÏù¥ ÏûàÏäµÎãàÎã§. </p></li></ul><h2 id="ÏïîÌò∏-ÏïåÍ≥†Î¶¨Ï¶ò"><a href="#ÏïîÌò∏-ÏïåÍ≥†Î¶¨Ï¶ò" class="headerlink" title="ÏïîÌò∏ ÏïåÍ≥†Î¶¨Ï¶ò"></a>ÏïîÌò∏ ÏïåÍ≥†Î¶¨Ï¶ò</h2><h3 id="Îã®Î∞©Ìñ•"><a href="#Îã®Î∞©Ìñ•" class="headerlink" title="Îã®Î∞©Ìñ•"></a>Îã®Î∞©Ìñ•</h3><ol><li><code>SHA</code> : Í∞ÄÏû• ÎåÄÌëúÏ†ÅÏù∏ Ìï¥ÏãúÌï®Ïàò</li><li><code>PBKDF2</code> : Ìï¥ÏãúÌï®ÏàòÏùò Ïª®ÌÖåÏù¥ÎÑàÏù∏ PBKDF2Îäî ÏÜîÌä∏Î•º Ï†ÅÏö©Ìïú ÌõÑ Ìï¥Ïãú Ìï®ÏàòÏùò Î∞òÎ≥µ ÌöüÏàòÎ•º ÏûÑÏùòÎ°ú ÏÑ†ÌÉùÌï† Ïàò ÏûàÎã§. PBKDF2Îäî Íµ¨ÌòÑÌïòÍ∏∞ Ïâ¨Ïö¥ ÏïåÍ≥†Î¶¨Ï¶òÏù¥Î©∞ SHAÏôÄ Í∞ôÏù¥ Í≤ÄÏ¶ùÎêú Ìï¥Ïãú Ìï®ÏàòÎßå ÏÇ¨Ïö©Ìï©ÎãàÎã§.</li><li><code>bcrypt</code> : Ìå®Ïä§ÏõåÎìú Ï†ÄÏû•ÏùÑ Î™©Ï†ÅÏúºÎ°ú ÏÑ§Í≥ÑÎêòÏóàÏúºÎ©∞ Í∞ÄÏû• ÎßéÏù¥ Ïì∞Ïù¥Îäî ÏïåÍ≥†Î¶¨Ï¶òÏûÖÎãàÎã§. ÏûÖÎ†•Í∞íÏùÑ 72 byteÎ°ú Ìï¥ÏïºÌïòÍ∏∞ ÎïåÎ¨∏Ïóê Ï°∞Í∏à ÏÇ¨Ïö©Ïóê Î∂àÌé∏Ìï®Ïù¥ ÏûàÏùÑ Ïàò ÏûàÏäµÎãàÎã§. </li><li><code>scrypt</code> : scryptÎäî ÏÉÅÎåÄÏ†ÅÏúºÎ°ú ÏµúÏã† ÏïåÍ≥†Î¶¨Ï¶òÏù¥Î©∞ ÏúÑÏóê ÏïåÍ≥†Î¶¨Ï¶òÎì§ Î≥¥Îã§ Îçî ÏÑ±Îä•Ï†ÅÏúºÎ°ú Îõ∞Ïñ¥ÎÇúÎã§Í≥† ÌèâÍ∞ÄÎêòÏßÄÎßå Ïûò ÏïåÎ†§Ï†∏ ÏûàÏßÄ ÏïäÏäµÎãàÎã§. scryptÎäî Îã§Ïù¥Ï†úÏä§Ìä∏Î•º ÏÉùÏÑ±Ìï† Îïå Î©îÎ™®Î¶¨ Ïò§Î≤ÑÌó§ÎìúÎ•º Í∞ñÎèÑÎ°ù ÏÑ§Í≥ÑÎêòÏñ¥, ÏñµÏßÄ Í∏∞Î≤ï Í≥µÍ≤© (brute-force attack)ÏùÑ ÏãúÎèÑÌï† Îïå Î≥ëÎ†¨Ìôî Ï≤òÎ¶¨Í∞Ä Îß§Ïö∞ Ïñ¥Î†µÏäµÎãàÎã§. Îî∞ÎùºÏÑú PBKDF2Î≥¥Îã§ ÏïàÏ†ÑÌïòÍ≥† bcryptÏóê ÎπÑÌï¥ Îçî Í≤ΩÏüÅÎ†• ÏûàÎã§Í≥† Ïó¨Í≤®ÏßëÎãàÎã§. </li></ol><h3 id="ÏñëÎ∞©Ìñ•"><a href="#ÏñëÎ∞©Ìñ•" class="headerlink" title="ÏñëÎ∞©Ìñ•"></a>ÏñëÎ∞©Ìñ•</h3><ol><li><code>AES</code>:  ÌòÑÏû¨ Í∞ÄÏû• Î≥¥Ìé∏Ï†ÅÏúºÎ°ú Ïì∞Ïù¥Îäî ÏïîÌò∏ÏôÄ Î∞©ÏãùÏù¥Î©∞ ÎØ∏Íµ≠ ÌëúÏ§Ä Î∞©ÏãùÏù∏ AES. 128 ~ 256 byte ÌÇ§Î•º Ï†ÅÏö© Ìï† Ïàò ÏûàÏñ¥ÏÑú Î≥¥ÏïàÏÑ±Ïù¥ Îõ∞Ïñ¥ÎÇú Í≥µÍ∞úÎêú ÏïåÍ≥†Î¶¨Ï¶òÏûÖÎãàÎã§. </li><li><code>RSA</code> : Í≥µÍ∞úÌÇ§ ÏïîÌò∏ ÏãúÏä§ÌÖúÏùò ÌïòÎÇòÎ°ú ÏïîÌò∏ÏôÄ ÎøêÎßå ÏïÑÎãàÎùº Ï†ÑÏûêÏÑúÎ™ÖÍπåÏßÄ Í∞ÄÏ¶ùÌïú ÏïåÍ≥†Î¶¨Ï¶òÏûÖÎãàÎã§. </li></ol><h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h4><p><a href="https://sieunlim.tistory.com/16">https://sieunlim.tistory.com/16</a></p><p><a href="https://record22.tistory.com/44">https://record22.tistory.com/44</a></p>]]></content:encoded>
      
      <comments>https://minkyuchoi-07.github.io/2019/12/07/encryption-decryption/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Realtime Virus Scanning</title>
      <link>https://minkyuchoi-07.github.io/2019/11/20/nifi-virus-scanning/</link>
      <guid>https://minkyuchoi-07.github.io/2019/11/20/nifi-virus-scanning/</guid>
      <pubDate>Wed, 20 Nov 2019 20:14:06 GMT</pubDate>
      <description>
      
        &lt;p&gt;To protect our system and computer we should make sure that data which we download is clean. Everytime we bring data to our system or user upload data such as file attachments, we must make sure that data is free from viruses and trojans. &lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>To protect our system and computer we should make sure that data which we download is clean. Everytime we bring data to our system or user upload data such as file attachments, we must make sure that data is free from viruses and trojans. </p><span id="more"></span><p>If our system has sensitive data and critical for operation you have to be more cautious about bringing data to your system - cyber attack, nowadays, is being serious and cunning. </p><p>In a normal usecase, we set up Anti Virus (AV) scanner on a file system. AV scanner monitor our file system and RAM in real-time or batch. However, it cannot make sure that each file doesn‚Äôt have any malicious content in real-time. In this project, we will use two open source products to detect virus/trojan in realtime. We are going to use <code>Apache Nifi</code> and <code>ClamAV</code> </p><p><a href="https://nifi.apache.org/">Apache Nifi</a> is a very powerful, easy to use and stable system to process and distribute data between disparate system. Apache Nifi is a real time data ingestion platform, which can transfer and manage data transfer between different sources and destination systems. </p><p><a href="https://www.clamav.net/">ClamAV</a> is an open source antivirus engine for detecting trojans, viruses, malware &amp; other malicious threats. </p><h2 id="1-Usecase"><a href="#1-Usecase" class="headerlink" title="1. Usecase"></a>1. Usecase</h2><p>A usecase is that user need to transfer some files to the applicaion, and we have to make sure that the files don‚Äôt contain any malicious codes or contents. Since this is not bulk transformation, we want to transfer a file to endpoint in realtime after scanning. A diagram below is a high level work flow of this usecase. </p><p><img src="/2019/11/20/nifi-virus-scanning/workflow1.png" alt="workflow1"></p><h2 id="2-Setting-Nifi-Server"><a href="#2-Setting-Nifi-Server" class="headerlink" title="2. Setting Nifi Server"></a>2. Setting Nifi Server</h2><p>There are many different ways that you could set up Nifi server depending on the operating system. In this project, I am using <code>Ubuntu 16.04</code>. </p><h3 id="Updating-and-Upgrading-apt-get"><a href="#Updating-and-Upgrading-apt-get" class="headerlink" title="Updating and Upgrading apt-get"></a>Updating and Upgrading apt-get</h3><figure class="highlight shell hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">apt-get autoclean</span><br><span class="line">apt-get clean all</span><br><span class="line">apt-get -y update</span><br><span class="line">apt-get -y upgrade</span><br></pre></td></tr></table></figure><h3 id="Installing-Java-JRE"><a href="#Installing-Java-JRE" class="headerlink" title="Installing Java (JRE)"></a>Installing Java (JRE)</h3><p>Apache Nifi is built on Java. We have to have java installed in the system</p><figure class="highlight shell hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt install oracle-java8-installer -y</span><br></pre></td></tr></table></figure><h3 id="Installing-Nifi"><a href="#Installing-Nifi" class="headerlink" title="Installing Nifi"></a>Installing Nifi</h3><figure class="highlight shell hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">wget &quot;https://www-us.apache.org/dist/nifi/1.10.0/nifi-1.10.0-bin.tar.gz&quot;</span><br><span class="line"></span><br><span class="line">mkdir /opt/nifi</span><br><span class="line"></span><br><span class="line">tar -xvzf nifi-1.10.0-bin.tar.gz --directory /opt/nifi --strip-components 1</span><br></pre></td></tr></table></figure><h3 id="Set-JAVA-HOME"><a href="#Set-JAVA-HOME" class="headerlink" title="Set JAVA_HOME"></a>Set JAVA_HOME</h3><figure class="highlight shell hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bash_profile</span><br><span class="line">export JAVA_HOME=/usr/lib/jvm/java-8-oracle</span><br><span class="line">source ~./bashrc</span><br></pre></td></tr></table></figure><h3 id="Start-Stop-Apache-Nifi"><a href="#Start-Stop-Apache-Nifi" class="headerlink" title="Start/Stop Apache Nifi"></a>Start/Stop Apache Nifi</h3><figure class="highlight shell hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/opt/nifi/bin/nifi.sh start</span><br><span class="line"><span class="meta prompt_">#</span><span class="hljs-language-bash">or</span></span><br><span class="line">/opt/nifi/bin/nifi.sh stop</span><br></pre></td></tr></table></figure><h3 id="Get-Started"><a href="#Get-Started" class="headerlink" title="Get Started"></a>Get Started</h3><p>You should open a browser to access NiFI GUI.</p><figure class="highlight shell hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="hljs-language-bash">default</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="hljs-language-bash">http://localhost:8080/nifi</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="hljs-language-bash">or</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="hljs-language-bash">http://IP-Address:8080/nifi</span></span><br></pre></td></tr></table></figure><p>If you need to change port:</p><figure class="highlight shell hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vi /opt/nifi/conf/nifi.properties</span><br><span class="line"><span class="meta prompt_"># </span><span class="hljs-language-bash">change the defalt port to what you desire</span></span><br></pre></td></tr></table></figure><p>If everything is good you should be able to see this screen.</p><p><img src="/2019/11/20/nifi-virus-scanning/nifi_main_page.png" alt="nifi_main_page"></p><h2 id="2-Setting-ClamAV-Server-at-rest"><a href="#2-Setting-ClamAV-Server-at-rest" class="headerlink" title="2. Setting ClamAV Server at rest"></a>2. Setting ClamAV Server at rest</h2><p>We are going to deply a virus scanner and make it usable in a server at REST. Even though we have multiple applications like one for email attachment, SFTP, etc., we just need to deploy a AV scanner for many applicaions.  </p><p>Simple Clam AV REST Proxy. This will be built on top of clamav-java. Pleas fine more detail <a href="https://github.com/solita/clamav-rest">here</a>.</p><p>We need two containers. One is <a href="https://hub.docker.com/r/mkodockx/docker-clamav">ClamAV daemon</a> as a Docker images. It <em>builds</em> with a current virus database and <em>runs</em> <code>freshclam</code> in the background constantly updating the virus signature database. <code>clamd</code> itself is listening on exposed port <code>3310</code>.</p><p>Another one is the server implementation. This is a precompiled and packaged docker container running the server. You also need the ClamAV virus scanner for the REST endpoint.</p><p>To run use something like this.</p><ol><li>Start ClamAV server, using <a href="https://hub.docker.com/r/mkodockx/docker-clamav/">https://hub.docker.com/r/mkodockx/docker-clamav/</a> here <code>docker run -d --name clamav-server -p 3310:3310 mkodockx/docker-clamav</code></li><li>Test that it‚Äôs running ok: <code>curl localhost:3310</code> <code>UNKNOWN COMMAND</code></li><li>Start the REST API image, clamd-server docker container linked to this container. <code>docker run -d -e &#39;CLAMD_HOST=clamav-server&#39; -p 8080:8080 --link clamav-server:clamav-server -t -i lokori/clamav-rest</code></li><li>Test the REST api: <code>curl localhost:8080</code> <code>Clamd responding: true</code></li></ol><p><strong>Testing the REST service</strong></p><p>You can use <a href="http://curl.haxx.se/">curl</a> as it‚Äôs REST. Here‚Äôs an example test session:</p><figure class="highlight plaintext hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">curl localhost:8080</span><br><span class="line">Clamd responding: true</span><br><span class="line"></span><br><span class="line">curl -F &quot;name=blabla&quot; -F &quot;file=@./eicar.txt&quot; localhost:8080/scan</span><br><span class="line">Everything ok : false</span><br></pre></td></tr></table></figure><p>EICAR is a test file which is recognized as a virus by scanners even though it‚Äôs not really a virus. Read more <a href="http://www.eicar.org/86-0-Intended-use.html">EICAR information here</a>.</p><h2 id="3-Design-Dataflow-in-Nifi"><a href="#3-Design-Dataflow-in-Nifi" class="headerlink" title="3. Design Dataflow in Nifi"></a>3. Design Dataflow in Nifi</h2><p>In our previous discussion, we‚Äôve setup nifi server. </p><p>We‚Äôll use three processor to make it working. <code>GetFile</code>, <code>ExecuteStreamCommand</code>, <code>RouteOnAttribute</code> and <code>PutFile</code>. <code>GetFile</code> and <code>PutFile</code> can be chnaged to any endpoint of your application. For example, we could get a file from SFTP and put file to HDFS. </p><p><img src="/2019/11/20/nifi-virus-scanning/Nifi-Data-Flow.png" alt="Nifi-Data-Flow"></p><p>I would like to focus on <code>ExecuteStreamCommand</code> becase rest of processors are straight forward. Please find more information about those processors from an offical <a href="https://nifi.apache.org/">Apache Nifi Website</a>.</p><p><code>ExecuteStreamCommand</code> will executes an external command on the contents of a flow file, and creates a new flow file with the results of the command. We will use Python. Therefore, when files are come from GetFile Nifi will execute a python script to check the virus via API from ClamAV server. You should install <code>python</code> properly in your Nifi server. </p><p> ![ExecuteStreamCommand Configuration](/2019/11/20/nifi-virus-scanning/ExecuteStreamCommand Configuration.png)</p><p>This is a configuration of <code>Command Arguments</code></p><ol><li><code>Command Path</code> is where your python command located.</li><li><code>Working Directory</code> is where your python script is located.</li><li><code>Command Arguments</code> is your python script</li><li><code>OutPut Destination Attribute</code> Make sure that you define this value because we have to keep our content of file. By doing so we will write the result of scanning as an attribute. And then we will sort out files throught <code>RouteOnAttribute</code> processor. </li></ol><p>Let‚Äôs take a look our python script.</p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> sys</span><br><span class="line"><span class="hljs-keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">if</span> __name__ ==<span class="hljs-string">&#x27;__main__&#x27;</span>:</span><br><span class="line">      url = <span class="hljs-string">&#x27;http://localhost:9090/scan&#x27;</span></span><br><span class="line">      payload = &#123;<span class="hljs-string">&#x27;name&#x27;</span>: <span class="hljs-string">&#x27;value1&#x27;</span>&#125;</span><br><span class="line">      systemin = sys.stdin</span><br><span class="line">      files = &#123;<span class="hljs-string">&#x27;file&#x27;</span>: systemin&#125;</span><br><span class="line">      r = requests.post(url, files=files, data=payload)</span><br><span class="line">      <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;false&#x27;</span> <span class="hljs-keyword">in</span> r.text:</span><br><span class="line">         sys.stdout.write(<span class="hljs-string">&quot;False&quot;</span>)</span><br><span class="line">          </span><br><span class="line">      sys.stdout.write(<span class="hljs-string">&quot;True&quot;</span>)</span><br></pre></td></tr></table></figure><p>If a file is clean, it will have an Attribute value <code>True</code>. If it‚Äôs not an attribute value will look like this <code>FalseTrue</code> . We will route our files based one this value. Let‚Äôs check the configuration of <code>RouteOnAttribute</code>.</p><p><img src="/2019/11/20/nifi-virus-scanning/RouteOnAttribute.png" alt="RouteOnAttribute Configuration"></p><p>It will let your nifi to send your files to next processor only if the files are clean. </p><h2 id="4-Conclusion"><a href="#4-Conclusion" class="headerlink" title="4. Conclusion"></a>4. Conclusion</h2><p>It might not be only way to do this process. However, both Nifi and Clam are open source, so we don‚Äôt need to purchase any other license like Mcafee. Also with these simple tools we could process the files in realtime, and it works fairly well! Hopefully you enjoy this article. If you have a question or comment, you are very welcome to email me at any time. </p><h5 id="References"><a href="#References" class="headerlink" title="References"></a>References</h5><p><a href="https://www.tutorialspoint.com/apache_nifi/apache_nifi_introduction.html">link1</a><br><a href="https://dev.solita.fi/2015/06/02/rest-virusscan.html">link2</a><br><a href="https://hub.docker.com/r/lokori/clamav-rest/">link3</a><br><a href="https://hub.docker.com/r/mkodockx/docker-clamav">link4</a></p>]]></content:encoded>
      
      <comments>https://minkyuchoi-07.github.io/2019/11/20/nifi-virus-scanning/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Secure File Transfer Protocol (SFTP)</title>
      <link>https://minkyuchoi-07.github.io/2019/11/19/sftp/</link>
      <guid>https://minkyuchoi-07.github.io/2019/11/19/sftp/</guid>
      <pubDate>Tue, 19 Nov 2019 00:34:49 GMT</pubDate>
      <description>
      
        &lt;p&gt;SFTP (SSH File Transfer Protocol) is a secure file transfer protocol. It runs over the &lt;a href=&quot;https://www.ssh.com/ssh/protocol/&quot;&gt;SSH protocol&lt;/a&gt;. It supports the full security and authentication functionality of SSH.&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>SFTP (SSH File Transfer Protocol) is a secure file transfer protocol. It runs over the <a href="https://www.ssh.com/ssh/protocol/">SSH protocol</a>. It supports the full security and authentication functionality of SSH.</p><span id="more"></span><p><strong>System Requirement</strong></p><blockquote><p>Ubuntu 16.04</p></blockquote><h2 id="Step-1-OpenSSH"><a href="#Step-1-OpenSSH" class="headerlink" title="Step 1 - OpenSSH"></a>Step 1 - OpenSSH</h2><p>First, we need to check the SSH connection. By default OpenSSH comes with the most of the Lunux system. Please confirm this with this command.</p><figure class="highlight shell hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -v localhost</span><br></pre></td></tr></table></figure><p>If everything is good, you should be able to see this.</p><figure class="highlight plaintext hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; debug1: Connecting to localhost [127.0.0.1] port 22.</span><br><span class="line">&gt; debug1: Connection established.</span><br></pre></td></tr></table></figure><p>If you don‚Äôt have OpenSSH set up. You should install it on your system. </p><figure class="highlight shell hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sudo apt update</span><br><span class="line">sudo apt install openssh-server</span><br><span class="line"></span><br><span class="line">sudo systemctl stop ssh.service</span><br><span class="line">sudo systemctl start ssh.service</span><br><span class="line">sudo systemctl enable ssh.service</span><br></pre></td></tr></table></figure><h2 id="Step-2-Create-SFTP-GROUP-and-USER"><a href="#Step-2-Create-SFTP-GROUP-and-USER" class="headerlink" title="Step 2 - Create SFTP GROUP and USER"></a>Step 2 - Create SFTP GROUP and USER</h2><p><strong>Create a New User</strong></p><p>Switch to the root user:</p><figure class="highlight shell hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo -s</span><br></pre></td></tr></table></figure><p>Add a new user</p><figure class="highlight shell hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">adduser &lt;UbuntuUsername&gt;</span><br></pre></td></tr></table></figure><p>You will be prompted to add a password. Put a simple password and change it later.</p><p><strong>Create a Group</strong></p><p>We have to create the sftp_group first. You could name it whatever you want.</p><figure class="highlight shell hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo groupadd sftp_group</span><br></pre></td></tr></table></figure><p>Now, we could add user into this group</p><figure class="highlight shell hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo usermod -aG sftp_group &lt;UbuntuUsername&gt;</span><br></pre></td></tr></table></figure><h2 id="Step-3-Configure-SFTP-Chroot"><a href="#Step-3-Configure-SFTP-Chroot" class="headerlink" title="Step 3 - Configure SFTP / Chroot"></a>Step 3 - Configure SFTP / Chroot</h2><p>A chroot enable system to isolate application form the rest of your computer by limiting them. If you turn on chroot on user account, the account will be isolated and can only access its own directory and files.</p><p>There are two different ways which you could do access control. </p><p><strong>Locking down per user</strong></p><p>We might need to provide limited access to our user because if we give full access to our user it would be a huge security flaw. If you want to lock down user to only specific directory to add and remove files, please follow steps below.</p><ol><li>Create desired path and directory.</li></ol><figure class="highlight shell hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="hljs-language-bash">For example</span></span><br><span class="line">/home/sftp_root/sftp_home</span><br></pre></td></tr></table></figure><p><code>/home/sftp_root</code> is owned by root while <code>../sftp_home</code> can be ownd by our user or user group</p><ol start="2"><li>Change a permission</li></ol><figure class="highlight shell hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chmod 755 /home/sftp_root</span><br></pre></td></tr></table></figure><p>This changes our permissions to only allow writing by the user who owns the directory while read and execute to everyone else.</p><figure class="highlight shell hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="hljs-language-bash">it changes a directory to be owned by the user root and group root.</span></span><br><span class="line">chown root:root /home/sftp_root</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="hljs-language-bash">it gives ownership to the user and usergroup only to sftp_home.</span></span><br><span class="line">chown &lt;User&gt;:&lt;Usergroup&gt; /home/sftp_root/sftp_home</span><br></pre></td></tr></table></figure><ol start="3"><li>Locking down user</li></ol><figure class="highlight shell hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc.ssh/sshd_config</span><br></pre></td></tr></table></figure><p>Find this and comment it out</p><figure class="highlight shell hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Subsystem sftp /var/lib/openssh/sftp-server</span><br><span class="line"><span class="meta prompt_">#</span><span class="hljs-language-bash">to</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="hljs-language-bash">Subsystem sftp /var/lib/openssh/sftp-server</span></span><br></pre></td></tr></table></figure><p>And add this:</p><figure class="highlight shell hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Subsystem sftp internal-sftp</span><br><span class="line">   </span><br><span class="line">Match User [Your New Username] ChrootDirectory /home/sftp_root</span><br><span class="line">X11Forwarding no</span><br><span class="line">AllowTcpForwarding no</span><br><span class="line">AllowAgentForwarding no</span><br><span class="line">ForceCommand internal-sftp</span><br><span class="line">PasswordAuthentication yes</span><br></pre></td></tr></table></figure><p><strong>Match User</strong>: Tells the SSH server to only apply the following settings to the one user</p><p><strong>ChrootDirectory:</strong> This tells the server what directory our user is allowed to ONLY work within this directory</p><p><strong>X11Forwading, AllowTCPForwarding, AllowAgentForwarding:</strong> Prohibits the user from port forwarding, tunneling and X11 forwarding fot the user. These are all security things.</p><p><strong>ForceCommand internal-sftp:</strong> Forces the SSH server to the run the SFTP program upon access which disables shell access.</p><p><strong>PasswordAuthentication:</strong> Allows for the user to login with a typed password. You can remove this is you would rather use a security key which is by far safer.</p><figure class="highlight shell hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl restart ssh.service</span><br><span class="line"><span class="meta prompt_">#</span><span class="hljs-language-bash">or</span></span><br><span class="line">/etc/init.d/ssh restart</span><br></pre></td></tr></table></figure><h4 id="Locking-down-User-Group"><a href="#Locking-down-User-Group" class="headerlink" title="Locking down User Group"></a>Locking down User Group</h4><p>Only step 4 is different from locking down per user. Add this:</p><figure class="highlight shell hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Subsystem sftp internal-sftp</span><br><span class="line"></span><br><span class="line">Match Group sftp_group</span><br><span class="line">X11Forwarding no</span><br><span class="line">AllowTcpForwarding no</span><br><span class="line">ChrootDirectory /home/sftp_root</span><br><span class="line">ForceCommand internal-sftp</span><br></pre></td></tr></table></figure><h5 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h5><p><a href="https://monovm.com/blog/connect-to-sftp-on-centos-without-shell-access/">Thumnail</a><br><a href="http://www.inanzzz.com/index.php/post/ef2z/setting-up-a-sftp-server-and-users-on-ubuntu-16-04">link1</a><br><a href="https://websiteforstudents.com/setup-retrictive-sftp-with-chroot-on-ubuntu-16-04-17-10-and-18-04/">link2</a></p>]]></content:encoded>
      
      <comments>https://minkyuchoi-07.github.io/2019/11/19/sftp/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Simulation - Output Analysis</title>
      <link>https://minkyuchoi-07.github.io/2019/07/30/output-analysis/</link>
      <guid>https://minkyuchoi-07.github.io/2019/07/30/output-analysis/</guid>
      <pubDate>Tue, 30 Jul 2019 20:40:25 GMT</pubDate>
      <description>
      
        &lt;p&gt;Ananyzing the ouput of a simulation model is important. How can we be sure that our output is proper and will not hurt an experiment result using those outputs. &lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>Ananyzing the ouput of a simulation model is important. How can we be sure that our output is proper and will not hurt an experiment result using those outputs. </p><span id="more"></span><p>Keep this in mind - out is rearely i.i.d. Why do we worry about output? In put processes driving a simulation are random variables. It means our output from the simulation must be random. If we runs the simulation it only yields estimates of measure of system performace, and these estimators are themselves random variables, and are therfore subject to sampling error. Sampling error must be taken into account to make valid inferences concerning system performance. </p><p><strong>Measures of Interest</strong></p><ul><li>Means - what is the mean customer waiting time? </li><li>Variances - how much is the waiting time liable to vary?</li><li>Quantiles - what‚Äôs the 99% quantile of the line length in a certain queue?</li><li>Sucess probabilities - will my job be completed on time?</li><li>Would like point estimators and confidence intervals for the above. </li></ul><p>There are two general types of simulations with respect to output analysis. To facilitate the presentation, we identify two types of simulations with respect to output analysis: </p><ul><li><em>Finite-Horizon (Terminating) Simulations</em> - Interested in short-term performance<ul><li>The termincation of finite-horizon simulation takes place at a specific time or is caused by the occurrence of a specific event.</li><li>EX1 - Mass transit system during rush hour</li><li>EX2 - Distribution system over one month</li></ul></li><li><em>Steady-State simulations</em> - Interested in long-term performance<ul><li>The purpose of <em>steady-state simulation</em> is to study the long-run behavior of a system. A performance measure is a steady-state parameter if it is a characteristic of the equilibrium distribution of an output process. </li><li>EX1 - Continuously operating communication system where the objective is the computation of the mean delay of a packet in the long run</li><li>EX2 - Distribution system over a long period of time</li><li>EX3 - Markov chains</li></ul></li></ul><p><strong>Finite-Horizon Simulation</strong></p><p>First thing we have to do to conduct this simulation is getting expected values from replications. So basically, we need to decide a number of independetn replications (IR). IR estimates Var($\bar{Y}_m$) by conducting $r$ independent simulation runs (replications) of the system under study, where each replication consists of $m$ observations. It is easy to make the replications independent - just re-initialize each replication with a different pseudo-random number seed Sample means from replication</p><p>If each run is started under the same operating conditions (e.g., all queues empty and idle), then the replication sample means $Z_1, Z_2, . . . , Z_r$ are $i.i.d.$ random variables. </p><p><img src="/2019/07/30/output-analysis/02.png" alt></p><p><img src="/2019/07/30/output-analysis/04.png" alt></p><p>Suppose we want to estimate the expected average waiting time for the first m = 5000 customers at the bank. We make r = 5 independent replications of the system, each initialized empty and<br>idle and consisting of 5000 waiting times. The resulting replicate means are:</p><p><img src="/2019/07/30/output-analysis/01.png" alt></p><p><img src="/2019/07/30/output-analysis/05.png" alt></p><p><strong>Steady-state simulation</strong></p><p>How about we need to simulate the entire time line? We should consider to use a steady-state simulation. Estimate some parameter of interest, e.g., the mean customer waiting time or the expected profit produced by a certain factory configuration. In particular, suppose the mean of this output is the unknown quantity $\mu$. We‚Äôll use the sample mean $\bar{Y}_n$ to estimate $\mu$</p><p>We must accompany the value of any point extimator with a measure of its variance. In stead of Var($\bar{Y}_n)$ we canestimate the <em>variance parameter</em>,</p><p><img src="/2019/07/30/output-analysis/06.png" alt></p><p>Thus, $\sigma^2$ is imply the sume of all covariances! $\sigma^2$ pops up all over the place: simulation output analysis, Brownian motions, fnancial engineering application, etc. </p><p><img src="/2019/07/30/output-analysis/07.png" alt></p><p>Many methods for estimating $\sigma^2$ and for conducting steady-state output analysis in general:</p><ol><li><p>Batch means</p><p>The method of <em>batch means</em> (BM) is often used to estimate $\sigma^2$ and to calculate confidence intervals for $\mu$</p><p>Idea: Divide one long simulation run into a number of contiguous batches, and then appeal to a central limit theorem to assume that the resulting batch sample means are approximately i.i.d. normal. </p><p>In particular, suppose that we partition $Y_1, Y_2, . . . , Y_n$ into $b$ nonoverlapping, contiguous batches, each consisting of $m$ observations (assume that $n = bm$) </p><p><img src="/2019/07/30/output-analysis/08.png" alt></p><p>The $i$th batch mean is the sample mean of the $m$ observations from batch $i = 1, 2, . . . , b$</p><p><img src="/2019/07/30/output-analysis/10.png" alt></p><p><img src="/2019/07/30/output-analysis/11.png" alt></p><p><img src="/2019/07/30/output-analysis/12.png" alt></p><p><img src="/2019/07/30/output-analysis/13.png" alt></p><p><img src="/2019/07/30/output-analysis/14.png" alt></p><p><img src="/2019/07/30/output-analysis/15.png" alt></p></li></ol><p>$E[H]$ decreases in b, though it smooths out around b = 30. A common recommendation is to take b =. 30 and concentrate on increasing the batch size m as much as possible. </p><p>The technique of BM is intuitively appealing and easy to understand. </p><p>But problems can come up if the Yj ‚Äôs are not stationary (e.g., if significant initialization bias is present), if the batch means are not normal, or if the batch means are not independent. </p><p>If any of these assumption violations exist, poor confidence interval coverage may result ‚Äî unbeknownst to the analyst.</p><p>To ameliorate the initialization bias problem, the user can truncate some of the data or make a long run </p><p>In addition, the lack of independence or normality of the batch means can be countered by increasing the batch size m.</p><p><img src="/2019/07/30/output-analysis/16.png" alt></p><p><img src="/2019/07/30/output-analysis/17.png" alt></p><p><img src="/2019/07/30/output-analysis/18.png" alt></p><p><img src="/2019/07/30/output-analysis/19.png" alt></p><p><img src="/2019/07/30/output-analysis/20.png" alt></p><p><img src="/2019/07/30/output-analysis/21.png" alt></p><p><img src="/2019/07/30/output-analysis/22.png" alt></p><h5 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h5><p><a href="https://www.computerhope.com/jargon/s/stdin.htm">Thumnail</a><br>Georgia Tech‚Äôs <code>ISYE6644</code> class content</p>]]></content:encoded>
      
      <comments>https://minkyuchoi-07.github.io/2019/07/30/output-analysis/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Simulation - Input Analysis</title>
      <link>https://minkyuchoi-07.github.io/2019/07/23/input-analysis/</link>
      <guid>https://minkyuchoi-07.github.io/2019/07/23/input-analysis/</guid>
      <pubDate>Tue, 23 Jul 2019 20:40:08 GMT</pubDate>
      <description>
      
        &lt;p&gt;How can we tell our random variables are well made? In simulation terminology, we have something called &lt;code&gt;input analysis&lt;/code&gt;.&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>How can we tell our random variables are well made? In simulation terminology, we have something called <code>input analysis</code>.</p><span id="more"></span><p>In my previous two postings, <a href>random number</a>, <a href>random variable</a>, I‚Äôve talked about how to generate a random number and how we could make a radom variable by using those random numbers. Then how can we tell our random variables are well made? In simulation terminology, we have something called <code>input analysis</code>. The random variables are our input, and we need to analyze those inputs to verify it‚Äôs relevance. If you use the wrong input random variables in the simulation model, it will result in wrong output. A proper input analysis can save you from <code>Garbage-in-garbage-out</code></p><p>How can we conduct a proper input analysis?</p><ol><li>We have to collect data<ul><li>Data Sampling - we could shuffle the data and take some samples from there</li></ul></li><li>We have to figure out a distribution of data<ul><li>Plot the data to histogram </li><li>Discrete vs contunuous</li><li>Univariate / Multivariate</li><li>If data is not enough ‚Äî we can guess a good distribution</li></ul></li><li>We have to do a statistical test to verify the distribution</li></ol><h2 id="1-Point-Estimation"><a href="#1-Point-Estimation" class="headerlink" title="1. Point Estimation"></a>1. Point Estimation</h2><p>A <em>statistic</em> can not explicitly depend on any <code>unknown parameters</code> because statics are based on the actural observations. <em>Statistics</em> are random variable - we could expect to have two different values of a static when we take two different samples. But we need to find the <code>unknown parameters</code>. How can we do it? we could estimate <code>unknown parameters</code> from the existing probability of distribution.  </p><p>Let $X_1, . . . . , X_n$ be i.i.d. Random Variables and let $M(X) ‚â° M(X_1, . . . . , X_n)$ be a statistic based on the $X_i$‚Äôs. <strong>Suppose we use $M(X)$ to estimate some unknown parameter $Œ∏$ Then $M(X)$ is called a <em>point estimator</em> for $Œ∏$ .</strong></p><ul><li>$\bar{x}$ is a point estimator for the mean $Œº = E[Xi]$</li><li>$S^2$ is a point estimator for the variance  $œÉ2 = Var(Xi) $</li></ul><p>*<em>It would be nice if $M(X)$ had certain properties: *</em></p><ul><li>Its expected value should equal the parameter it‚Äôs trying to estimate</li><li>It should have low variance</li></ul><p>We all know that good estimator should be unbiased because if we use the biased estimator we won‚Äôt figure out whether our model is good or not. We will be fooled by a biased estimator. $T(X)$ is <em>unbiased</em> for $Œ∏$ if $E[M(X)] = Œ∏$</p><p><strong>EX1</strong> -  Suppose $X_1, . . . , X_n$ are i.i.d. anything with mean Œº. Then</p><p><img src="/2019/07/19/random-variable/sample_mean.png" alt="sample_mean"></p><p>So $\bar{X}$ is alwys unbiased for $\mu$. That‚Äôs why ${\bar{X}}$ is called the <em>sample mean</em> </p><p><strong>EX2</strong> -  Suppose $X_1, . . . , X_n$ are i.i.d. anything with mean Œº and variance $\sigma^2$. Then</p><p><img src="/2019/07/19/random-variable/sample_variance.png" alt="sample_variance"></p><p>Thus, $S^2$ is always unbiased for $\sigma^2$. This is why $S^2$ is called the sample variance. </p><h2 id="2-Mean-Square-Error"><a href="#2-Mean-Square-Error" class="headerlink" title="2. Mean Square Error"></a>2. Mean Square Error</h2><p>In a perfect sceanario, our estimator will be exactly same as $\theta$. Then we will see no error. However, it‚Äôs not the real case. Our goal is to reduce the error between an estimator and $\theta$. </p><p>The <em>mean squared error</em> of an estimator $M(X)$ of $\theta$ is </p><p>$$<br>\begin{aligned}<br> MSE(M(X)) ‚â° E[(M(X)-\theta)^2]<br>\end{aligned}<br>$$</p><p>$$<br>\begin{aligned}<br> Baia(M(X)) ‚â° E[T(X) - \theta]<br>\end{aligned}<br>$$</p><p>We could interpret MSE like this: </p><p><img src="/2019/07/19/random-variable/mse.png" alt="mse"></p><p>Lower MSE mean we are avoiding the bias and <strong>variance</strong>. Our goal should be finding a good estimator which can lower our error. If $M_1(X)$ and $M_2(X)$ are two estimators of $\theta$, we‚Äôd usually prefer the one with the lower MSE ‚Äî even if it happens to have higer bias.</p><h2 id="3-Maximum-Linelihood-Estimators"><a href="#3-Maximum-Linelihood-Estimators" class="headerlink" title="3. Maximum Linelihood Estimators"></a>3. Maximum Linelihood Estimators</h2><p>What if we don‚Äôt have a set of data, but we have a pdf/pmf $f(x)$ of the distribution. How can we find the $\theta$?</p><p>Consider an i.i.d. random sample $X_1, . . . , X_n$, where each $X_i$ has pdf/pmf $f(x)$. Further, suppose that $\theta$ is some unknown parameter from $X_i$. The likelihood function is $L(\theta) ‚â° \prod f(x_i)$</p><p>The maximum likelihood estimator (MLE) of $\theta$ is the value of $\theta$ that maximizes $L(\theta)$. The MLE is a function of the $X_i$‚Äôs and is a RV. </p><p><strong>EX1</strong> -  Suppose $X_1, . . . , X_n$ ~ Exp$(\gamma)$. Find the MLE for $\gamma$</p><p><img src="/2019/07/19/random-variable/mle-1.png" alt="mle-1"></p><p>Since the natural log function is one-to-one, it‚Äôs easy to see that the $\gamma$ that maximizes $L(\gamma)$ also maximize $ln(L(\gamma))$</p><p><img src="/2019/07/19/random-variable/mle-2.png" alt="mle-2"></p><p><img src="/2019/07/19/random-variable/mle-3.png" alt="mle-3"></p><p>This implies that the MLE is $\hat{\gamma} = 1 / \bar{X}$</p><p><strong>Invariance Property</strong> </p><p>If $\hat{\theta}$ is the MLE of some parameter $\theta$ and $h(.)$ is a one-to-one function, then $h(\hat{\theta})$ is the MLE of $h(\theta)$</p><p>For Bern(p) distribution the MLE of $p$ is $\hat{p}=\bar{X}$ (which also happens to be unbiased). If we consider the 1:1 function $h(\theta) = \theta^2$ for ($\theta &gt; 0)$, then the <em>Invariance property</em> says that the MLE of $p^2$ is $\bar{X}^2$</p><p>But such a property does not hold for unbiasedness</p><p>$$<br>\begin{aligned}<br>E[S^2] = œÉ^2<br>\end{aligned}<br>but<br>\begin{aligned}<br>E[\sqrt{S^2}]= œÉ<br>\end{aligned}<br>$$</p><p>Really that MLE for $\sigma^2$ is $\hat{\sigma^2} = 1/n\sum(X_i - \bar{X})^2$. The good news is that we can still get the MLE for $\sigma$. If we consider the 1:1 function $h(\theta) = +\sqrt{\theta}$, then the invariance property says that the MLE of $\sigma$ is </p><p>$$<br>\begin{aligned}<br>\hat{\sigma} = \sqrt{\hat{\sigma^2}} = \sqrt{\sum(X_i-\bar{X})^2/n}<br>\end{aligned}<br>$$</p><h2 id="4-The-Method-of-Moments"><a href="#4-The-Method-of-Moments" class="headerlink" title="4. The Method of Moments"></a>4. The Method of Moments</h2><p>Recall: the $k$th moment of a random variable X is </p><p><img src="/2019/07/19/random-variable/mom-1.png" alt="mom-1"></p><p>Suppose $X_1, . . . , X_n$ are i.i.d. from p.m.f. / p.d.f. $f(x)$. Then the method of moments(MOM) estimator for $E[X^k]$ is</p><p>$$<br>m_k ‚â° 1/n\sum X^k_i<br>$$</p><p><img src="/2019/07/19/random-variable/mom-2.png" alt="mom-2"></p><h2 id="5-Goodness-of-Fit-Tests"><a href="#5-Goodness-of-Fit-Tests" class="headerlink" title="5. Goodness-of-Fit Tests"></a>5. Goodness-of-Fit Tests</h2><p>We finanlly guessed a reasonable distribution and then estimated the relevant parameters. Now let‚Äôs conduct a formal test to see just how sucessful our toils have been.</p><p>In particular, test</p><p>$$<br>H_0 : X_1, X_2, . . . , X_n - p.m.f / p.d.f. f(x)<br>$$</p><p>At level of significance</p><p>$$<br>\alpha ‚â° P(Reject H_0 | H_0 true) =P(\text{Type I Error})<br>$$</p><h5 id="Chi-Square-Test"><a href="#Chi-Square-Test" class="headerlink" title="Chi-Square-Test"></a>Chi-Square-Test</h5><p>Goodness-of-fit test procedure: </p><ol><li>Divide the domain of $f(x)$ into $k$ sets, say, $A_1, A_2, A_3, . . . , A_k$ (distinct points if X is discret r intervals if X is continuous)</li><li>Tally the actual number of observations that fall in ach set, say, $O_i i = 1, 2, . . . , k$. If $p_i ‚â° P(X ‚àà Ai)$, then $O_i \text{~ Bin(}n,p_i)$</li><li>Determine the expected number of observations that would fall in each set if $H_0$ were true, say, $E_i = E[O-I] = np_i, i = 1,2, . . . , k$</li><li>Calculate a test statistic based on the differences between the $E_i$ and $O_i$.</li></ol><p><img src="/2019/07/19/random-variable/gof.png" alt="gof"></p><ol start="5"><li>A large value of $X^2_0$ indicate a bad fit. We reject $H_0 \text{ if } \chi^2_0 &gt; \chi^2_{\alpha,k-1-s}$, where $s$ is the number of nuknown paramets from $f(x) that have to be estimated. </li></ol><p>Usual recommendation: For the $\chi^2$ g-o-f test to work, pick $k,n$ such that $E_i &gt;= 5$ and $n$ at least 30</p><p><strong>Kolmogorov_Smirnov Goodness-of-Fit Test</strong></p><p>We‚Äôll test $H_0 : X_1, X_2, . . . , X_n$ some distribution with $c.d.f. F (x)$.</p><p>Recall the difincation of the <em>empirical c.d.f. (also called the sample c.d.f)</em> of the data is</p><p>$$<br>\hat{F_n}(x) ‚â° (\text{number of } X_i &lt;= x) / n<br>$$</p><p>The Glivenko-Cantelli Lemma says that $\hat{F_n}(x) -&gt; F(x)$ for all $x$ as $ n-&gt;\infinite$.  So if $H_0$ is ture then $\hat{F_n}(x)$ should be a good approxmination to the true c.d.f. $F(x)$, for large $n$ The main question: Does the empirical distribution actually support the assumption that H0 is true? </p><p> <img src="/2019/07/19/random-variable/ks.png" alt="ks"></p><h5 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h5><p><a href="https://www.computerhope.com/jargon/s/stdin.htm">Thumnail</a><br>Georgia Tech‚Äôs <code>ISYE6644</code> class content</p>]]></content:encoded>
      
      <comments>https://minkyuchoi-07.github.io/2019/07/23/input-analysis/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Simulation - Random variable</title>
      <link>https://minkyuchoi-07.github.io/2019/07/19/random-variable/</link>
      <guid>https://minkyuchoi-07.github.io/2019/07/19/random-variable/</guid>
      <pubDate>Fri, 19 Jul 2019 04:00:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;Random variable can be generated from a good random number generator. If real variables has moved the reality, we could design a future with a good random variables.&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>Random variable can be generated from a good random number generator. If real variables has moved the reality, we could design a future with a good random variables.</p><span id="more"></span><blockquote><p>Author: Minkyu Choi<br>Last updated: 07/19/2019</p></blockquote><p><strong>Inverse Transform Method</strong><br>Inverse transform sampling is a method for generating random numbers from any probability distribution by using its inverse cumulative distribution F‚àí1(x)F‚àí1(x). Recall that the cumulative distribution for a random variable XX is FX(x)=P(X‚â§x)FX(x)=P(X‚â§x). In what follows, we assume that our computer can, on demand, generate independent realizations of a random variable UU uniformly distributed on [0,1]</p><p><strong>Cutpoint Method</strong><br>This inverse-transform method has the advantage of having an optimal O(n) setup time. However, the average number of steps required to sample X is not optimal, and if several samples of X are needed, then the cutpoint method offers an average number of two comparison steps needed to sample an observation, yet still has an O(n) initial setup time</p><p>Without loss of generality, we can assume that X = [1, n]. Also, let qi = P(X ‚â§ i). Then the idea behind the cutpoint method is to choose m ‚â• n, and define sets Q1, . . . , Qm for which</p><p><img src="/2019/07/19/random-variable/cutpoint.png" alt="Cutpoint Method"></p><p>for all i = 1, . . . , m. In words, the unit interval [0, 1] is partitioned into m equal sub-intervals of the form $[\frac{(i‚àí1)} m,  \frac{i}m)$, i = 1, . . . , m. And when U falls into the i th sub-interval, then Qi contains all the possible qj values for which F ‚àí1 (U) = j. That way, instead of searching through all of the q values, we save time by only examining the qj values in Qi , since these are the only possible values for which $F^{-1} (U) = j$.</p><p><strong>Convolution Method</strong></p><ul><li>Sum of n variables: $x = y_1 + y_2 + ‚Ä¶ y_n$</li><li>Generate n random variate yi‚Äôs and sum </li><li>For sums of two variables, pdf of x = convolution of pdfs of y1 and y2. Hence the name </li><li>Although no convolution in generation </li><li>If pdf or CDF = Sum ‚áí Composition </li><li>Variable x = Sum ‚áí Convolution</li></ul><p><strong>Acceptance-Rejection Method</strong><br>Finding an explicit formula for F ‚àí1 (y) for the cdf of a rv X we wish to generate, F(x) = P(X ‚â§ x), is not always possible. Moreover, even if it is, there may be alternative methods for generating a rv distributed as F that is more efficient than the inverse transform method or other methods we have come across. Here we present a very clever method known as the acceptance-rejection method.</p><p><strong>Composition Method</strong><br>Can be used when m can be expressed as a convex combination of other distributions Fi , where we hope to be able to sample from $F_i$ more easily than from F directly.</p><p><img src="/2019/07/19/random-variable/Cm.png" alt="Composition Method"></p><p><strong>References</strong><br><a href="https://newonlinecourses.science.psu.edu/stat414/node/104/">Link-1</a><br><a href="https://stephens999.github.io/fiveMinuteStats/inverse_transform_sampling.html">Link-2</a><br><a href="http://web.csulb.edu/~tebert/teaching/lectures/552/variate/variate.pdf">Link-3</a><br><a href="https://www.cse.wustl.edu/~jain/cse567-08/ftp/k_28rvg.pdf">Link-4</a><br><a href="http://www.columbia.edu/~ks20/4703-Sigman/4703-07-Notes-ARM.pdf">Link-5</a></p>]]></content:encoded>
      
      <comments>https://minkyuchoi-07.github.io/2019/07/19/random-variable/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Simulation - Random Number</title>
      <link>https://minkyuchoi-07.github.io/2019/07/18/random-number/</link>
      <guid>https://minkyuchoi-07.github.io/2019/07/18/random-number/</guid>
      <pubDate>Thu, 18 Jul 2019 13:27:50 GMT</pubDate>
      <description>
      
        &lt;p&gt;A simulation is not real but it can represent the real. It‚Äôs because a simulation is an imitation of real situation - it can‚Äôt be exact but it can be approximate.&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>A simulation is not real but it can represent the real. It‚Äôs because a simulation is an imitation of real situation - it can‚Äôt be exact but it can be approximate.</p><span id="more"></span><p>Most of simulation models are strated from generating random number because randomness creates value on a simulation modeling. It is really important to give an algorithm that produces a sequence of pseudo-random number (PRNs) $R_1, R_2,‚Ä¶$ that ‚Äúappear‚Äù to be iid Unif(0,1). There are many different Uniform(0,1) Generators.</p><p><strong>Output of random device</strong></p><ul><li>Nice randomness properties. However, Unif(0,1) sequence storage difficult, sot it‚Äôs tough to repeat experiment</li><li>Examples:<ul><li>flip a coin</li><li>particle count by Geiger coutner</li><li>least significant digits of atomic clock</li></ul></li></ul><p><strong>Table of random numbers</strong></p><ul><li>List of digits supplied in tables - A Million random Digits with 100,00 Normal Deviates.</li><li>Cumbersome, slow, table too small - not very useful </li></ul><p><strong>Mid-Square</strong></p><ul><li>Idea - Take the middle part of square of the previous random number. John von Neumann was a brilliant and fun-loving guy, but method is terrible</li><li>Example: Take $R_i = X_i/10000$, ‚àÄi, where the Xi‚Äôs are positive<br>integers &lt; 10000.</li><li>Set seed $X_0 = 6642$; then $6632^2$ = 43<strong>9834</strong>24</li><li>so $X_1 = 9834$; then $9834^2$ - 96<strong>7075</strong>56</li><li>so $X_2$ = 7075, etc,‚Ä¶</li><li>Unfortunately, positive serial correlation in $R_i$‚Äôs. Also, occasionally degenerates; eg., consider $X_i$ = 0003</li></ul><p><strong>Fibonacci</strong></p><ul><li>These methods are also no good!! </li><li>Take $X_i = (X_{i-1} + X_{i-2})mod(m), i = 1,2,‚Ä¶,$ where $R_i = X_i/m$ ,$m$ is the modulus, $X_01,X_0$ are seeds, and $a = b mod m$ if $a$ is the remainer of $b/m$ </li><li>Problem: small numbers follow small numbers</li><li>Also, it‚Äôs not possible to get $X_{i-1} &lt; X_{i+1} &lt; X_i$ or </li><li>$X_i &lt; X_{i+1} &lt; X_{i-1} $ (which should occur w.p 1/3)</li><li>$X_{i+1}$ </li></ul><p><strong>Linear congruential (most commonly used in practice</strong></p><ul><li>LCGs are the most widely used generators. These are pretty good when implemented properly. </li><li>$X_i = (aX_{i-1} + c) mod(m)$, where $X_0$ is the seed.</li><li>$R_i = X_i/m, i = 1,2,‚Ä¶$ </li><li>Choose a,c,m carefully to get good stastistical quality and long period or cycle length, i.e., time until LCG starts to repeat itself. </li><li>If $c = 0$, LCG is called a multiplicative generator </li></ul><p><strong>Tausworthe (linear recursion mod 2)</strong></p><ul><li><p>Tausworthe Generator is a kind of multicative recursive generator.</p></li><li><p>$X_{i+1} = (aX_{i-1} + c) mod(2)$, where $X_0$ is the seed.</p></li></ul><p><strong>Reference</strong></p><p>Georgia Tech‚Äôs <code>ISYE6644</code> class content</p>]]></content:encoded>
      
      <comments>https://minkyuchoi-07.github.io/2019/07/18/random-number/#disqus_thread</comments>
    </item>
    
  </channel>
</rss>
