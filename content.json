{"pages":[{"title":"","text":"","link":"/404.html"},{"title":"","text":"CV Viewer .pdf-container { position: relative; width: 100%; height: 0; padding-top: 141.4%; /* Aspect ratio for A4 */ overflow: hidden; } .pdf-container iframe { position: absolute; top: 0; left: 0; width: 100%; height: 100%; border: none; } .download-button { display: inline-block; padding: 10px 20px; background-color: #007bff; color: white; text-decoration: none; border-radius: 5px; margin-top: 20px; } It appears your browser doesn't support iframes. You can click here to download the PDF. Download CV (PDF)","link":"/cv/index.html"},{"title":"About Minkyu Choi","text":"IntroMinkyu Choi is an Artificial Intelligence (AI) research scientist and engineer with over five years of experience across various industries. Currently pursuing a Ph.D. at the University of Texas at Austin under Dr. Sandeep Chinchali‘s supervision. His research interests include Deep Reinforcement Learning, Neuro-Symbolic AI, Video Understanding, and Computer Vision Assurance. He possesses expertise in Data Engineering, Cloud and Edge Computing, DevSecOps, and MLOps, and has worked in commercial and government sectors on machine learning and deep learning projects. Google Scholar LinkedIn Email #contact-buttons { width: 100%; display: flex; } #contact-butons a.button { flex-grow: 1; } @media screen and (max-width: 720px) { #contact-buttons a.button { width: 100%; margin-right: 0; } } News2025 Febuary : My first authored paper is accepted to CVPR 2025! 2024 October : I present “Towards Neuro-Symbolic Video Understanding” at ECCV24 conference (oral presentation) 2024 July: Thrilled to announce that my paper “Towards Neuro-Symbolic Video Understanding” has been accepted to European Conference on Computer Vision (ECCV) 2024! 2024 June: Excited to announce that our paper has been accepted for publication in the Proceedings of the 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2024) as both an oral pitch and an interactive presentation. Click to expand 2023 April: Delighted to reveal that I have accepted a PhD program offer from the University of Texas at Austin! 2023 March: Ecstatic to announce that I have been honored with the prestigious fellowship from the Cockrell School of Engineering at The University of Texas at Austin. 2022 June: Delighted to share that I successfully presented our research paper at the esteemed ICUAS conference. 2021 May: Proudly announcing my recent achievement of obtaining a Master of Science degree in Analytics from the prestigious Georgia Tech. 2021 April: Thrilled to join Lockheed Martin’s Artificial Intelligence Center as a research engineer, where I’ll be contributing to pioneering AI research and innovation alongside a team of esteemed professionals. EducactionUniversity of Texas at AustinA Ph.D. Student in Artificial Intelligence (Fall 2023) Georgia Institute of TechnologyMS in Analytics Baruch CollegeBBA in Computer Information System ProfessionalLockheed Martin (AI Innovation team @Artificial Intelligence Center)Artificial Intelligence Research Engineer The Joint Artifical Intelligence Center (Now: Chief Digital and Artificial Intelligence Office)Artificial Intelligence Engineer Techfield LLCSubject Matter Expert in Big Data and Data Science Endpoint ClinicalData Engineer Argus Information and Advisory ServicesData Analyst PublicationChoi, Minkyu*, S P Sharan*, Sahil Shah, Harsh Goel, Mohammad Omama, and Sandeep P. Chinchali. “Neuro-Symbolic Evaluation of Text-to-Video Models using Formal Verification.” In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2025. Choi, Minkyu, Harsh Goel, Mohammad Omama, Yunhao Yang, Sahil Shah, and Sandeep Chinchali. “Towards Neuro-Symbolic Video Understanding.” In Proceedings of the European Conference on Computer Vision (ECCV), September 2024 – Accepted for oral presentation! Walker, Thayne T., Jaime S. Ide, Minkyu Choi, Michael Guarino, and Kevin Alcedo. “Multi-Agent Reinforcement Learning with Epistemic Priors.” In 2023 9th International Conference on Control, Decision and Information Technologies (CoDIT), pp. 2514-2518. IEEE, 2023. Choi, Minkyu, Max Filter, Kevin Alcedo, Thayne T. Walker, David Rosenbluth, and Jaime S. Ide. “Soft actor-critic with inhibitory networks for retraining UAV controllers faster.” In 2022 International Conference on Unmanned Aircraft Systems (ICUAS), pp. 1561-1570. IEEE, 2022. PreprintMinkyu Choi*, Sharan, S. P*.,Sahil Shah, Harsh Goel, Mohammad Omama, and Sandeep Chinchali. “Neuro-Symbolic Evaluation of Text-to-Video Models Using Formal Verification.” arXiv preprint arXiv:2411.16718 (2024). Salfity, Jonathan, Selma Wanna, Minkyu Choi, and Mitch Pryor. “Temporal and Semantic Evaluation Metrics for Foundation Models in Post-Hoc Analysis of Robotic Sub-tasks.” arXiv preprint arXiv:2403.17238 (2024).","link":"/about/index.html"}],"posts":[{"title":"Simulation - Random Number","text":"A simulation is not real but it can represent the real. It’s because a simulation is an imitation of real situation - it can’t be exact but it can be approximate. Most of simulation models are strated from generating random number because randomness creates value on a simulation modeling. It is really important to give an algorithm that produces a sequence of pseudo-random number (PRNs) $R_1, R_2,…$ that “appear” to be iid Unif(0,1). There are many different Uniform(0,1) Generators. Output of random device Nice randomness properties. However, Unif(0,1) sequence storage difficult, sot it’s tough to repeat experiment Examples: flip a coin particle count by Geiger coutner least significant digits of atomic clock Table of random numbers List of digits supplied in tables - A Million random Digits with 100,00 Normal Deviates. Cumbersome, slow, table too small - not very useful Mid-Square Idea - Take the middle part of square of the previous random number. John von Neumann was a brilliant and fun-loving guy, but method is terrible Example: Take $R_i = X_i/10000$, ∀i, where the Xi’s are positiveintegers &lt; 10000. Set seed $X_0 = 6642$; then $6632^2$ = 43983424 so $X_1 = 9834$; then $9834^2$ - 96707556 so $X_2$ = 7075, etc,… Unfortunately, positive serial correlation in $R_i$’s. Also, occasionally degenerates; eg., consider $X_i$ = 0003 Fibonacci These methods are also no good!! Take $X_i = (X_{i-1} + X_{i-2})mod(m), i = 1,2,…,$ where $R_i = X_i/m$ ,$m$ is the modulus, $X_01,X_0$ are seeds, and $a = b mod m$ if $a$ is the remainer of $b/m$ Problem: small numbers follow small numbers Also, it’s not possible to get $X_{i-1} &lt; X_{i+1} &lt; X_i$ or $X_i &lt; X_{i+1} &lt; X_{i-1} $ (which should occur w.p 1/3) $X_{i+1}$ Linear congruential (most commonly used in practice LCGs are the most widely used generators. These are pretty good when implemented properly. $X_i = (aX_{i-1} + c) mod(m)$, where $X_0$ is the seed. $R_i = X_i/m, i = 1,2,…$ Choose a,c,m carefully to get good stastistical quality and long period or cycle length, i.e., time until LCG starts to repeat itself. If $c = 0$, LCG is called a multiplicative generator Tausworthe (linear recursion mod 2) Tausworthe Generator is a kind of multicative recursive generator. $X_{i+1} = (aX_{i-1} + c) mod(2)$, where $X_0$ is the seed. Reference Georgia Tech’s ISYE6644 class content","link":"/2019/07/18/random-number/"},{"title":"Simulation - Random variable","text":"Random variable can be generated from a good random number generator. If real variables has moved the reality, we could design a future with a good random variables. Author: Minkyu ChoiLast updated: 07/19/2019 Inverse Transform MethodInverse transform sampling is a method for generating random numbers from any probability distribution by using its inverse cumulative distribution F−1(x)F−1(x). Recall that the cumulative distribution for a random variable XX is FX(x)=P(X≤x)FX(x)=P(X≤x). In what follows, we assume that our computer can, on demand, generate independent realizations of a random variable UU uniformly distributed on [0,1] Cutpoint MethodThis inverse-transform method has the advantage of having an optimal O(n) setup time. However, the average number of steps required to sample X is not optimal, and if several samples of X are needed, then the cutpoint method offers an average number of two comparison steps needed to sample an observation, yet still has an O(n) initial setup time Without loss of generality, we can assume that X = [1, n]. Also, let qi = P(X ≤ i). Then the idea behind the cutpoint method is to choose m ≥ n, and define sets Q1, . . . , Qm for which for all i = 1, . . . , m. In words, the unit interval [0, 1] is partitioned into m equal sub-intervals of the form $[\\frac{(i−1)} m, \\frac{i}m)$, i = 1, . . . , m. And when U falls into the i th sub-interval, then Qi contains all the possible qj values for which F −1 (U) = j. That way, instead of searching through all of the q values, we save time by only examining the qj values in Qi , since these are the only possible values for which $F^{-1} (U) = j$. Convolution Method Sum of n variables: $x = y_1 + y_2 + … y_n$ Generate n random variate yi’s and sum For sums of two variables, pdf of x = convolution of pdfs of y1 and y2. Hence the name Although no convolution in generation If pdf or CDF = Sum ⇒ Composition Variable x = Sum ⇒ Convolution Acceptance-Rejection MethodFinding an explicit formula for F −1 (y) for the cdf of a rv X we wish to generate, F(x) = P(X ≤ x), is not always possible. Moreover, even if it is, there may be alternative methods for generating a rv distributed as F that is more efficient than the inverse transform method or other methods we have come across. Here we present a very clever method known as the acceptance-rejection method. Composition MethodCan be used when m can be expressed as a convex combination of other distributions Fi , where we hope to be able to sample from $F_i$ more easily than from F directly. ReferencesLink-1Link-2Link-3Link-4Link-5","link":"/2019/07/19/random-variable/"},{"title":"Simulation - Input Analysis","text":"How can we tell our random variables are well made? In simulation terminology, we have something called input analysis. In my previous two postings, random number, random variable, I’ve talked about how to generate a random number and how we could make a radom variable by using those random numbers. Then how can we tell our random variables are well made? In simulation terminology, we have something called input analysis. The random variables are our input, and we need to analyze those inputs to verify it’s relevance. If you use the wrong input random variables in the simulation model, it will result in wrong output. A proper input analysis can save you from Garbage-in-garbage-out How can we conduct a proper input analysis? We have to collect data Data Sampling - we could shuffle the data and take some samples from there We have to figure out a distribution of data Plot the data to histogram Discrete vs contunuous Univariate / Multivariate If data is not enough — we can guess a good distribution We have to do a statistical test to verify the distribution 1. Point EstimationA statistic can not explicitly depend on any unknown parameters because statics are based on the actural observations. Statistics are random variable - we could expect to have two different values of a static when we take two different samples. But we need to find the unknown parameters. How can we do it? we could estimate unknown parameters from the existing probability of distribution. Let $X_1, . . . . , X_n$ be i.i.d. Random Variables and let $M(X) ≡ M(X_1, . . . . , X_n)$ be a statistic based on the $X_i$’s. Suppose we use $M(X)$ to estimate some unknown parameter $θ$ Then $M(X)$ is called a point estimator for $θ$ . $\\bar{x}$ is a point estimator for the mean $μ = E[Xi]$ $S^2$ is a point estimator for the variance $σ2 = Var(Xi) $ *It would be nice if $M(X)$ had certain properties: * Its expected value should equal the parameter it’s trying to estimate It should have low variance We all know that good estimator should be unbiased because if we use the biased estimator we won’t figure out whether our model is good or not. We will be fooled by a biased estimator. $T(X)$ is unbiased for $θ$ if $E[M(X)] = θ$ EX1 - Suppose $X_1, . . . , X_n$ are i.i.d. anything with mean μ. Then So $\\bar{X}$ is alwys unbiased for $\\mu$. That’s why ${\\bar{X}}$ is called the sample mean EX2 - Suppose $X_1, . . . , X_n$ are i.i.d. anything with mean μ and variance $\\sigma^2$. Then Thus, $S^2$ is always unbiased for $\\sigma^2$. This is why $S^2$ is called the sample variance. 2. Mean Square ErrorIn a perfect sceanario, our estimator will be exactly same as $\\theta$. Then we will see no error. However, it’s not the real case. Our goal is to reduce the error between an estimator and $\\theta$. The mean squared error of an estimator $M(X)$ of $\\theta$ is $$\\begin{aligned} MSE(M(X)) ≡ E[(M(X)-\\theta)^2]\\end{aligned}$$ $$\\begin{aligned} Baia(M(X)) ≡ E[T(X) - \\theta]\\end{aligned}$$ We could interpret MSE like this: Lower MSE mean we are avoiding the bias and variance. Our goal should be finding a good estimator which can lower our error. If $M_1(X)$ and $M_2(X)$ are two estimators of $\\theta$, we’d usually prefer the one with the lower MSE — even if it happens to have higer bias. 3. Maximum Linelihood EstimatorsWhat if we don’t have a set of data, but we have a pdf/pmf $f(x)$ of the distribution. How can we find the $\\theta$? Consider an i.i.d. random sample $X_1, . . . , X_n$, where each $X_i$ has pdf/pmf $f(x)$. Further, suppose that $\\theta$ is some unknown parameter from $X_i$. The likelihood function is $L(\\theta) ≡ \\prod f(x_i)$ The maximum likelihood estimator (MLE) of $\\theta$ is the value of $\\theta$ that maximizes $L(\\theta)$. The MLE is a function of the $X_i$’s and is a RV. EX1 - Suppose $X_1, . . . , X_n$ ~ Exp$(\\gamma)$. Find the MLE for $\\gamma$ Since the natural log function is one-to-one, it’s easy to see that the $\\gamma$ that maximizes $L(\\gamma)$ also maximize $ln(L(\\gamma))$ This implies that the MLE is $\\hat{\\gamma} = 1 / \\bar{X}$ Invariance Property If $\\hat{\\theta}$ is the MLE of some parameter $\\theta$ and $h(.)$ is a one-to-one function, then $h(\\hat{\\theta})$ is the MLE of $h(\\theta)$ For Bern(p) distribution the MLE of $p$ is $\\hat{p}=\\bar{X}$ (which also happens to be unbiased). If we consider the 1:1 function $h(\\theta) = \\theta^2$ for ($\\theta &gt; 0)$, then the Invariance property says that the MLE of $p^2$ is $\\bar{X}^2$ But such a property does not hold for unbiasedness $$\\begin{aligned}E[S^2] = σ^2\\end{aligned}but\\begin{aligned}E[\\sqrt{S^2}]= σ\\end{aligned}$$ Really that MLE for $\\sigma^2$ is $\\hat{\\sigma^2} = 1/n\\sum(X_i - \\bar{X})^2$. The good news is that we can still get the MLE for $\\sigma$. If we consider the 1:1 function $h(\\theta) = +\\sqrt{\\theta}$, then the invariance property says that the MLE of $\\sigma$ is $$\\begin{aligned}\\hat{\\sigma} = \\sqrt{\\hat{\\sigma^2}} = \\sqrt{\\sum(X_i-\\bar{X})^2/n}\\end{aligned}$$ 4. The Method of MomentsRecall: the $k$th moment of a random variable X is Suppose $X_1, . . . , X_n$ are i.i.d. from p.m.f. / p.d.f. $f(x)$. Then the method of moments(MOM) estimator for $E[X^k]$ is $$m_k ≡ 1/n\\sum X^k_i$$ 5. Goodness-of-Fit TestsWe finanlly guessed a reasonable distribution and then estimated the relevant parameters. Now let’s conduct a formal test to see just how sucessful our toils have been. In particular, test $$H_0 : X_1, X_2, . . . , X_n - p.m.f / p.d.f. f(x)$$ At level of significance $$\\alpha ≡ P(Reject H_0 | H_0 true) =P(\\text{Type I Error})$$ Chi-Square-TestGoodness-of-fit test procedure: Divide the domain of $f(x)$ into $k$ sets, say, $A_1, A_2, A_3, . . . , A_k$ (distinct points if X is discret r intervals if X is continuous) Tally the actual number of observations that fall in ach set, say, $O_i i = 1, 2, . . . , k$. If $p_i ≡ P(X ∈ Ai)$, then $O_i \\text{~ Bin(}n,p_i)$ Determine the expected number of observations that would fall in each set if $H_0$ were true, say, $E_i = E[O-I] = np_i, i = 1,2, . . . , k$ Calculate a test statistic based on the differences between the $E_i$ and $O_i$. A large value of $X^2_0$ indicate a bad fit. We reject $H_0 \\text{ if } \\chi^2_0 &gt; \\chi^2_{\\alpha,k-1-s}$, where $s$ is the number of nuknown paramets from $f(x) that have to be estimated. Usual recommendation: For the $\\chi^2$ g-o-f test to work, pick $k,n$ such that $E_i &gt;= 5$ and $n$ at least 30 Kolmogorov_Smirnov Goodness-of-Fit Test We’ll test $H_0 : X_1, X_2, . . . , X_n$ some distribution with $c.d.f. F (x)$. Recall the difincation of the empirical c.d.f. (also called the sample c.d.f) of the data is $$\\hat{F_n}(x) ≡ (\\text{number of } X_i &lt;= x) / n$$ The Glivenko-Cantelli Lemma says that $\\hat{F_n}(x) -&gt; F(x)$ for all $x$ as $ n-&gt;\\infinite$. So if $H_0$ is ture then $\\hat{F_n}(x)$ should be a good approxmination to the true c.d.f. $F(x)$, for large $n$ The main question: Does the empirical distribution actually support the assumption that H0 is true? ReferenceThumnailGeorgia Tech’s ISYE6644 class content","link":"/2019/07/23/input-analysis/"},{"title":"Simulation - Output Analysis","text":"Ananyzing the ouput of a simulation model is important. How can we be sure that our output is proper and will not hurt an experiment result using those outputs. Keep this in mind - out is rearely i.i.d. Why do we worry about output? In put processes driving a simulation are random variables. It means our output from the simulation must be random. If we runs the simulation it only yields estimates of measure of system performace, and these estimators are themselves random variables, and are therfore subject to sampling error. Sampling error must be taken into account to make valid inferences concerning system performance. Measures of Interest Means - what is the mean customer waiting time? Variances - how much is the waiting time liable to vary? Quantiles - what’s the 99% quantile of the line length in a certain queue? Sucess probabilities - will my job be completed on time? Would like point estimators and confidence intervals for the above. There are two general types of simulations with respect to output analysis. To facilitate the presentation, we identify two types of simulations with respect to output analysis: Finite-Horizon (Terminating) Simulations - Interested in short-term performance The termincation of finite-horizon simulation takes place at a specific time or is caused by the occurrence of a specific event. EX1 - Mass transit system during rush hour EX2 - Distribution system over one month Steady-State simulations - Interested in long-term performance The purpose of steady-state simulation is to study the long-run behavior of a system. A performance measure is a steady-state parameter if it is a characteristic of the equilibrium distribution of an output process. EX1 - Continuously operating communication system where the objective is the computation of the mean delay of a packet in the long run EX2 - Distribution system over a long period of time EX3 - Markov chains Finite-Horizon Simulation First thing we have to do to conduct this simulation is getting expected values from replications. So basically, we need to decide a number of independetn replications (IR). IR estimates Var($\\bar{Y}_m$) by conducting $r$ independent simulation runs (replications) of the system under study, where each replication consists of $m$ observations. It is easy to make the replications independent - just re-initialize each replication with a different pseudo-random number seed Sample means from replication If each run is started under the same operating conditions (e.g., all queues empty and idle), then the replication sample means $Z_1, Z_2, . . . , Z_r$ are $i.i.d.$ random variables. Suppose we want to estimate the expected average waiting time for the first m = 5000 customers at the bank. We make r = 5 independent replications of the system, each initialized empty andidle and consisting of 5000 waiting times. The resulting replicate means are: Steady-state simulation How about we need to simulate the entire time line? We should consider to use a steady-state simulation. Estimate some parameter of interest, e.g., the mean customer waiting time or the expected profit produced by a certain factory configuration. In particular, suppose the mean of this output is the unknown quantity $\\mu$. We’ll use the sample mean $\\bar{Y}_n$ to estimate $\\mu$ We must accompany the value of any point extimator with a measure of its variance. In stead of Var($\\bar{Y}_n)$ we canestimate the variance parameter, Thus, $\\sigma^2$ is imply the sume of all covariances! $\\sigma^2$ pops up all over the place: simulation output analysis, Brownian motions, fnancial engineering application, etc. Many methods for estimating $\\sigma^2$ and for conducting steady-state output analysis in general: Batch means The method of batch means (BM) is often used to estimate $\\sigma^2$ and to calculate confidence intervals for $\\mu$ Idea: Divide one long simulation run into a number of contiguous batches, and then appeal to a central limit theorem to assume that the resulting batch sample means are approximately i.i.d. normal. In particular, suppose that we partition $Y_1, Y_2, . . . , Y_n$ into $b$ nonoverlapping, contiguous batches, each consisting of $m$ observations (assume that $n = bm$) The $i$th batch mean is the sample mean of the $m$ observations from batch $i = 1, 2, . . . , b$ $E[H]$ decreases in b, though it smooths out around b = 30. A common recommendation is to take b =. 30 and concentrate on increasing the batch size m as much as possible. The technique of BM is intuitively appealing and easy to understand. But problems can come up if the Yj ’s are not stationary (e.g., if significant initialization bias is present), if the batch means are not normal, or if the batch means are not independent. If any of these assumption violations exist, poor confidence interval coverage may result — unbeknownst to the analyst. To ameliorate the initialization bias problem, the user can truncate some of the data or make a long run In addition, the lack of independence or normality of the batch means can be countered by increasing the batch size m. ReferenceThumnailGeorgia Tech’s ISYE6644 class content","link":"/2019/07/30/output-analysis/"},{"title":"Secure File Transfer Protocol (SFTP)","text":"SFTP (SSH File Transfer Protocol) is a secure file transfer protocol. It runs over the SSH protocol. It supports the full security and authentication functionality of SSH. System Requirement Ubuntu 16.04 Step 1 - OpenSSHFirst, we need to check the SSH connection. By default OpenSSH comes with the most of the Lunux system. Please confirm this with this command. 1ssh -v localhost If everything is good, you should be able to see this. 12&gt; debug1: Connecting to localhost [127.0.0.1] port 22.&gt; debug1: Connection established. If you don’t have OpenSSH set up. You should install it on your system. 123456sudo apt updatesudo apt install openssh-serversudo systemctl stop ssh.servicesudo systemctl start ssh.servicesudo systemctl enable ssh.service Step 2 - Create SFTP GROUP and USERCreate a New User Switch to the root user: 1sudo -s Add a new user 1adduser &lt;UbuntuUsername&gt; You will be prompted to add a password. Put a simple password and change it later. Create a Group We have to create the sftp_group first. You could name it whatever you want. 1sudo groupadd sftp_group Now, we could add user into this group 1sudo usermod -aG sftp_group &lt;UbuntuUsername&gt; Step 3 - Configure SFTP / ChrootA chroot enable system to isolate application form the rest of your computer by limiting them. If you turn on chroot on user account, the account will be isolated and can only access its own directory and files. There are two different ways which you could do access control. Locking down per user We might need to provide limited access to our user because if we give full access to our user it would be a huge security flaw. If you want to lock down user to only specific directory to add and remove files, please follow steps below. Create desired path and directory. 12#For example/home/sftp_root/sftp_home /home/sftp_root is owned by root while ../sftp_home can be ownd by our user or user group Change a permission 1chmod 755 /home/sftp_root This changes our permissions to only allow writing by the user who owns the directory while read and execute to everyone else. 12345#it changes a directory to be owned by the user root and group root.chown root:root /home/sftp_root#it gives ownership to the user and usergroup only to sftp_home.chown &lt;User&gt;:&lt;Usergroup&gt; /home/sftp_root/sftp_home Locking down user 1vi /etc.ssh/sshd_config Find this and comment it out 123Subsystem sftp /var/lib/openssh/sftp-server#to#Subsystem sftp /var/lib/openssh/sftp-server And add this: 12345678Subsystem sftp internal-sftp Match User [Your New Username] ChrootDirectory /home/sftp_rootX11Forwarding noAllowTcpForwarding noAllowAgentForwarding noForceCommand internal-sftpPasswordAuthentication yes Match User: Tells the SSH server to only apply the following settings to the one user ChrootDirectory: This tells the server what directory our user is allowed to ONLY work within this directory X11Forwading, AllowTCPForwarding, AllowAgentForwarding: Prohibits the user from port forwarding, tunneling and X11 forwarding fot the user. These are all security things. ForceCommand internal-sftp: Forces the SSH server to the run the SFTP program upon access which disables shell access. PasswordAuthentication: Allows for the user to login with a typed password. You can remove this is you would rather use a security key which is by far safer. 123sudo systemctl restart ssh.service#or/etc/init.d/ssh restart Locking down User GroupOnly step 4 is different from locking down per user. Add this: 1234567Subsystem sftp internal-sftpMatch Group sftp_groupX11Forwarding noAllowTcpForwarding noChrootDirectory /home/sftp_rootForceCommand internal-sftp ReferenceThumnaillink1link2","link":"/2019/11/19/sftp/"},{"title":"Realtime Virus Scanning","text":"To protect our system and computer we should make sure that data which we download is clean. Everytime we bring data to our system or user upload data such as file attachments, we must make sure that data is free from viruses and trojans. If our system has sensitive data and critical for operation you have to be more cautious about bringing data to your system - cyber attack, nowadays, is being serious and cunning. In a normal usecase, we set up Anti Virus (AV) scanner on a file system. AV scanner monitor our file system and RAM in real-time or batch. However, it cannot make sure that each file doesn’t have any malicious content in real-time. In this project, we will use two open source products to detect virus/trojan in realtime. We are going to use Apache Nifi and ClamAV Apache Nifi is a very powerful, easy to use and stable system to process and distribute data between disparate system. Apache Nifi is a real time data ingestion platform, which can transfer and manage data transfer between different sources and destination systems. ClamAV is an open source antivirus engine for detecting trojans, viruses, malware &amp; other malicious threats. 1. UsecaseA usecase is that user need to transfer some files to the applicaion, and we have to make sure that the files don’t contain any malicious codes or contents. Since this is not bulk transformation, we want to transfer a file to endpoint in realtime after scanning. A diagram below is a high level work flow of this usecase. 2. Setting Nifi ServerThere are many different ways that you could set up Nifi server depending on the operating system. In this project, I am using Ubuntu 16.04. Updating and Upgrading apt-get1234apt-get autocleanapt-get clean allapt-get -y updateapt-get -y upgrade Installing Java (JRE)Apache Nifi is built on Java. We have to have java installed in the system 1apt install oracle-java8-installer -y Installing Nifi12345wget &quot;https://www-us.apache.org/dist/nifi/1.10.0/nifi-1.10.0-bin.tar.gz&quot;mkdir /opt/nifitar -xvzf nifi-1.10.0-bin.tar.gz --directory /opt/nifi --strip-components 1 Set JAVA_HOME123vim ~/.bash_profileexport JAVA_HOME=/usr/lib/jvm/java-8-oraclesource ~./bashrc Start/Stop Apache Nifi123/opt/nifi/bin/nifi.sh start#or/opt/nifi/bin/nifi.sh stop Get StartedYou should open a browser to access NiFI GUI. 1234#default#http://localhost:8080/nifi#or#http://IP-Address:8080/nifi If you need to change port: 12vi /opt/nifi/conf/nifi.properties# change the defalt port to what you desire If everything is good you should be able to see this screen. 2. Setting ClamAV Server at restWe are going to deply a virus scanner and make it usable in a server at REST. Even though we have multiple applications like one for email attachment, SFTP, etc., we just need to deploy a AV scanner for many applicaions. Simple Clam AV REST Proxy. This will be built on top of clamav-java. Pleas fine more detail here. We need two containers. One is ClamAV daemon as a Docker images. It builds with a current virus database and runs freshclam in the background constantly updating the virus signature database. clamd itself is listening on exposed port 3310. Another one is the server implementation. This is a precompiled and packaged docker container running the server. You also need the ClamAV virus scanner for the REST endpoint. To run use something like this. Start ClamAV server, using https://hub.docker.com/r/mkodockx/docker-clamav/ here docker run -d --name clamav-server -p 3310:3310 mkodockx/docker-clamav Test that it’s running ok: curl localhost:3310 UNKNOWN COMMAND Start the REST API image, clamd-server docker container linked to this container. docker run -d -e 'CLAMD_HOST=clamav-server' -p 8080:8080 --link clamav-server:clamav-server -t -i lokori/clamav-rest Test the REST api: curl localhost:8080 Clamd responding: true Testing the REST service You can use curl as it’s REST. Here’s an example test session: 12345curl localhost:8080Clamd responding: truecurl -F &quot;name=blabla&quot; -F &quot;file=@./eicar.txt&quot; localhost:8080/scanEverything ok : false EICAR is a test file which is recognized as a virus by scanners even though it’s not really a virus. Read more EICAR information here. 3. Design Dataflow in NifiIn our previous discussion, we’ve setup nifi server. We’ll use three processor to make it working. GetFile, ExecuteStreamCommand, RouteOnAttribute and PutFile. GetFile and PutFile can be chnaged to any endpoint of your application. For example, we could get a file from SFTP and put file to HDFS. I would like to focus on ExecuteStreamCommand becase rest of processors are straight forward. Please find more information about those processors from an offical Apache Nifi Website. ExecuteStreamCommand will executes an external command on the contents of a flow file, and creates a new flow file with the results of the command. We will use Python. Therefore, when files are come from GetFile Nifi will execute a python script to check the virus via API from ClamAV server. You should install python properly in your Nifi server. ![ExecuteStreamCommand Configuration](/2019/11/20/nifi-virus-scanning/ExecuteStreamCommand Configuration.png) This is a configuration of Command Arguments Command Path is where your python command located. Working Directory is where your python script is located. Command Arguments is your python script OutPut Destination Attribute Make sure that you define this value because we have to keep our content of file. By doing so we will write the result of scanning as an attribute. And then we will sort out files throught RouteOnAttribute processor. Let’s take a look our python script. 12345678910111213import sysimport requestsif __name__ =='__main__': url = 'http://localhost:9090/scan' payload = {'name': 'value1'} systemin = sys.stdin files = {'file': systemin} r = requests.post(url, files=files, data=payload) if 'false' in r.text: sys.stdout.write(&quot;False&quot;) sys.stdout.write(&quot;True&quot;) If a file is clean, it will have an Attribute value True. If it’s not an attribute value will look like this FalseTrue . We will route our files based one this value. Let’s check the configuration of RouteOnAttribute. It will let your nifi to send your files to next processor only if the files are clean. 4. ConclusionIt might not be only way to do this process. However, both Nifi and Clam are open source, so we don’t need to purchase any other license like Mcafee. Also with these simple tools we could process the files in realtime, and it works fairly well! Hopefully you enjoy this article. If you have a question or comment, you are very welcome to email me at any time. Referenceslink1link2link3link4","link":"/2019/11/20/nifi-virus-scanning/"},{"title":"암호화복호화","text":"여러분들의 데이터는 안전합니까? 오늘은 암호화 (Encryption) 복호화(Decryption)에 대한 이야기를 나눠보고자 합니다. 암호와/복호와w암호화는 데이터를 암호화 하여서 누군가가 읽을 수 없도록 정보를 전달화는 과정입니다. 암호와에는 여러가지 알고리즘이 쓰입니다. 복호와는 암호화된 정보를 다시 읽을 수 있게하는 과정으로써 데이터가 누출되더라도 복호화를 하지못하면 암호화된 데이터를 읽을 수 없습니다. 암호와 종류 단방향 암호: 암호화 후 복호화 할 수 없습니다. 예를 들면 사용자 비밀번호 사용자가 입력한 비밀번호를 암호화 하고 모든 접근자는 암호화 된 코드를 다시 평문으로 볼 수 없습니다. 해킹이 되어도 복호화가 굉장히 어렵습니다. 예외적인 경우로는 RainbowTable 이 있습니다. 더 자세한 정보: 참고영상 양방향 암호: 암호와와 복호화 모두 가능합니다. 사용자 주소, 이메일, 전자서명 등과 같이 정보를 재사용해야 되는 경우에 사용합니다. 양방향 암호에는 크게 두 가지 종류가 있습니다. 대칭형 암호 (비밀키 암호) 대칭형 암호는 암호화 할 때 사용하는 키와 복호화 할 때 사용하는 키가 동일한 암호화 기법입니다. 예를 들면 “APPLE”를 “ABCDE”로 암호화 했다면 복호화도 반드시 “ABCDE”로 해야됩니다. 예를 들면 AES Algorithm 하지만 대칭형 암호에는 키 배송에 관한 문제가 발생됩니다. 송신 측에서는 데이터를 암호화한 후에 수신 측에 암호키를 전달해야되고 전달하는 과정에서 이 함호 키가 털리면 데이터가 유출됩니다. 그리고 키 관리가 어렵습니다. 비대칭형 암호 (공개키 암호) 비대칭현 암호는 암호와 키와 복호화 키가 다릅니다. 클라이언트와 서버가 각각의 공개키와 비밀키를 갖고, 서로 공개키를 공개합니다. 클라이언트는 서버의 공개키로 데이터를 암호화한 후에 서버로 보내면 서버는 자신의 비밀키를 가지고 클라이언트가 보낸 데이터를 복호화 합니다. 예를 들면 RSA, Diffe-Hellman, ECC, etc 공개키 는 공유되지만 암호키 는 공개되지 않기에 공개키가 중간에 탈취되어도 데이터를 안전하게 지킬 수 있습니다. 하지만 문제는 비대칭형 암호는 대칭형 암호에 비해 느리고 많은 자료를 암호와 복호화 하는데 불편합니다 단점이 있습니다. 암호 알고리즘단방향 SHA : 가장 대표적인 해시함수 PBKDF2 : 해시함수의 컨테이너인 PBKDF2는 솔트를 적용한 후 해시 함수의 반복 횟수를 임의로 선택할 수 있다. PBKDF2는 구현하기 쉬운 알고리즘이며 SHA와 같이 검증된 해시 함수만 사용합니다. bcrypt : 패스워드 저장을 목적으로 설계되었으며 가장 많이 쓰이는 알고리즘입니다. 입력값을 72 byte로 해야하기 때문에 조금 사용에 불편함이 있을 수 있습니다. scrypt : scrypt는 상대적으로 최신 알고리즘이며 위에 알고리즘들 보다 더 성능적으로 뛰어난다고 평가되지만 잘 알려져 있지 않습니다. scrypt는 다이제스트를 생성할 때 메모리 오버헤드를 갖도록 설계되어, 억지 기법 공격 (brute-force attack)을 시도할 때 병렬화 처리가 매우 어렵습니다. 따라서 PBKDF2보다 안전하고 bcrypt에 비해 더 경쟁력 있다고 여겨집니다. 양방향 AES: 현재 가장 보편적으로 쓰이는 암호와 방식이며 미국 표준 방식인 AES. 128 ~ 256 byte 키를 적용 할 수 있어서 보안성이 뛰어난 공개된 알고리즘입니다. RSA : 공개키 암호 시스템의 하나로 암호와 뿐만 아니라 전자서명까지 가증한 알고리즘입니다. Referencehttps://sieunlim.tistory.com/16 https://record22.tistory.com/44","link":"/2019/12/07/encryption-decryption/"},{"title":"머신러닝 - 기본용어","text":"supervised unsupervised feature label … 머신러닝을 시작하게 되면 새로 배워야 하는 용어들이 많죠? 하지만 이러한 용어들을 자신의 개념으로 잘 정리하는 것이 참 중요합니다. 왜냐하면, 우리가 앞으로 배우게 될 머신러닝의 기초가 되기 때문이죠… 안녕하세요 AI Nomad 최민규입니다. 머신러닝 속성코스 두 번째 세션에서는 머신러닝에서 사용되는 기본적인 용어들을 정리해보려 합니다. 1. Supervised and Unsupervised Learning머신러닝을 처음 시작하게 되면 가장 먼저 알게 되는 용어는 Supervised Learning 과 Unsupervised Learning 입니다. 영어로 supervised라고 하면 감독이 돼다, 관리되다 정도로 해석됩니다. 따라서 supervised learning 을 직역하면 감독의 지시 아래 배워지다 정도로 해석됩니다. 우리가 무언가를 배울 때 좋은 감독이 있으면 배움이 편합니다. 왜냐하면, 그들이 우리가 무엇을 배워야 할지 감독해주고 지시해주기 때문입니다. 머신러닝에서 supervised learning 도 비슷한 의미가 있다고 할 수 있습니다. 컴퓨터가 사람의 감독 아래 지시되고 학습되게 되는 거죠. 감독들은 선수들을 지도하고 지시합니다. A와 B를하라고 왜냐하면 감독들은 A와 B를 했을때 C라는 결과가 나올 거를 경험을 통해 알기 때문이죠. 머신러닝에서 A 와 B 를 feature 이라고 부릅니다. 그리고 C 를 label 이라고 부르죠. supervised learning 을 학습시키기 위해서는 feature 와 label 이 필요합니다. 감독이 있기 때문이죠. 그렇다면 감독이 존재하지 않는 unsupervised learning 은 어떨까요? 감독은 없고 선수들만 있는 팀이 있다고 가정합시다. 선수들은 매주 경기에 나가야 하죠. 구단주는 선수들에게 리그 3위 안에 들지 않으면 팀을 해체하겠다고 합니다. 감독이 없다는 것은 label 이 없다는 것이죠. 선수들은 직관적으로 어떠한 훈련을 해야 할지는 알 수 있을 것입니다. 하지만 감독의 경험이 없는 것이죠. 그러므로 unsupervised learning 에는 feature 만이 존재합니다. 이 상황에서 선수들은 어떻게 감독 없이 팀을 빌딩 할 수 있을까요? 스스로 답을 찾아야죠. 훈련과 경기를 통한 trial and error 즉 시행착오를 거쳐서 선수들 스스로 배워가는 수밖에 없겠죠. 이것이 머신러닝에서 unsupervised learning 이라고 불리는 것입니다. 데이터에는 feature 들만 존재하고 label 이 없습니다. 따라서 여러 알고리즘들이 feature 을 가지고 시행착오를 거쳐 가장 이상적인 label 을 찾아가는 것을 우리는 unsupervised learning 이라고 합니다 2. Labels (결과값)앞에서 나온 label은 어떠한 x들에 대한 y 즉 결과값이라고 정리 할 수 있겠습니다. 예를 들면, 미래의 주식가격, 사진 속에 등장한 동물의 종류, 어떤 소리에 의미 등이 있겠군요. 3. Feature (원인)Feature은 간단하게 어떤 결과에 대한 원인입니다. y라는 결과에 대한 x라는 원인이죠. 대학교 성적을 결과라고 한다면 이 결과에 대한 feature 즉 원인으로는 하루 공부 시간, 연애 여부, 아르바이트 여부, 등등이 있겠네요. 4. Model (모델)모델은 feature와 feature 사이 또 feature와 label의 관계라고 보시면 됩니다. 모델은 A와 B라는 feature들의 관계를 설명하고 A,B 라는 feature들과 C라는 label의 관계를 설명합니다. 이들의 관계를 잘 설명하는 모델을 우리는 좋은 모델이라고 부르며 좋은 결과를 내게 됩니다. 5. Train (학습)우리가 모델을 이야기할 때 train 시킨다 학습 시킨다는 용어를 많이 사용합니다. 모델을 학습시킨다 즉 train 한다는 말은 쉽게 말하면 모델을 만든다 모델을 학습시킨다고 이해하시면 되겠습니다. 그렇다면 모델을 만들고 학습시킨다는데 어떻게 무엇으로 학습시킨다는 거죠? 바로 데이터입니다. 데이터에 존재하는 많으면 많은 적으면 적은 feature들과 lable을 통해서 모델을 학습시키는 것입니다. 우리가 강아지에게 손을 달라고 했을 때 손을 주는 훈련을 한다고 가정을 해봅시다. 강아지가 이 훈련을 습득하는 방법은 보통 다음과 같겠죠 손 -&gt; 간식 (o)발 -&gt; 간식 (x)누움 -&gt; 간식 (x)손 -&gt; 간식 (o)손 -&gt; 간식 (o)손 -&gt; 간식 (o) 강아지는 손을 줬을 때 간식을 먹었다는 데이터를 기반으로 학습했고 사람이 손이라고 이야기했을 때 손을 주는 행동을 하게 되죠. 머신러닝도 마찮가지로 데이터를 기반으로 어떤 모델을 학습하고, 우리가 모델에 input을 주었을 때 모델은 학습된 데이터를 바탕으로 어떤 output 즉 inference 를 만들어 내게 됩니다. 6. Inference (암시)Inference는 unlabeled example을 학습된 모델에 적용할 때 사용합니다. 즉 모델을 학습시키고 새로운 input을 모델에 넣었을 때 모델이 만드는 output을 inference라고 부르죠. 결국 이 output도 예측값이기 때문에 결과를 암시한다 라는 의미로 inference라고 부릅니다. 7. 요약데이터를 빼놓고는 머신러닝을 이야기 할 수 없습니다. 데이터가 quantitative or qualitative 한지를 떠나서 머신러닝에서 데이터를 보는 관점은 딱 두 가지 입니다. feature 와 label. 이러한 feature와 label의 관계를 설명하는 것이 모델이고, 모델을 만들기 위해서는 데이터를 가지고 모델을 train (학습) 시키는 과정이 필요합니다. 모델을 통해 생성된 결과값 즉 prediction이 바로 inference 가 되게 되는 것이죠. 이렇게 이번 세션에서는 기본적인 머신러닝 용어들에 대해 배워봤습니다. 다음 세션은 본격적으로 모델에 사용되는 알고리즘과 알고리즘이 학습되는 로직에 대한 세션을 준비하겠습니다. &gt;&gt; Previous: Overview ReferenceThumnailGoogle Machine Learning Crash Course Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.","link":"/2019/12/17/mlcrash02-keyterm/"},{"title":"Feature Engineering","text":"If you ask yourself what’s the most important thing in machine learning, what’s your answer? All data scientist would have different answers. Among the other many answers, I believe feature engineering is one of the most important in machine learning. Sometimes, it’s a more critical step than a model selection and training a model because a model cannot improve a model itself even though we put a lot of effort on a hyper parameter turning. However, well selected/extracted features could be applied to many different models and improve a performance. A feature engineering includes feature selection and feature extraction. A feature selection is trial-error process to select relevant features from existing features. Since all features are simply selected from original features it’s easy to interpret what those features means. However, it is difficult to consider a relationship in selected features. On the other hands, a feature extraction is more like functional process to extract relevant features from existing features. It requires a form of function which enable an algorithm to create/extract a new set of features. A relationship between features will be considered and number of features could be significantly reduced. Yet, an interpretation of extracted features is not easy. We should use different methods of feature engineering depending on a machine learning algorithm we want to use. In supervised learning, we could select features form Information gain, Stepwise regression, LASSO, Genetic algorithms, etc. If we want to extract features, Partial Least Squares (PLS) is an option. In unsupervised learning, we could do a feature selection with PCA loading; a feature extraction uses Principal component analysis (PCA), Wavelets transforms, Autoencoder, etc. ReferenceThumnail","link":"/2020/01/24/feature-engineering-101/"},{"title":"Naive Bayes from scratch","text":"Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features/predcitors) in a learning problem. Maxumum-likelihood training can be done by evaluting a closed-form exporession, which takes linear time, rather tahn by expensive iterative approximation as used for many other typs of classifier. Wikipedia Usecase - Spam filterWe will use the Naive Bayes algorithm to fit a spam filter. Spam filters are used in alal email services to classify received emails as “Spam” or “Not Spam”. A simple apporach involves maintaining a vocabulary of words that commonly occur in “Spam” emails and classifying an email as “Spam” if the number of words from the dictionary that are present in the email is over a certain threshold. Assume we are given the vocabulary consists of 15 words $V$ = {secret, offer, low, price, valued, customer, today, dollar, million, sports, is, for, play, healthy, pizza} We will use $V_i$ to represent the ith word in $V$. As our training dataset, we are also given 3 example spam messages: • million dollar offer • secret offer today • secret is secret and 4 example non-spam messages• low price for valued customer • play secret sports today• sports is healthy• low price pizza 1234567import numpy as npV = ['secret', 'offer', 'low', 'price', 'valued', 'customer', 'today', 'dollar', 'million', 'sports', 'is', 'for', 'play', 'healthy', 'pizza']message = ['million dollar offer','secret offer today','secret is secret', 'low price for valued customer', 'play secret sports today', 'sports is healthy', 'low price pizza'] train is our input vector x corresponding to each training message, and it has length n = 15 (length of V). Since we have 7 training example of message, We will have 7 by 15 training data - 7 data 15 features. 12train = np.zeros((7, 15))label = [0,0,0,1,1,1,1] 12345678'''Converting 7 training message data set to x vector which has length n = 15'''for i in range(len(train)): for j in range(len(V)): for k in range(len(message[i].split(&quot; &quot;))): if V[j] == message[i].split(&quot; &quot;)[k]: train[i,j] += 1 123456'''This is the feture x matrix.row = n messagecol = i_th feature vector '''train array([[0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.], [1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], [2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], [0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.], [1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0.], [0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])Let’s list them separately 1234567for i in range(len(label)): if label[i] == 0: print(&quot;this is {}th message - It's SCAM&quot;.format(i)) print(train[i]) else: print(&quot;this is {}th message - It's NOT SCAM&quot;.format(i)) print(train[i]) this is 0th message - It&apos;s SCAM [0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.] this is 1th message - It&apos;s SCAM [1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] this is 2th message - It&apos;s SCAM [2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] this is 3th message - It&apos;s NOT SCAM [0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.] this is 4th message - It&apos;s NOT SCAM [1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0.] this is 5th message - It&apos;s NOT SCAM [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0.] this is 6th message - It&apos;s NOT SCAM [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]1234567891011121314151617'''spam and ham array will be count of x_i when each message is spam or not spam.spam = (Y=0|x_i)ham = (Y=1|x_i)'''spam = np.zeros((1, 15))ham = np.zeros((1, 15))for c in range(len(label)): for i in range(train.shape[1]): if label[c] == 0: # Spam spam[0,i] += train[c,i] else: # Not Spam ham[0,i] += train[c,i] 1spam array([[3., 2., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0.]])If you see a result above, spam[0] = 3 ,spam[1] = 2 , spam [2] = 0, it means total count of x0 = secret is three among training data classified as spam. We have two offer and zeor low in our training data. 1234567891011121314151617'''In this section, we will calculate probabiliy of each word xi when a message is spam or not.prob_spam = P(x_i|y=0)prob_ham = P(x_i|y=1)'''spam_counter = 0ham_counter = 0for c in range(len(label)): if label[c] == 0: spam_counter += 1 else: ham_counter += 1 prob_spam_x_i = spam/spam_counterprob_ham_x_i = ham/ham_counter 1prob_spam_x_i array([[1. , 0.66666667, 0. , 0. , 0. , 0. , 0.33333333, 0.33333333, 0.33333333, 0. , 0.33333333, 0. , 0. , 0. , 0. ]])1prob_ham_x_i array([[0.25, 0. , 0.5 , 0.5 , 0.25, 0.25, 0.25, 0. , 0. , 0.5 , 0.25, 0.25, 0.25, 0.25, 0.25]])123456789101112131415161718192021'''Bayes TherormCalculating P(y=0|x_i)prob_spam_per_word = P(y=0|x_i)'''prob_spam_per_word = np.zeros((1, 15))prob_ham_per_word = np.zeros((1, 15))prob_spam = 3/7prob_ham = 4/7for i in range(prob_spam_per_word.shape[1]): spam_a = (prob_spam_x_i[0,i]*prob_spam) spam_b = spam_a + (prob_ham_x_i[0,i]*prob_ham) spam_c = spam_a/spam_b ham_a = (prob_ham_x_i[0,i]*prob_ham) ham_b = ham_a + (prob_spam_x_i[0,i]*prob_spam) ham_c = ham_a/ham_b prob_spam_per_word[0,i] = spam_c prob_ham_per_word[0,i] = ham_c 1prob_spam_per_word array([[0.75, 1. , 0. , 0. , 0. , 0. , 0.5 , 1. , 1. , 0. , 0.5 , 0. , 0. , 0. , 0. ]])The result above means that the probability of spam per i_th words. For example, the probability of spam when we have secret is 75% and a probability of spam of offer is 100%. 1prob_ham_per_word # This is the probability of ham per words. array([[0.25, 0. , 1. , 1. , 1. , 1. , 0.5 , 0. , 0. , 1. , 0.5 , 1. , 1. , 1. , 1. ]])Since we don’t have enough data, I will just use training dataset to evaluate the bayes classifier 1234567891011121314151617181920212223threshold = 0.50 # In what percentage do you want to classfy an eamil as spam.for t in range(train.shape[0]): conditional_prob_spam = prob_spam conditional_prob_ham = prob_ham for i in range(prob_spam_per_word.shape[1]): if train[t][i] == 1: conditional_prob_spam = conditional_prob_spam * prob_spam_per_word[0,i] conditional_prob_ham = conditional_prob_ham * prob_ham_per_word[0,i] if conditional_prob_spam != 0: prob = conditional_prob_spam / (conditional_prob_spam + conditional_prob_ham) * 100 else: prob = 0.0 if prob &gt; threshold*100: label = &quot;SPAM&quot; else: label = &quot;NOT SPAM&quot; print(&quot;{} th email is {} with a probability of being spam {}%&quot;.format(t,label,prob)) #print(prob) 0 th email is SPAM with a probability of being spam 100.0% 1 th email is SPAM with a probability of being spam 100.0% 2 th email is NOT SPAM with a probability of being spam 42.857142857142854% 3 th email is NOT SPAM with a probability of being spam 0.0% 4 th email is NOT SPAM with a probability of being spam 0.0% 5 th email is NOT SPAM with a probability of being spam 0.0% 6 th email is NOT SPAM with a probability of being spam 0.0%Given a new message “today is secret”, decide whether it is spam or not spam, based on the Naive Bayes classifier, learned from the above data. 123456'''&quot;today is secret&quot;Vectorizing the email.. '''target = np.array([1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.])target[0] 12345678910111213141516171819202122232425'''Please play around with different threshold. '''threshold = 0.50 # In what percentage do you want to classfy an eamil as spam.conditional_prob_spam = prob_spamconditional_prob_ham = prob_hamfor i in range(prob_spam_per_word.shape[1]): if target[i] == 1: conditional_prob_spam = conditional_prob_spam * prob_spam_per_word[0,i] conditional_prob_ham = conditional_prob_ham * prob_ham_per_word[0,i]if conditional_prob_spam != 0: prob = conditional_prob_spam / (conditional_prob_spam + conditional_prob_ham) * 100else: prob = 0.0if prob &gt; threshold*100: label = &quot;SPAM&quot; else: label = &quot;NOT SPAM&quot;print(&quot;{} th email is {} with a probability of being spam {}%&quot;.format(t,label,prob))#print(prob) 6 th email is SPAM with a probability of being spam 69.23076923076923%","link":"/2020/03/23/Naive%20Bayes%20from%20scratch/"},{"title":"Boosting (AdbBoost)","text":"In machine learning, boosting is an ensemble meta-algorithm for primarily reducing bias, and also variance in supervised learning, and a family of machine learning algorithms that convert weak learners to strong ones. Wikipedia BoostingToday, I would like to introduce Boosting method in Machine Learning. Basically, Boosting is a set of algorithms (classifiers) which changes weak learner to strong learners. It is one of the ensemble methods for improving the model predictions of any given learning algorithm. However, unlike the regular ensemble method which we group several model and use them in parallel, Boosting is using a single model in sequential order with differnt weights. Boosting is also using ramdom sampling with a replacement. It will start training model from Sample 1 to Sample N. In each training, there would be well-classified data and wrong-classified data. Since it allows a replacement, some wrong classified data might be included in different samples or not. Since each previous model affect a current model by assigning weights to data which it coudln’t classify correctly, this is not parallel - it’s sequential. At the end, it will use all trained models with different weights and generate the output. One of the disadvantages of Boosting is that it easily get corrupted by outliers because it will assign heavy weights to those outliers and it will mislead the models. AdaBoost (Adaptive Boosting) AdaBoost works in a way putting more weights on difficult to classify data and less on those already handled well. In AdaBoost, we use something called Decision Stumps, the simplest model we could construct on data. It split the data into two subsets based on the feature. To find the best decision stump, we should lay out all features of data along with every possible threshold and look for one gives us best accuracy. In this example, I will consturct AdaBoost from the scratch with some mathematical explanation. ExampleX1 = (−1,0,+), X2 = (−0.5,0.5,+)X3 = (0,1,−), X4 = (0.5,1,−)X5 = (1,0,+), X6 = (1,−1,+)X7 = (0,−1,−),X8 = (0,0,−) How to? Constructing $D_t$. It’s basically weight of each $i^{th}$ data. $D_{t+1}(i)$ = $\\frac{D_t(i)}{Z_t} * e^{-\\alpha_t}$ if $y_i = h_t(x_i)$$D_{t+1}(i)$ = $\\frac{D_t(i)}{Z_t} * e^{\\alpha_t}$ if $y_i \\ne h_t(x_i)$, where $Z_t$ = Normalization Constant = Sum of $D_t$ and $\\alpha_t$ = $\\frac{1}{2}ln(\\frac{1-\\epsilon_t}{\\epsilon_t})$. $h_t: \\epsilon_t = \\sum\\limits_{i=1}^{m}D_t\\vert(y_i\\ne h_t(x_i))$ if $(y_i\\ne h_t(x_i)$ then 1 otherwise 0​ Step 1 Put an initial random decision stump aka classifier $h_1$ h1 is a randomly assinged decision stump.Let’s get $D_1$ = $\\frac{1}{m}$, where $m = 8$ t err alpha Z d1 d2 d3 d4 d5 d6 d7 d8 1 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125 Now, in a second row, I will put 1 if $h_1$ classify $y_i$ incorrectly, else 0. t err alpha Z d1 d2 d3 d4 d5 d6 d7 d8 1 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0 1 0 0 0 0 1 1 $h_1$ has classified correctly $d_t(1),d_t(3),d_t(4),d_t(5),d_t(6)$ Once we have $D_1(i)$ and a result of classification by $h_1$ we could calculate $\\epsilon_1$, where $h_t: \\epsilon_t = \\sum\\limits_{i=1}^{m}D_t\\vert(y_i\\ne h_t(x_i))$ if $(y_i\\ne h_t(x_i)$ then 1 otherwise 0 In a thrid row, I will get all $D_1\\vert(y_i\\ne h_1(x_i)$, and if take sumation of all values, it’s our $\\epsilon_1$. t err alpha Z d1 d2 d3 d4 d5 d6 d7 d8 1 0.375 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0 1 0 0 0 0 1 1 0 0.13 0 0 0 0 0.13 0.13 Let’s get $\\alpha_1$ and $Z_1$. Remember $Z_t$ = Normalization Constant = Sum of $D_t$, and $\\alpha_t$ = $\\frac{1}{2}ln(\\frac{1-\\epsilon_t}{\\epsilon_t})$. t err alpha Z d1 d2 d3 d4 d5 d6 d7 d8 1 0.375 0.255 1.000 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0 1 0 0 0 0 1 1 0 0.13 0 0 0 0 0.13 0.13 If you see the table above, you could notice that d2, d7, d8 has more weight, 0.13 becase $h_1$ failed to classify them correctly. Based on this information, we could move onto second iteration. In order to calculate $D_2(i)$ we need additional information other than table above. $D_{t+1}(i)$ = $\\frac{D_t(i)}{Z_t} * e^{-\\alpha_t}$ if $y_i = h_t(x_i)$$D_{t+1}(i)$ = $\\frac{D_t(i)}{Z_t} * e^{\\alpha_t}$ if $y_i \\ne h_t(x_i)$ We need to what what’s $e^{-\\alpha_1}$ and $e^{\\alpha_1}$ $e^{-\\alpha_1}=0.775$ $e^{\\alpha_1}=1.291$ t err alpha Z d1 d2 d3 d4 d5 d6 d7 d8 1 0.375 0.255 1.000 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0 1 0 0 0 0 1 1 0 0.13 0 0 0 0 0.13 0.13 2 0.323 0.371 0.968 0.097 0.161 0.097 0.097 0.097 0.097 0.161 0.161 We’ve got new weights for data. d1 was 0.125; it has become 0.097. d2 has become 0.161 from 0.125 because $h_1$ failed to classfy this data. Let’s get the $h_2$ - I was focusing on having all +in the same group. If you do same calculation we’ve done above, You will get this table: t err alpha Z d1 d2 d3 d4 d5 d6 d7 d8 1 0.375 0.255 1.000 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0 1 0 0 0 0 1 1 0 0.13 0 0 0 0 0.13 0.13 2 0.323 0.371 0.968 0.097 0.161 0.097 0.097 0.097 0.097 0.161 0.161 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0.16 0.16 $e^{-\\alpha_2}=0.490$ $e^{\\alpha_2}=2.041$ Lastly, let’s do the final iteration. t err alpha Z d1 d2 d3 d4 d5 d6 d7 d8 1 0.375 0.255 1.000 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0 1 0 0 0 0 1 1 0 0.13 0 0 0 0 0.13 0.13 2 0.194 0.713 0.968 0.097 0.161 0.097 0.097 0.097 0.097 0.161 0.161 0 0 1 1 0 0 0 0 0 0 0.1 0.1 0 0 0 0 3 0.049 0.082 0.204 0.204 0.049 0.049 0.082 0.082 A reasoning of $h_3$ is to have all o in a same group. Also, d5 and d6 have never been misclassified, so I want to have a information of d5 and d6 in my model. And the result will be: t err alpha Z d1 d2 d3 d4 d5 d6 d7 d8 1 0.375 0.255 1.000 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0 1 0 0 0 0 1 1 0 0.13 0 0 0 0 0.13 0.13 2 0.323 0.371 0.968 0.097 0.161 0.097 0.097 0.097 0.097 0.161 0.161 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0.16 0.16 3 0.138 0.916 0.943 0.069 0.115 0.069 0.069 0.069 0.069 0.241 0.241 0 0 0 0 1 1 0 0 0 0 0 0 0.07 0.07 0 0 Graphically, it will look like: Lastly, the final classifier is: $H_{final}(x)=sign(0.255h_1+0.371h_2+0.943*h_3)$","link":"/2020/04/02/boosting/"},{"title":"Soft Actor Critic with Inhibitory Networks for Retraining UAV Controllers Faster","text":"Minkyu Choi, Max Filter, Kevin, Alcedo, Thayne T. Walker, David Rosenbluth, and Jaime S. Ide.International Conference on Unmanned Aircraft Systems (ICUAS), 2022 The rapid evolution in autonomous unmanned aerial vehicles (UAVs) technology has spurred significant advancements in their control systems. A prominent challenge in this domain is balancing the agility of Proportional-Integral-Derivative (PID) systems for low-level control with the adaptability of Deep Reinforcement Learning (DRL) for navigation through complex environments. This post delves into a novel approach that combines these technologies to improve the retraining efficiency of UAV controllers using Soft Actor-Critic (SAC) with inhibitory networks. Introduction to UAV Control SystemsAutonomous UAVs benefit immensely from DRL due to its ability to handle nonlinear airflow effects caused by multiple rotors and to operate in uncertain environments such as those with wind and obstacles. Traditional DRL methods have shown success in training efficient UAV navigation systems. However, real-world applications often necessitate retraining these systems to adapt to new, more challenging tasks. The Challenge of Retraining in DRLOne significant issue with traditional DRL algorithms like SAC is catastrophic forgetting, where previously learned skills are lost when the system is retrained for new tasks. This problem is especially pronounced in dynamic environments where UAVs must quickly adapt to new challenges without losing their established navigation capabilities. Introducing Soft Actor-Critic with Inhibitory Networks (SAC-I)Inspired by mechanisms in cognitive neuroscience, the proposed SAC-I approach addresses this retraining challenge. Inhibitory control in neuroscience refers to the brain’s ability to modify ongoing actions in response to changing task demands. Similarly, SAC-I utilizes separate and adaptive state value evaluations along with distinct automatic entropy tuning. Key Components of SAC-I Multiple Value Functions: SAC-I employs multiple value functions that operate in a state-dependent manner. This allows the system to retain knowledge of familiar situations while adapting to new ones without forgetting previously learned skills. Inhibitory Networks: An additional value network, termed the inhibitory network, is introduced. This network specifically learns new evaluations required for novel tasks, thus preventing interference with the previously learned value network. Dual Entropy Estimation: The approach also includes estimating two distinct entropy parameters to manage the exploration-exploitation trade-off more effectively during retraining. Experimental ValidationThe efficacy of SAC-I was validated through experiments using a simulated quadcopter in a high-fidelity environment. The results demonstrated that SAC-I significantly accelerates the retraining process compared to standard SAC methods. Key metrics such as sample efficiency and cumulative success rates were used to benchmark the performance. Figure 1: Takeoff-Target-Aviary-v0 task with obstacle. Go episode: agent starts at (0,0,0.11) and it has to reach and hover around target point at (0, 0, 0.7). Stop episode: obstacle appears randomly after the episode starts, and agent has to avoid it and reach the target. Results Summary Faster Retraining: SAC-I agents showed a remarkable reduction in the time required for retraining, achieving new task proficiency up to five times faster than standard SAC agents. Improved Sample Efficiency: The novel approach maintained high levels of sample efficiency, crucial for practical applications where real-world data collection is expensive and time-consuming. Figure 2: Average reward during agents training. Figure 3: Cumulative success during agents training. ConclusionSAC-I presents a groundbreaking advancement in the retraining of UAV controllers. By leveraging inhibitory networks inspired by cognitive control mechanisms, this approach ensures rapid adaptation to new tasks while preserving established skills. This innovation holds significant promise for real-world UAV applications, where quick adaptation to changing environments is critical. Future DirectionsFurther research will focus on extending this approach to other DRL algorithms and applying it to more complex and dynamic UAV tasks. Additionally, improving the auto-tuning methods for entropy parameters will enhance the stability and performance of SAC-I in various applications. Citation12345678910@INPROCEEDINGS{9836052, author={Choi, Minkyu and Filter, Max and Alcedo, Kevin and Walker, Thayne T. and Rosenbluth, David and Ide, Jaime S.}, booktitle={2022 International Conference on Unmanned Aircraft Systems (ICUAS)}, title={Soft Actor-Critic with Inhibitory Networks for Retraining UAV Controllers Faster}, year={2022}, volume={}, number={}, pages={1561-1570}, keywords={Training;Neuroscience;Parameter estimation;Transfer learning;Reinforcement learning;Autonomous aerial vehicles;Entropy;deep reinforcement learning;soft actor-critic;transfer learning;UAV navigation;quadcopter simulation}, doi={10.1109/ICUAS54217.2022.9836052}}","link":"/2022/06/01/soft-actor-critic-with-inhibitory-networks-for-retraining-uav-controllers-faster/"},{"title":"Distill Reinforcement Learning","text":"Distilling Reinforcement Learning What is Reinforcement LearningConsider an agent situated in an unfamiliar setting, capable of garnering rewards through engagment with its surroundings. The agent’s objective is to perform actions that optimize the accumulation of rewards. Practical examples of this concept incldue a gaming bot striving for top scors or a robot executing physical tasks with tanglible objects, although the possibilities extend beyond these instances. The primary objective of Reinforcement Learning (RL) is to develop an effective strategy for an agent through iterative trials and straightforward feedback. By emplying the optimial strategy, the agent can dynamically adpat to the environment, thereby maximizing future rewards. Let’s delve into some fundamental concepts in RL. An agent operates within an environment, and the way the environment, and the way the environment reacts to specific actions is dictacted by a model, which may be known or unknown to us. The agent can occupy on of many state ($s \\in S$) within the environment and choose to take one of several actions ($a \\in A$) to transition between states. The transition probabilities between state (P) determine the agents’ destination state. Upon taking an action, the environment offers a reward ($r \\in R$) as feedback. The model outlines the reward function and transition probabilities. Depending on whether we know the model’s working or not, there are two distnct situations. Known Model: Execute model-based RL with perfect inffomration for planning. When the environment is fully understood, the optimal solution can be found using Dynamic Programming (DP). This is reminiscent of solving problems such as the “longest increasing subsequence” or “traveling salesman problem.” Unknown Model: Perform model-free RL or attempt to explicitly learn the model as part of the algorithm, given that information is incomplete. The majority of the subsequent content addresses scenarios where the model is unknown. The agent’s policy, $\\pi(s)$, offeres guidance on the optimal action to take in a specific state to maximize total rewards. Each state is linked to a value function, $V(s)$, which forecasts the anticipated amount of future rewards obtainable in that state by following the corresponding policy. In essence, the value funcution measures a state’s quality. Both policy and value functions are the targets of reinforcement learning. The interaction between the agent and the environment involves a series of actions and observed reward over time, $t = 1,2, …, T$. Throughout this process, the agent accumulates knowledge about the environment, learns the optimial policy, and decides on the next action to take to efficiently learn the best policy. Let’s denote the state, action, and reward at time step $t$ as $S_t, A_t$ and $R_t$ respectively. Therefore, the interaction sequence is fully represented by a single episode (also referred to as “trial” or “trajectory”) that concludes at the terminal state $S_T$:$$S_1, A_1,R_1,S_2,A_2,R_2,…,S_T$$ ModelThe model serves as a representation of the environment, enabling us to understand or deduce how the environment interacts with and offers feedback to the agent. The model consists of two primary components: the transition probability function $P$ and the reward function $R$. Consider a situation where we are in state $s$ and decide to take action $a$, leading to the next state s' and receiving reward r. This single transtion step is denoted by the tuple $(s, a, s’, r)$. The transition function $P$ documents the likelihood of transitioning from state s to s' upon taking action a and acquiring reward $r$. $\\mathbb{P}$ symbolizes “probability” in this context.$$P(s’,r|s,a) = \\mathbb{P}[S_{t+1}=s’, R_{t+1}=r|S_t=s, A_t=a]$$Thus, the state=transition function can be defined as a function of $P(s’, r|s,a):$$$P_{ss’}^a =P(s’|s,a)=\\mathbb{P}[S_{t+1}=s’,|S_t=s, A_t=a]=\\sum_{r \\in R}P(s’,r|s,a)$$The reward function R is obtained by the next reward triggered by an action:$$R(s,a) = \\mathbb{E}[R_{t+1}|S_t=s, A_t=a]=\\sum_{r \\in R}r\\sum_{s’\\in S}P(s’,r|s,a)$$Model-Based vs Model-free: Model-based methods rely on constructing and utilizing a model of the environment to plan and make decions, while model-free methods learn the optimal policy directly from the agent’s experiences without building an explicit model. Each approach has its own advantages and disadvantages, depending on the problem and environment at hand Model-based: It relies on building an explicit model of the environment, which includes transition probabilities (how the environment changes with actions) and reward functions. It uses the model to plan and make decisions, siulating potential future scenarios to find the optimal policy. It can be more sample-efficient, as it leverages the environment’s model to learn faster and require fewer interactions with the environment. However, building an accurate model can be challenging, particularly for complex environments with large state and action spaces. Example: Dynamic Programming techniques like Value Iteration and Policy Iteration Model-free: It does not rely on explicit model of the environment; instead, it learns the optimal policy directly from the agent’s experiences (state transitions and rewards). It learns through trial-and-error, using techniques like Temporal Difference learning or Monte Carlo methods. This method can be more straightforward to implement, as they do not require an explicit model of the environment, making them suitable for complex and high-dimensional environments. However, they might require more interations with the environment to learn the optimal policy, making them less sample-efficient. Example: Q-Learning, SARSA, and Actor-Critic algorithms. PolicyA policy, denoted as the agent’s behavior function $\\pi$, provides guidance on which action to take in a given state $s$. It represents a mapping from state $s$ to action $a$ and can be either deterministic or stochastic: Deterministic: $\\pi(s)=a$ Stochastic: $\\pi(a|s)= \\mathbb{P}_{\\pi}[A=a|S=s]$ On-policy vs Off-policy: On-policy methods learn directly from the agent’s interactions with the environment while adhearing to the current policy. In contrast, off-policy methods learn from experiences generated by a different policy, allowing for greater flexibility and more efficient learning from a diverse set of experiences. On-policy: On-policy methods learn the value function and policy by using the same policy for both exploration and exploitation. Thy follow the current policy while making decisions and simultaneously update the policy based on the experiences gathered. These methods usually strike a balance between exploration and exploitation during learning. Off-policy: It separate the policy used for learning (target policy) from the policy used for exploration (behavior policy). The behavior policy is responsible for generating experiences, while the target policy is updated based on the experiences collected by the behavior policy. This separation allows off-policy methods to learn from a wider range of experiences, including historical data or experiences from other agents. Value FunctionThe value function represents the expected cumulative reward that an agent can obtain, starting from a specific state and following a given policy. The value function helps the agent to estimate the long-term value of each state, considering future rewards it may receive by taking actions according to the policy. It is a key concept in RL, as it helps guid the agent’s decision-making process to maximize the total reward over time. Let’s compute the return $G_t$ starting from time $t$:$$G_t = R_{t+1} + \\gamma R_{t+2} + … \\sum_{k=0}^{\\infty}\\gamma R_{t+k+1}$$The discount factor $\\gamma$ which ranges from 0 to 1, discounts future rewars for several reasons: Future reward may be more uncertain Immediate rewards are oftern more appealing than delayed gratification, as humans tend to prefer to enjoying themselves now rather than waiting for years Discounting offeres mathematical simplicity, as it eliminates the need to account for infinite future steps when calculating returns It helps mitigate concerns about infinite loops in the state transition graph, ensuring the agent converges on an optimal policy State-value function ($V(s)$): This function estimates the expected cumulative reward when starting from state $s$ and following a particular policy. It considers the value of the current state without taking into account the immediate action the agent will take.$$V_{\\pi}(s) = \\mathbb E_{\\pi}[G_t|S_t=s]$$Action-value function($Q(s,a)$): This function estimates the expectued cumulative reward when starting from state $s$, taking action $a$, and then following a particular policy. It considers the value of the current state-action pair, accouting for both the state and the action the agent will take.$$Q_{\\pi}(s,a) = \\mathbb E_{\\pi}[G_t|S_t=s, A_t=a]$$Furthermore, when following the target policy $\\pi$ we can leverage the probability destribution over possible actions and the Q-values to derive the state-value:$$V_{\\pi}(s)=\\sum_{a \\in A}Q_{\\pi}(s,a) \\pi (a|s)$$The action advantage function, also known as the “A-value,” represents the difference between the action-value (Q-value) and state-value (V-value). The function quantifies the relative benefit of taking a specific acion in a given state, compared to the average value of that state following the target policy. By calculating the A-value, we can identify which actions are advantageous or disadvantageous in a particular state, guiding the agent’s decision-making process for improved performance.$$A_{\\pi}(s,a) = Q_{\\pi}(s,a)- V_{\\pi}(s)$$ Markov Decision ProcessesIn more formal terms, the majority of reinforcement learning problems can be formulated as Markov Decision Processes (MDPs). All states in an MDP exhibit the “Markov” property, which means that the future depends solely on the current state, rather than the entire history:$$\\mathbb{P}[S_{t+1} | S_t] = \\mathbb{P}[S_{t+1} | S_1, …, S_t]$$In other words, given the present state, the future and the past are conditionally independent. This is because the current state contains all the necessary information to determine future outcomes. A Markove decision process can be defined as $\\mathcal{M} =&lt;\\mathcal{S},\\mathcal{A},\\mathcal{P},\\mathcal{R}, \\gamma&gt;$, where: $\\mathcal{S}$: a set of states; $\\mathcal{A}$: a set of actions; $\\mathcal{P}$: transition probability function; $\\mathcal{R}$: reward function; $\\gamma$: the discount factor is used to account for future rewards. In an unfamilar environment, we lack complete information about the transition probabilities ($\\mathcal{P}$) and reward function ($\\mathcal{R}$). Bellman EquationsBellman equations are essential in RL because they provide a systematic way to decompose and solve complex decision-making problems. They underpin many widely-used RL algorithms and help acheive optimality and convergence to effective policies - more reasons: Optimal Substructure: Bellman equations exploit the fact that optimal solutions can be built from optimal-sub solutions. This property allows RL algorithms to find the best actions to take in any given state by decomposing complex problems into simpler sub-problems. Dynamic Programming: Bellman equations form the foundation of dynamic programming techinques such as Value Iteration and Policy Iteration. These technique are used to compute optimal policies by iteratively updating state values or action values until convergence. Temporal Difference Learning: Bellman equations are a key component of temporal difference learning algorithm like Q-learning and State-Action-Reward-State-Action (SARSA). These algorithms use the difference between successive state values to update the value estimates, which allows them to learn online and incrementally from experience. Convergence to Optimal Policies: The recursive nature of the Bellman equations ensures that algorithms based on them can find optimal policies, provided certain conditions are met (like sufficient exploration). This guarantee of optimality is an attractive feature for solving complex decision-making problems. Markov Decision Processes: Bellman equations enable the modeling of RL problems as Markov Decision Processes (MDPs), where the optimal decision-making process depends only on the current state and not the history. This simplificiation allows for efficient computation and generalization across a wide range or problems. The value function can be broken down into the immediate reward combined with the discounted future values.$$\\begin{flalign}v(s) &amp; = \\mathbb{E}[G_t|S_t=s] \\ &amp; = \\mathbb{E}[R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+…|S_t=s] \\ &amp; =\\mathbb{E}[R_{t+1}+\\gamma(R_{t+2}+\\gamma R_{t+3}+…)|S_t=s] \\ &amp; =\\mathbb{E}[R_{t+1}+\\gamma G_{t+1}|S_t=s] \\ &amp; =\\mathbb{E}[R_{t+1}+\\gamma V(S_{t+1})|S_t=s] \\\\end{flalign}$$ $$\\begin{flalign}Q(s,a) &amp; =\\mathbb{E}[R_{t+1}+\\gamma V(S_{t+1})|S_t=s, A_t=a] \\&amp; = \\mathbb{E}[R_{t+1}+\\gamma \\mathbb{E}{a \\backsim \\pi}Q(S{t+1}, a) | S_t=s, A_t = a]\\end{flalign}$$The above recursive update process can be further separated into equations based on both state-value and action-value functions. As we progress through future action steps, we alternately extend $V$ and $Q$ by adhearing to the policy $\\pi$ Meta Reinforcement LearningA robust meta-learning model should effectively generalize to novel tasks or environments, even those it hasn’t encountered during training. Minimal adaptation should occur during testing with limited exposure. Ideally, the model should self-adjust its internal hidden states to learn without requiring explicit fine-tuning (no gradient backpropagation on trainable variables). Referenceshttps://lilianweng.github.io/posts/2018-04-08-policy-gradient/ https://towardsdatascience.com/policy-gradients-in-reinforcement-learning-explained-ecec7df94245 https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b","link":"/2023/04/02/distill-rl/"},{"title":"Multi-Agent Reinforcement Learning with Epistemic Priors","text":"Thayne T. Walker, Jaime S. Ide, Minkyu Choi, Michael John Guarino, and Kevin Alcedo.International Conference on Control, Decision and Information Technologies (CoDit), 2023 The coordination of multiple autonomous agents is essential for achieving collaborative goals efficiently, especially in environments with limited communication and sensing capabilities. Our recent study, presented at CoDIT 2023, explores a novel method to tackle this challenge. We introduce Multi-Agent Reinforcement Learning with Epistemic Priors (MARL-EP), a technique that leverages shared mental models to enable high-level coordination among agents, even with severely impaired sensing and zero communication. Problem DefinitionImagine a scenario where multiple autonomous agents need to navigate from their respective starting positions to specific goal locations. The challenge intensifies when these agents cannot communicate with each other and have limited sensing capabilities, leading to potential collisions or inefficient goal achievement. This problem is prevalent in various applications, including warehouse logistics, firefighting, surveillance, and transportation. Motivating ExampleConsider a cooperative navigation problem where agents must move from their start states to goal states without colliding with each other or obstacles. Traditional methods rely on accurate state information and communication, but what happens when these are unavailable? Figure 1 in our paper illustrates such a scenario where two agents must navigate to their goals without knowing each other’s real-time positions. If each agent independently chooses the shortest path, they will collide. However, with shared knowledge of each other’s goals and an understanding of common conventions, they can avoid collisions and navigate efficiently. BackgroundMulti-Agent Reinforcement LearningMulti-Agent Reinforcement Learning (MARL) involves training multiple agents to make decisions that maximize a cumulative reward. This is often modeled using Decentralized Partially Observable Markov Decision Processes (DEC-POMDPs). These processes account for the fact that agents have only partial information about the environment and must make decisions based on this limited view. Traditional MARL approaches face significant challenges when communication is restricted or when sensing is limited. Figure 1: (a) An example instance of a cooperative navigation problem and (b) a solution for the problem instance. Epistemic LogicEpistemic logic deals with reasoning about knowledge and beliefs. It allows agents to estimate the knowledge of other agents and predict their actions based on this estimation. By incorporating epistemic logic into MARL, we enable agents to infer unobservable parts of the environment, thereby making more informed decisions. Reinforcement Learning with Epistemic PriorsOur approach, MARL-EP, integrates epistemic priors into the decision-making process of each agent. These priors serve as a shared mental model, helping agents infer the unobservable parts of the environment and coordinate their actions. Convention of OperationA convention of operation is a set of predefined rules or protocols that guide agents’ actions to achieve coordination. For example, in traffic systems, the convention might be to drive on the right side of the road. By following such conventions, agents can predict each other’s actions even without direct communication. Epistemic BlueprintsIn the MARL-EP architecture, each agent uses a deterministic multi-agent planner to generate an epistemic blueprint, a complete multi-agent plan. This blueprint guides the agent’s actions, assuming that all agents have identical plans and follow the same conventions. This method allows agents to coordinate implicitly, leveraging shared knowledge and conventions to achieve their goals. Multi-Agent RL with Epistemic PriorsWe employ the QMIX algorithm, which decomposes the global value function into local value functions for each agent, making the learning process more efficient. In our modified approach, we incorporate epistemic priors into the training process. These priors enhance the agents’ understanding of the global state, improving coordination and overall performance. Figure 2: MARL-EP System Architecture. AlgorithmHere’s a simplified version of our algorithm: Initialize the parameters for the mixing network, agent networks, and hypernetwork. Estimate epistemic priors for each agent at the beginning of each episode. Train agents using local observations augmented with these epistemic priors. Update model parameters iteratively based on the observed rewards and transitions. Experimental ResultsWe validated our approach using the Simple-Spread task in the Multi-agent Particle Environment (MPE). This task involves multiple agents cooperating to reach specific landmarks while avoiding collisions. We tested five different scenarios to evaluate the performance of our method: No sensing (baseline): Agents have no access to other agents’ locations. Limited sensing: Agents can sense nearby agents. Perfect sensing: Agents know the locations of all other agents. No sensing, with priors (QMIX-EP): Similar to baseline but with estimated locations of other agents. Limited sensing, with priors (QMIX-EP): Limited sensing augmented with estimated locations. Figure 3: MPE: Simple-spread task. Three agents cooperate to reach the three landmarks as quick as possible, while avoiding collisions. ResultsThe results, depicted in Figure 4 of our paper, show significant improvements in performance with the use of epistemic priors. In scenarios with no or limited sensing, MARL-EP achieved performance levels close to those with perfect sensing. This demonstrates the effectiveness of using epistemic priors for enhancing coordination among agents. Figure 4: Evaluation of trained QMIX agents for different cases. ConclusionsOur study demonstrates that integrating epistemic priors into multi-agent reinforcement learning can significantly enhance coordination in environments with limited sensing and communication. By leveraging shared mental models and conventions of operation, agents can infer the actions of others and achieve high levels of coordination without direct communication. Future work will focus on applying this approach in real-world scenarios and exploring real-time updates to the epistemic blueprints. We believe that MARL-EP holds great potential for advancing the capabilities of autonomous multi-agent systems in various applications. For those interested in the technical details and further results, we encourage you to read our full paper presented at CoDIT 2023. Citation12345678@inproceedings{walker2023multiagent,title={Multi-Agent Reinforcement Learning with Epistemic Priors},author={Thayne T. Walker and Jaime S. Ide and Minkyu Choi and Michael John Guarino and Kevin Alcedo},booktitle={PRL Workshop Series {\\textendash} Bridging the Gap Between AI Planning and Reinforcement Learning},year={2023},url={https://openreview.net/forum?id=5cWF3p2jDi}}","link":"/2023/07/01/multi-agent-reinforcement-learning-with-epistemic-priors/"},{"title":"Distill Neuro-Symbolic AI","text":"Distilling Neuro-Symbolic AI What is Neuro-Symbolic AINeuro-Symbolic AI Problem Statement: In the evolving landscape of Artificial Intelligence, there exists a pressing need for system capable of learning from experience and subsequently reasoning about the acquired knowledge from uncertain environments. These systems, inspired by temporal logic, should be adept at handling changes in their environment, emphasizing the computational efficiency of their learning processes. Central to their reasoning capabilities are the foundational principles of abduction, deduction, and induction, which collectively form the cornerstone of a neuro-symbolic AI approach.","link":"/2023/08/18/distill-neuro-symbolic-ai/"},{"title":"Distill Diffusion","text":"Distill series – diffusion model. Conditional Diffusion for VideoLet’s test first learn about a diffusion model. A diffusion model generates data by reversing a diffusion process that gradually adds noise to the data unitl it becomes pure noise. The process is modeled in two phases: the foreard process and the reverse (or backward) process. Let $x_0 \\in \\mathbb{R}^d$ be a simple from the data distribution $p_{\\text{data}}$. Forward Process (Diffusion): This is where we start with data $x_0$​ from our desired distribution and add noise over several steps until we get to a point where our data is indistinguishable from noise. Reverse Process (Denoising): We learn to reverse the noise addition process. If we do this correctly we can start with noise and apply the reverse process to generate new data sample that appear as if they were drawn from the target data distribution. Algorithm with codeThe process is mathematically decribed using Markov chains with Gaussian transitions. Forward Transition Kernel: $q(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t}x_{t-1}, \\beta_t \\mathbf{I})$ At each time step $t$, a Gaussian noise is added to the data. $\\beta_t$​​ is a variance schedule that is chosen beforehand and determines how much noise to add at each step. Note that we take a square of $\\beta$ to scale down $x_{t-1}$ so that after adding noise, the total variance remains 1. It scales down the previous difussed image to make room for the noise and then adding Gaussian noise with just the right variance to maintain the overall variance of the process. 123456def forward_diffusion(x_0, T): x_t = x_0 for t in range(1, T+1): beta_t = beta[t-1] x_t = np.sqrt(1. - beta_t) * x_t + np.sqrt(beta_t) * np.random.normal(size=x_0.shape) return x_t Accumulated Kernel: This describes how to sample $x_t$ directly from $x_0$ without going through all intermidate steps. $q(x_t | x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t}x_0, (1 - \\bar{\\alpha}_t)$​ $\\bar{\\alpha}_t$ is the accumulated product of (1-$\\beta_s$) from time 1 to $t$ and represents the overall scale of the original data present in $x_t$. 123def sample_from_x0(x_0, t, alpha_bar): alpha_t = alpha_bar[t-1] return np.sqrt(alpha_t) * x_0 + np.sqrt(1. - alpha_t) * np.random.normal(size=x_0.shape) Reverse Transition Kernel (Reverse Diffusion Process) The Reverse Diffusion Process (RDP), used to generate new samples from a distribution by reversing the Forward Diffusion Process (FDP). The FDP gradually adds noise to the data, and the RDP aims to learn to reverse this noise addition to recreate the data from the noisy distribution. $\\tilde{\\mu}_t$ is the mean of the reverse transition kernel at time $t$, which is a linear combination of the original data point $x_0$ and the current noisy data point $x_t$. The $\\tilde \\beta_t$ term is the variance of the Gaussian distribution for the reverse step, which changes at each step, reflecting how the uncertainty decreases as we approach the original data distribution. The coefficient for $x_0$ determines how much of the original data point $x_0$ is retained in the reverse transition. As $t$ decreases, $\\bar \\alpha_{t-1}$ increases, and $1-\\bar \\alpha_{t-1}$ decreases, making the coefficient larger. This means that as we get closer to $t=0$, we are realying more on the original data point $x_0$ to reconstruct $x_{x-1}$. The coefficient for $x_t$ determine the contribution of the current noisy data point $x_t$ to the reverse transition. As $t$ decreases, $\\alpha_t$ and $\\bar \\alpha_{t-1}$ is decreases, making the coefficient smaller. We want to remove just the right amount of noise (represented by $x_t$) while restoring the right amount of signal (represented by $x_0$). Initially, $x_t$ is mostly noisy, so its coefficient is relatively small, but as we reverse the diffusion process, $x_t$ gradually becomes less noisy, and its influence increases. In the reverse diffusion process, we are effectively using these coefficient to “interploate” between the noise and the data at each step. Initially, the process is noise-dominated, and we have only small “hint” of the data. But as we step back through time, the data’s contribution becomes larger and the noise’s contribution becomes smaller, ultimately allowing us to reconstruct a sample that resembles the original data distribution. It’s important to note that in practice, $\\tilde \\mu$​ would be predicted by a neural network trained to estimate the denoising step. With a Neural NetworkIn a neural network-based approach to reversing the diffusion process to generate data, we focus on estimating the original data $x_0$ from its noisy version $x_t$ without having direct knowledge of $x_0$. This is a critical aspect of the reverse process in diffusion-based generative models. Estimation of $x_0$: Since $x_0$ is unknown during generation, we estimate it from $x_t$, the noisy data at a current timestep. The estimation equation $ \\hat{x}_0 = \\frac{(x_t - \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon)}{\\sqrt{\\bar{\\alpha}_t}} $ rearranges the forward process (Equation 2). The noise $ \\epsilon $ added at each diffusion step is unknown; thus, we utilize a neural network parameterized by $ \\theta(x_t | t) $ to estimate this noise. Neural Network and Loss Function: The neural network is defined by its parameters $ \\theta $, which are refined during training. The loss function $ L(\\theta) $ aims to minimize the discrepancy between the estimated noise $ \\epsilon $ and the noise predicted by the network $ \\theta(x_t | t) $. By minimizing this loss, the network is trained to predict the noise added to $ x_0 $ to produce $ x_t $. Score Function: Estimating $ \\epsilon $ is tantamount to calculating the scaled gradient of the log density of $ x_t $ with respect to itself, scaled by $ \\frac{1}{1 - \\bar{\\alpha}_t} $. This score function denotes the most significant increase direction in the log probability density of the noisy data. Simply put, it indicates the direction from the noisy data $ x_t $ towards the original data $ x_0 $. To delve deeper into the neural network training and the role of the loss function: The neural network $ \\theta(x_t | t) $ is time-conditional, altering its behavior based on the timestep $ t $, which allows for the accommodation of the varying levels of noise in $ x_t $. The loss function $ L(\\theta) $ is explicitly designed to make $ \\theta(x_t | t) $ an effective estimator of the noise $ \\epsilon $ that was incorporated into $ x_0 $ at the time $ t $ during the forward process. Training the network to minimize this loss effectively instructs it on reversing the diffusion process. The score function $ \\nabla_{x_t} \\log q_t(x_t | x_0) $ allows the network’s output to be directly associated with the data’s probability density gradients. This gradient, also known as the score, informs us on adjusting $ x_t $ to enhance its likelihood under the data distribution, which is fundamental to generating representative samples of the original data distribution. In practice, this methodology enables generative models to create samples from intricate distributions by methodically refining noise into structured data. The neural network is trained to counteract the noise introduced in the diffusion process, thus efficiently generating new data samples from the target distribution. The entire training process includes the following iterative steps: Begin with a sample $ x_0 $ from the true data distribution. Introduce noise to generate $ x_t $ following the forward diffusion process. Employ the neural network to estimate the noise $ \\epsilon $. Adjust the neural network parameters $ \\theta $ to minimize the disparity between the estimated and the actual noise, in adherence to the loss function. Upon successful training, the neural network is capable of generating new data samples by initiating from noise and applying the learned reverse process. References[1] MCVD: Masked Conditional Video Diffusion for Prediction, Generation, and Interpolation","link":"/2024/03/16/distill-diffusion/"},{"title":"Neuro Symbolic Video Search with Temporal Logic","text":"Minkyu Choi, Harsh Goel, Mohammad Omama, Yunhao, Yang, Sahil Shah, and Sandeep ChinchaliEuropean Conference on Computer Vision (ECCV), 2024 – Accepted for oral presentation! IntroductionImagine if I asked you to locate the iconic “I am flying” scene from the 3-hour-long Titanic movie. This scene is a complex symphony of multiple semantic events and their long-term temporal relations. Modern state-of-the-art (SOTA) activity recognition networks, which couple semantic reasoning and temporal logic, surprisingly fail at long-term reasoning across frames. Is there a way to decouple the two for effective long-term video understanding? Introducing NSVS: Neuro-Symbolic Video Search. Our recent paper, accepted at ECCV 2024, tackles this problem and outperforms competing baselines by 9-15% on state-of-the-art datasets such as Waymo and NuScenes. Why do we need long-term reasoning in videos?There has been a significant increase in video data production, with platforms such as YouTube receiving 500 hours of uploads every minute. Additionally, autonomous vehicle companies like Waymo generate 10-100 TB of data daily, and worldwide security cameras record around 500 PB daily. Consequently, we require tools with sophisticated query capabilities to navigate this immense volume of video content. For instance, a query such as “Find me all scenes where event A happened, event B did not occur, and event C occurs hours later” requires advanced methods capable of long-term temporal reasoning. Such long-term reasoning is a common use case in surveillance, video analysis, and similar fields that existing video foundation models fail to address. Why do existing methods fail at long-term reasoning in videos?Our key insight is that video foundation models intertwine per-frame perception and temporal reasoning into a single deep network. This makes it difficult for them to understand temporal nuances over the long term. Hence, decoupling but co-designing semantic understanding and temporal reasoning is essential for efficient scene identification. We propose a system thatleverages vision-language models for semantic understanding of individual frames but effectively reasons about the long-term evolution of events using state machines and temporal logic (TL) formulae that inherently capture memory. The figure below shows comparative performance on the event identification tasks. The accuracy of event identification with Video Language Models (Blue/Green) drops as video length or query complexity increases. On the other hand, NSVS (Orange) shows consistent performance irrespective of video length or query complexity. Fig. 1 Comparative Performance on the Event Identification Task: Video Language Models versus NSVS-TL. The accuracy of event identification with Video Language Models (Blue/Green) drops as video length or query complexity increases. In contrast, NSVS-TL (Orange) shows consistent performance irrespective of video length or query complexity. NSVS - DemystifiedWe attribute the consistent performance of NSVS observed in the above figure to the decoupling of per-frame semantic understanding and temporal reasoning. While we plug and play off-the-shelf foundation models like YOLO, CLIP, or LLAVA for semantic understanding, we build upon the massive literature on Formal Methods using state machines and temporal logic (TL) formulae for temporal reasoning. Formal Methods are mathematical techniques used to specify, verify, and prove the correctness of systems. Temporal logic (TL) is a subset of formal methods that describe sequences of events or states over time. TL provides a structured framework for describing and reasoning about the temporal properties of sequences or processes. It extends classical logic with temporal operators to express propositions about the flow of time. To the best of our knowledge, this is the first work to adapt TL for long-term activity recognition. Although it is not necessary to understand this blog, we recommend that readers refer to this crash course for an in-depth understanding of TL. The NSVS PipelineComing back to our example of locating the “I am flying” scene from the 3-hour-long Titanic movie, how does NSVS solve it? The query “I’m flying” is first decomposed into semantically meaningful atomic propositions such as “man hugging woman”, “ship on the sea”, and “kiss” from a high-level user query. SOTA vision and vision-language models are then employed to annotate the existence of these atomic propositions in each video frame. Subsequently, we construct an automaton or state machine that models the video’s temporal evolution based on the list of per-frame atomic propositions detected in the video. Finally, we evaluate when and where this automaton satisfies the user’s query. This also provides confidence measures through formal verification which enables the user to further assess the specific scenes pertaining to a complex query in a long video. We further assess this pipeline for long-term reasoning in videos for queries with varying complexity on a suite of experiments. Fig. 2 NSVS-TL Pipeline. The input query — “Find the ‘I’m Flying’ scene from Titanic” — is first decomposed into semantically meaningful atomic propositions such as “man hugging woman”, “ship on the sea”, and “kiss” from a high-level user query. SOTA vision and vision-language models are then employed to annotate the existence of these atomic propositions in each video frame. Subsequently, we construct a probabilistic automaton that models the video’s temporal evolution based on the list of per-frame atomic propositions detected in the video. Finally, we evaluate when and where this automaton satisfies the user’s query. We do this by expressing it in a formal specification language that incorporates temporal logic. The TL equivalent of the above query is ALWAYS (☐) “man hugging woman” UNTIL (𝕌) “ship on the sea” UNTIL (𝕌) “kiss”. Formal verification techniques are utilized on the automaton to retrieve scenes that satisfy the TL specification. Long-term Video Understanding ResultsAs shown previously, current video-language foundation models such as Video-Llama and ViCLIP excel at scene identification and description in short videos, however, they struggle with long-term and complex temporal queries. Hence, we’ve crafted stronger benchmarks that couple Large Language Models (LLMs) like GPT for reasoning with per-frame annotations from a CV model. Essentially, we replace the sophisticated state machines in NSVS that reason about temporal logic queries with an LLM. This allows us to see how video length impacts scene identification performance when utilizing LLMs. Fig. 3 Performance of NSVS-TL Across Different Video Lengths. Illustrates the F1 scores for scene retrieval against the video length, fulfilling the ``A until B'' temporal specification. Our comprehensive evaluations include scene identification tasks in multi-event sequences with extended temporal events. Specifically, these tasks focus on scenarios where event A persists from the beginning until event B occurs at the end. Therefore, these tasks provide crucial insights into the long-term reasoning capabilities of Large Language Models (LLMs), especially as the temporal distances between events increase. We found that while GPT-3.5 and GPT-3.5 Turbo Instruct struggle with videos longer than 500 seconds, and GPT-4’s performance declines sharply beyond 1000 seconds, our NSVS method maintains consistent accuracy even for videos up to 40 minutes long. This demonstrates NSVS’s robust capability in handling complex, temporally extended video content, potentially opening new avenues for video analysis and understanding. Fig. 4 Comparative Performance of NSVS-TL Across Complex Temporal Logic Specifications. Demonstrate NSVS-TL’s performance to benchmarks across different TL specifications. The TLV DatasetsExisting datasets comprise video annotations for events across short durations. To address this gap in state-of-the-art video datasets for temporally extended activity, we introduce the Temporal Logic Video (TLV) datasets. These datasets come in two flavors: synthetic and real-world. Our synthetic TLV datasets are crafted by cleverly stitching together static images from popular collections like COCO and ImageNet, allowing us to inject a wide array of temporal logic specifications. We’ve also created two video datasets with TL specifications based on real-world autonomous vehicle driving footage from NuScenes and Waymo open-source datasets. We believe that the proposed datasets would enable researchers to benchmark their methods for long-term video understanding and temporal reasoning tasks. More on NSVS-TLFor more information, come see us at the upcoming ECCV 2024 conference. You can find the paper here, the project webpage, and play with our open-sourced datasets and code. Citation12345678@inproceedings{@inproceedings{Choi_2024_ECCV, author={Choi, Minkyu and Goel, Harsh and Omama, Mohammad and Yang, Yunhao and Shah, Sahil and Chinchali, Sandeep}, title={Towards Neuro-Symbolic Video Understanding}, booktitle={Proceedings of the European Conference on Computer Vision (ECCV)}, month={September}, year={2024}}","link":"/2024/03/16/neuro-symbolic-video-search/"},{"title":"Diffusion Based Video Compression","text":"Recent advances have enabled diffusion models to efficiently compress videos while maintaining high visual quality. By storing only keyframes and using these models to interpolate frames during playback, this method ensures high fidelity with minimal data. The process is adaptive, balancing detail retention and compression ratio, and can be conditioned on lightweight information like text descriptions or edge maps for improved results. IntroductionThe rapid growth of digital video content across various platforms necessitates more efficient video compression technologies. Traditional compression methods struggle to balance compression efficiency with high visual fidelity. This blog post discusses a novel approach using diffusion-based models for adaptive video compression, showcasing substantial improvements in efficiency and quality. Traditional Compression Challenges: Standard compression algorithms, like H.264/AVC, H.265/HEVC, and the newer H.266/VVC, primarily focus on reducing spatial and temporal redundancies. However, they often fall short when dealing with high-quality streaming requirements, particularly for content with complex textures or fast movements. Breakthrough with Diffusion Models: Diffusion models, originally designed for image synthesis, have demonstrated potential in enhancing video compression. These models capture intricate image patterns, offering a paradigm shift from predictive to conditional frame generation. Theoretical FrameworkThis section delves into the mathematical foundations and technical descriptions of how diffusion models are applied to video compression. Diffusion for Frame Generation: Diffusion models perform a forward process where noise is progressively added to the video frames, and a reverse process where this noise is removed to reconstruct the original video content, hence allowing for efficient frame prediction. Video Compression via Diffusion: The core idea is to use the generative capabilities of diffusion models to predict future frames based on past frames, significantly reducing the need to transmit every frame. Figure 1: Main Design Architecture: A video streaming pipeline using our method: Identical diffusion models are placed at a host and client; a quality function to govern frames to send downstream; and compression using Matryoshka Representation Learning (MRL). Combining all these design features significantly increase reconstruction quality and reduce compression size MethodologyWe introduce a unique approach that combines traditional video compression techniques with the advanced capabilities of diffusion models. Adaptive Video Compression Using MCVD: Our method employs the Masked Conditional Video Diffusion (MCVD) model, which facilitates the generation of high-quality frames from masked past and future frames. This allows for dynamic adjustment of compression based on frame quality. Integration of Matryoshka Representation Learning: To enhance compression further, we incorporate Matryoshka Representation Learning (MRL), which optimizes the encoding space and significantly reduces the size of data packets transmitted across networks. Figure 2: Matryoshka Representation Learning: Identical losses are calculated for each chunk of the embedding and then summed up (as shown on the left). This leads to better size-to-performance ratio on classification tasks (as shown on the right). Experimental ValidationWe conducted several experiments to validate the effectiveness of our proposed method against traditional compression standards. Experimental Setup: The experiments were carried out using a dataset from the Kodak Image Suite, with metrics such as Bits Per Pixel (BPP) and Peak Signal to Noise Ratio (PSNR) used to evaluate performance. Figure 3: Extreme image compression on Kodak images: Our method can reconstruct the image with much smaller bpps on a universal test image, Kodak. Results: Our results, seen in Figure 3, exemplify MRL improves the image compression without loss of reconstructive quality. We use the same NIC model in our video streaming application, where due to limited infrastructure, we use a pre-trained MCVD diffusion model trained on a Cityscape dataset. Our method shows comparable video reconstruction performance, measured in PSNR, with lower bits-per-pixel (BPP)s, signifying greater compression without performance loss. Our main results are shown in Figure 4 and Figure 6. Figure 4: Extreme video compression with higher performance: Our method has a higher compression rate with better performance than the benchmark. Colored circles represent variance around the mean values for 40 sample videos, with blue for the benchmark and orange for our method. Figure 5: Better video reconstruction with a higher compression rate: Given small embedding dimensions, 50 and 100, our method can reconstruct better video with smaller bpps. We first compare our method to DBVC with respect to the size of the dimension. Our experiment shows that given a small dimension, our method can have much higher PSNRs and lower BPPs, as shown in Figure 5. Our results also illustrate that our method has a higher compression rate due to MRL reducing the encoded vector size, which induces a lower BPP. Next, we compare the overall performance of our method against the benchmark, DBVC, through the BPP and PSNR metrics.Our method has an ~8% higher average PSNR than the benchmark for the same compression rate, shown in Figure 4. Upon careful examination, we can see improvements of nearly 30% for smaller dimensions and larger BPPs. Figure 6: Ablation across encoding dimension size: Our method outperforms the baseline for lower encoded dimension size at the cost of a slightly reduced PSNR. ConclusionThe integration of diffusion models into video compression marks a significant advancement in multimedia technologies. By leveraging these models, we can significantly reduce the bandwidth required for high-quality video streaming. The next steps include optimizing these models for real-time processing and exploring their applications in live video feeds.","link":"/2024/07/15/diffusion-based-video-compression/"},{"title":"PEERNet: Benchmarking Networked Robotics on Wifi, 5G, and Beyond","text":"Aditya Narayanan, Pranav Kasibhatla, Minkyu Choi, Po-han Li, Ruihan Zhao, Sandeep ChinchaliInternational Conference on Intelligent Robots and Systems (IROS), 2024. This post introduces PEERNet, a Python package for real-time benchmarking of networked robotic systems. It provides concise and modular methods for performance analysis of the entire system stack. Fig. 2 PEERNet precisely quantifies inference costs at the edge and in the cloud. For the EfficientNetV2 family of models, local computation on an edge device is roughly 2.5 times as slow as offloaded computation to a cloud server, but cloud servers display a high variance in inference latency. Identifying non-intuitive behavior in Vison Language ModelsIn our second experiment, we use PEERNet to explore inference with Vision Language Models (VLMs) at the edge, an emerging task in robotic pipelines. We show that PEERNet is capable of identifying non-intuitive behaviors in VLM inference, such as a bimodal output token distribution. Fig. 4 Profiling with PEERNet reveals latency tradeoffs and costs in end-to-end teleoperation pipelines. PEERNet identifies the combination of an RTX3090 GPU, local image resizing, and a LAN to be the most performative teleoperation configuration, with lower end-to-end latency than local inference. Looking to try PEERNet out?Our code is open-source! You can find it at github.com/UTAustin-SwarmLab/PEERNet","link":"/2024/12/01/peernet/"},{"title":"FeUdal Networks for Hierarchical Reinforcement Learning","text":"The paper introduces a hierarchical reinforcement learning framework called Feudal Networks (FuN) that enables agents to learn a hierarchy of temporal abstractions. The proposed model consists of a Manager and a Worker, with the Manager learning high-level policies and the Worker learning low-level policies. The approach is demonstrated to achieve state-of-the-art performance on a range of challenging tasks in the Atari domain. OverviewFeUda Network (FuN) is a hierarchincal reinforcement learning method inspired by the feudal reinforcement learning proposal of Dayan and Hinton. The need for hierarchical reinforcement learning arises because most of the State-Of-The-Art (SOTA) reinforcement learning algorithms are focused on task specifications and are lmited to low-level executions, which receive states and execute actions. They do not perform any high-level planning or goal selection based on state information. Hierarchical reinforcement learning overcomes this limitation by introducing higher-level policies or networks that plan and selects goals while lower-level policies execute actions accordingly. FuN employs two modules: the Manager and the Worker. The Manager operates at a lower temporal resolution, receiving states and setting abstract goals that are conveyed to and enacted by the Worker [1]. In contrast, the Worker generates primitive actions by observing the environment at every tick. By decoupling the Manager and the Worker, FuN facilitates long timescale credit assignment and the emergence of sub-policies associated with different goals set by the Manager. This enables FuN to outperform strong baseline agents on tasks that require long-term credit assignment or memorization. The modelFuN is a modular neural network consisting of two modules - the Worker and the Manager [1]. Let’s go little bit in depth on the role of two modules. Figure 1 illustrates the overal design and the following equation explaines the forward dynmaics of FuN: The Manager and the Worker modules of FuN share a perceptual module that computes a shared representation $z_t$ based on environment observation $x_t$. The Manager’s main objective is to generate a goal given states, while the Worker produces primitive actions conditioned on external observation, internal state, and the goal from the Manager. The goals $g_t$ are trained using an approximate transition policy gradient, which exploits the knowledge that the Worker’s behavior will align with the goal defined by the Manager. The goals are then converted to embedded vectors $w_t$ via a linear transform $φ$, which are used to train the Worker and eventually produce the policy – a vector of probabilities over primitive actions. The Worker is trained through intrinsic reward to generate actions that move towards the goal directions to be achieved. The Worker’s RNN produces $U_t$ for the policy, which is computed from a product between $U_t$ and $w_t$. This training approach enables FuN to learn a hierarchy of sub-policies associated with different goals, and facilitates long-term credit assignment, allowing the agent to outperform a strong baseline agent on tasks involving long-term credit assignment or memorization. Goal EmbeddingComputing the goal embedding is one of novelties of this paper. Essentially, it modulates the policy via “a multiplicative interaction in a low dimensional goal-embedding space.” As a reminder, the embedded goal $w_t$ is obtained by a linear transformation from $R^d$ to $R^k$, where the last $c$ goals are first pooled by summation and then embedded into a vector $w_t$ to ensure that the embedded vector can incorporate multiple goals. By having a multiplicative interaction between $U_t$ and $w_t$ the policy can be modulated to incorporate different goal directions from the Manager. Firstly, we have to pool the last $c$ goals and do summation. Due to pooling the goals over several time-steps, the conditioning from the Manager varies smoothly. The reason for pooling the last $c$ goals is to create a summary of the recent sub-goals that the Manager has selected. This summary allows the Worker to have a sense of the overall direction the Manager wants to pursue. And then we need to embed the pooled goals into vector $w$ using linear projection $φ$. The projection φ is linear, with no biases, and is learnt with gradients coming from the Worker’s actions. “Since $\\phi$ has no biases it can never produce a constant non-zero vector – which is the only way the setup could ignore the Manager’s input. It makes sure that the goal output by the Manager always influences the final policy.” [1]. By not including any bias terms, the linear projection function $\\phi$ ensures that the output vector is never shifted away from zero. This is important because if $\\phi$ did include bias terms, it could potentially produce a non-zero vector even if the input vector from the Manager was zero. In this case, the Worker’s policy would not be influenced by the Manager’s input Lastly, the Worker’s embedding matrix U is then combined with the goal embedding w via a matrix-vector product LearningThe whole learning process is following a standard reinforcement learning set up. The agent selects the action given observations from the environment and receives reward for the selected actions. The goal of the agent is to maximize the discounted return. The agent’s behaviour is defined by its action-selection policy $\\pi$. However, FuN propose “instead to independently train the Manager to predict advantageous directions (transitions) in state space and to intrinsically reward the Worker to follow these directions.” If the Worker can fulfil the goal of moving these directions (as it is rewarded for doing), then we should end up taking advantageous trajectories through state space. The Worker network receives two inputs - the current stat of the environment and a goal vector selected by the Manager. Based on these inputs, The Worker network provides an output action to be executed by the agent in the environment. The primary objective of the Worker network is to acquire a policy that maximizes the intrinsic reward, which is determined based on the progress made towards the present goal vector. The FuN architecture uses policy gradient training to learn the policy, which involves computing gradients of the expected reward with respect to the network parameters of the embedded goal from the Manager and using them to update the network parameters The issue with computing these gradients in the FuN architecture is that the state representation used by the Worker network depends on the network parameters as well as world observations from the environment. This presents a challenge, as unlike the conventional reinforcement learning process that trains end-to-end through gradient descent on either the “Policy” directly or via TD-learning, FuN takes a different approach. Specifically, the goal from the Manager would lose any semantic meaning if it were incorporated as an input to the Worker network, so FuN treats these goals as internal latent variables of the model. The FeUdal Network architecture overcomes the dependence of the state representation on the network parameters when computing the gradient of the cosine similarity between the state representation and the goal vector by assuming that the state representation is fixed and does not change with the network parameters. By doing this, the FeUdal Network architecture is able to utilize the cosine similarity between the state representation and the goal vector to provide the Worker network with a useful learning signal without encountering any issues with the chain rule of calculus. The update rule (gradient) of the goal from the Manger is as follow:$$\\nabla g_t = A_t^M \\nabla_\\theta d_{cos} (s_{t+c}-s_t,g_t(\\theta)),$$where $A_t^M = R_t - V_t^M(x_t,\\theta)$ is the Manager’s advantage function, coputed using a value function estimate $V_t^M(x_t,\\theta)$ from the internal critic. The definition of intrinsic reward that incentivizes the Wroker to pursue the specified goals is as follows:$$r_t^I = 1/c \\sum_{i=1}^cd_{cos}(s_t-s_{t-i}, g_{t-i})$$The paper suggests that incorporating directions (latent goals) enables the Worker network to make directional shifts in the latent space without assuming arbitrary absolute locations, making it a more practical approach for high-dimensional and continuous latent spaces where exploring all possible locations would be difficult. In fact, the feudal reinforcement learning formulation by (Dayan and Hinton in 1993) proposed completely hiding the reward signal from lower levels of hierarchy in the learning process. However, this paper, a softer approach of adding an instrinsic reward for following the goals to the environment reward was taken. The Worker, therfore, is trained to maximise a weighted sum $R_t+\\alpha R_t^I$ , where $\\alpha$ is a hyperparameter that regulates the influence of the intrinsic reward. A large $\\alpha$ value would give more influence to the Worker than the task-specific policies. At the end, the task specific policy $\\pi$ can be trained to maximize intrinsic reward from a conventional deep reinforcement learning such as actor critic (Mnih et al., 2016):$$\\nabla\\pi_t=A_t^D \\nabla_\\theta \\log \\pi(a_t|x_t;\\theta),$$where $A_t^D = (R_t + \\alpha R_t^I - V_t^D(x_t;\\theta))$ a.k.a. the Advantage function is calculated using an internal critic, which estimates the value functions for both rewards. Importantly, the Worker and Manager have different discount factors $\\lambda$ for comuting the return since the Worker can be more greedy and focus on immediate rewards while the Manager can put efforts on a long-term planning. Transition Policy GradientsThe transition policy refer to to the meathod of transitioning from one policy to another policy. A hight-level policy $o_t = \\mu(s_t,\\theta)$ selects amoung sub-policies (possibly from a continuous set) where it’s only used for fixed behaviours - remember goal is lasting for c steps. Then a follow-up question is how sub-policies are transitioned over to others. The paper said a transition distribution, $p(s_{t_c}|s_t,o_t)$ , the distribution of states that end up at the end of the sub policy, given the start state is used to enact next sub-policy. The transition policy, $\\pi^{TP}(s_{t+c}|s_t)=p(s_{t_c}|s_t,\\mu(s_t,\\theta))$, is composed with the high-level policy with the transition distribution. The paper says it’s valid to refer to this as a policy instead of a general transition distribution is becase the original MDP is isomorphic to a new MDP with policy $\\pi^{TP}$ and transition function $s_{t+c} = \\pi^{TP}(s_t)$ Therefore, the policy gradient theorem can be applied to the transition policy $\\pi^{TP}$$$\\nabla_{\\theta \\pi_t^{TP}} = \\mathop{\\mathbb{E}}[(R_t-V(s_t))]\\nabla_{\\theta} \\log p(s_{t_c}|s_t,\\mu(s_t,\\theta)))$$The FuN framework offers a departure from the conventional approach of directly optimizing the policy to maximize the cumulative reward. Instead, it models and optimizes the transition policy, which is a distribution over future states given the current state and action. By modeling the transitions, the agent can bypass the complex trajectory of the Worker and directly optimize the predicted transition distribution. This is possible because, with knowledge of where the trajectories are likely to end up, we can follow the policy gradient of the predicted transition, rather than the Worker’s behavior. FuN assumes that the direction in state-space, $s_{t+c}−s_t$, follows a von Mises-Fisher distribution. The von Mises-Fisher distribution is used in FuN to model the direction in state-space that the agent may transition to when executing a given sub-policy. The Manager outputs the goal vector that encodes the desired direction of transition and sets the mean direction of the distribution. Meanwhile, the concentration parameter determines how likely the agent is to transition in the direction of the goal vector versus in a random direction. ArchitectureThe perceptual module $f^{percept}$ is a convolutional network (CNN) followed by a fully connected layer. I think the perceptual module can be modified to differenty type of networks depedning on use cases. Each convolutional and fully-connected layer is followed by a rectifier non-linerarity - this is substantially the same CNN as in, the only difference is that in the pre-processing state all colour channels are retained. Dilated LSTM The paper proposed a novel RNN architecture for the Manager, which runs at lower temporal resolution than the world observation from the environment. A dilate LSTM is analogously defined to dilated convolutional networks (Yu &amp; Koltun, 2016). It basically separte $r$ groups of sub-states or ‘cores’ instead of full state $h_t$. So the hidden state of Dilated LSTM is $h= (\\hat{h^i})_{i=1}^r,$ meaning that it’s composed of r separate groups. Threfore, in each timestep $t$, the network equestion is as follow:$$\\hat{h}_t^{t\\zeta r},g_t=LSTM(s_t,\\hat{h} _{t-1}^{t \\zeta r}; \\theta^{LSTM}),$$where $\\zeta$ denotes the modulo operation and allows us to indicate which group of cores is currently being updated. $\\theta^{LSMT}$ explicitly stree that the same set of parameters governs the update for each or the $r$ groups within the dLSTM. The advantage of this Dilated LSTM is that the $r$ groups of cores in the dLSTM can preserve the memories for long periods while the dLSTM as a whole is still processing and learning from every experience, and it can update the output at every step, which is similar to clockwork RNNs (Kulkarni et al., 2016), however there the top level “ticks” at a fixed, slow pace, whereas the dLSTM observes all the available training data instead. In the experiments we set $r = 10$, and this was also used as the predictions horizon, $c$ [1] ConclusionIn conclusion, this paper presents an interesting approach to the challenge of task selection for reinforcement learning agents, which is often a difficult and non-deterministic problem. The FeUdal Network architecture allows for end-to-end training of both the Manager and Worker modules, which is a remarkable achievement. While training the Manager module on its own is not a difficult task, the paper’s approach of training the Manager and Worker together in a holistic process is novel and promising. However, the scalability of the Manager’s role remains a potential issue, as an increasing number of goals and state dimensions may not always result in appropriate goals for the Worker. Further research is necessary to determine the effectiveness of this approach in more complex and diverse environments, as well as to investigate potential solutions for the scalability issue. Overall, the FeUdal Network architecture presents an innovative approach to reinforcement learning with potential for further exploration and refinement. If you want to explore experiment result, please find details from the original paper. References[1] Vezhnevets, Alexander Sasha, et al. “Feudal networks for hierarchical reinforcement learning.” International Conference on Machine Learning. PMLR, 2017. [2] Dayan, Peter and Hinton, Geoffrey E. Feudal reinforcement learning. In NIPS. Morgan Kaufmann Publishers, 1993. [3] Jaderberg, Max, Mnih, Volodymyr, Czarnecki, Wojciech Marian, Schaul, Tom, Leibo, Joel Z, Silver, David, and Kavukcuoglu, Koray. Reinforcement learning with unsupervised auxiliary tasks. arXiv preprint arXiv:1611.05397, 2016. [4] Kulkarni, Tejas D., Narasimhan, Karthik R., Saeedi, Ardavan, and Tenenbaum, Joshua B. Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. arXiv preprint arXiv:1604.06057, 2016.","link":"/2023/03/20/feudal/"}],"tags":[{"name":"Simulation","slug":"simulation","link":"/tags/simulation/"},{"name":"Statistics","slug":"statistics","link":"/tags/statistics/"},{"name":"English","slug":"english","link":"/tags/english/"},{"name":"Linux","slug":"linux","link":"/tags/linux/"},{"name":"Data Engineering","slug":"data-engineering","link":"/tags/data-engineering/"},{"name":"Security","slug":"security","link":"/tags/security/"},{"name":"Korean","slug":"korean","link":"/tags/korean/"},{"name":"Machine Learning","slug":"machine-learning","link":"/tags/machine-learning/"},{"name":"Artificial Intelligence","slug":"artificial-intelligence","link":"/tags/artificial-intelligence/"},{"name":"Reinforcement Learning","slug":"reinforcement-learning","link":"/tags/reinforcement-learning/"},{"name":"Distill Series","slug":"distill-series","link":"/tags/distill-series/"},{"name":"Neuro-symbolic AI","slug":"neuro-symbolic-ai","link":"/tags/neuro-symbolic-ai/"},{"name":"Generative AI","slug":"generative-ai","link":"/tags/generative-ai/"},{"name":"Diffusion","slug":"diffusion","link":"/tags/diffusion/"},{"name":"Computer Vision","slug":"computer-vision","link":"/tags/computer-vision/"},{"name":"Formal Method","slug":"formal-method","link":"/tags/formal-method/"},{"name":"Networked Robotics","slug":"networked-robotics","link":"/tags/networked-robotics/"},{"name":"Edge Computing","slug":"edge-computing","link":"/tags/edge-computing/"},{"name":"Literature Review","slug":"literature-review","link":"/tags/literature-review/"}],"categories":[{"name":"2. Project","slug":"2-project","link":"/categories/2-project/"},{"name":"3. Blog Post","slug":"3-blog-post","link":"/categories/3-blog-post/"},{"name":"1. Research","slug":"1-research","link":"/categories/1-research/"}]}